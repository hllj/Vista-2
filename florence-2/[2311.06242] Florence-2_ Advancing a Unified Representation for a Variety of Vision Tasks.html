<!DOCTYPE html>
<!-- saved from url=(0044)https://ar5iv.labs.arxiv.org/html/2311.06242 -->
<html lang="en-US" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>[2311.06242] Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</title><meta property="og:description" content="We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer lear…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2311.06242">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/2311.06242">

<!--Generated on Tue Feb 27 20:00:44 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en-US">
<h1 class="ltx_title ltx_title_document">Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bin Xiao<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">†</span></sup>  Haiping Wu<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">∗</span></sup>  Weijian Xu<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">∗</span></sup>  Xiyang Dai  Houdong Hu 
<br class="ltx_break"> Yumao Lu  Michael Zeng  Ce Liu<sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">‡</span></sup>  Lu Yuan<sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">‡</span></sup>  
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic" style="font-size:90%;">†</span></sup><span id="id8.8.2" class="ltx_text" style="font-size:90%;">project lead   <sup id="id8.8.2.1" class="ltx_sup"><span id="id8.8.2.1.1" class="ltx_text ltx_font_italic">∗</span></sup>equal contribution   <sup id="id8.8.2.2" class="ltx_sup"><span id="id8.8.2.2.1" class="ltx_text ltx_font_italic">‡</span></sup>direcional lead 
<br class="ltx_break">
Azure AI, Microsoft
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p"><span id="id15.id1.1" class="ltx_text">We introduce <em id="id15.id1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em>, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. <em id="id15.id1.1.2" class="ltx_emph ltx_font_italic">Florence-2</em> was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed <em id="id15.id1.1.3" class="ltx_emph ltx_font_italic">FLD-5B</em> that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train <em id="id15.id1.1.4" class="ltx_emph ltx_font_italic">Florence-2</em> to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated <em id="id15.id1.1.5" class="ltx_emph ltx_font_italic">Florence-2</em> to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">We aim to build a vision foundation model to enable extensive perception capabilities including spatial hierarchy and semantic granularity. To achieve this, a single unified model <em id="S1.F1.5.2.1" class="ltx_emph ltx_font_bold ltx_font_italic">Florence-2</em> is pre-trained on our <em id="S1.F1.5.2.2" class="ltx_emph ltx_font_bold ltx_font_italic">FLD-5B</em> dataset encompassing a total of 5.4B comprehensive annotations across 126M images, which are collected by our Florence data engine. </span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the realm of Artificial General Intelligence (AGI) systems, there has been a notable shift towards utilizing pre-trained, versatile representations, acknowledged for task-agnostic benefits accross diverse applications. This trend is evident in natural language processing (NLP), where advanced models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib5" title="" class="ltx_ref">5</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib19" title="" class="ltx_ref">19</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib43" title="" class="ltx_ref">43</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib66" title="" class="ltx_ref">66</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib6" title="" class="ltx_ref">6</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib65" title="" class="ltx_ref">65</a>]</cite> show adaptability with comprehensive knowledge spanning various domains and tasks with simple instructions. The success of NLP motivates a parallel approach in computer vision.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Universal representation for diverse vision-related tasks presents unique challenges, notably the need for comprehensive perceptual abilities. Unlike NLP, which deals mainly with text, computer vision requires handling intricate visual data like object location, masked contours, and attributes. Attaining universal representation in computer vision demands adept management of a spectrum of complex tasks, organized two-dimensionally as illustrated in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S1.F1" title="In 1 Introduction ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Spatial Hierarchy</span>: The model must discern spatial details across varying scales, understanding image-level concepts and fine-grained pixel specifics. Accommodating the intricate spatial hierarchy within vision demands the model’s proficiency in handling diverse levels of granularity.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Semantic Granularity</span>: Universal representation in computer vision should span a spectrum of semantic granularity. The model transitions from high-level captions to nuanced descriptions, enabling versatile understanding for diverse applications.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">This pursuit is characterized by distinctiveness and substantial challenges. A key hurdle is the scarcity of <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">comprehensive visual annotations</em>, hindering the development of a foundational model capable of capturing the intricate nuances of spatial hierarchy and semantic granularity. Existing datasets, such as ImageNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, and Flickr30k Entities&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, tailored for specialized applications, are extensively labeled by humans. To overcome this constraint, it is imperative to generate extensive annotations for each image on a larger scale.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Another challenge is the absence of a <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">unified pre-training framework with a singular network architecture</em> that seamlessly integrates spatial hierarchy and semantic granularity in computer vision. Traditional models excel in tasks like object detection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib26" title="" class="ltx_ref">26</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, semantic segmentation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib16" title="" class="ltx_ref">16</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib82" title="" class="ltx_ref">82</a>]</cite>, and image captioning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib45" title="" class="ltx_ref">45</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib78" title="" class="ltx_ref">78</a>]</cite> with task-specific design. However, it is essential to develop a comprehensive, unified model that is capable of adapting across various vision tasks in a task-agnostic manner, even accommodating new tasks with minimal or no task-specific fine-tuning.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The model <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">Florence</em>&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite> pioneers the integration of spatial, temporal, and multi-modal aspects in computer vision through unified pre-training and network architecture. The first evolutionary version&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite> excels in transfer learning via pre-training with noisy text-image pairs and task-specific fine-tuning using specialized adapters. However, it relies on large task-specific datasets and adapters, leaving gaps in addressing the above dual key challenges.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this paper, we introduce <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">Florence-2</em>, a universal backbone achieved through multitask learning with extensive visual annotations. This results in a unified, prompt-based representation for diverse vision tasks, effectively addressing the challenges of limited comprehensive data and the absence of a unified architecture.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Multitask learning necessitates large-scale, high-quality annotated data. Our data engine, instead of relying on labor-intensive manual annotation, autonomously generates a comprehensive visual dataset called <em id="S1.p7.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em>, encompassing a total of 5.4B annotations for 126M images. This engine consists of two efficient processing modules. The first module uses specialized models to collaboratively and autonomously annotate images, moving away from the traditional single and manual annotation approach. Multiple models work together to reach a consensus, reminiscent of the wisdom of crowds concept&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib89" title="" class="ltx_ref">89</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib80" title="" class="ltx_ref">80</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, ensuring a more reliable and unbiased image understanding. The second module iteratively refines and filters these automated annotations using well-trained foundational models.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">By utilizing this extensive dataset, our model employs a sequence-to-sequence (seq2seq) architecture&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib76" title="" class="ltx_ref">76</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib17" title="" class="ltx_ref">17</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib66" title="" class="ltx_ref">66</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, which integrates an image encoder and a multi-modality encoder-decoder. This design accommodates a spectrum of vision tasks without the need for task-specific architectural modifications, aligning with the ethos of the NLP community for versatile model development with a consistent underlying structure. All annotations in the dataset <em id="S1.p8.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em>, are uniformly standardized into textual outputs,
facilitating a unified multi-task learning approach with consistent optimization with the same loss function as the objective. The outcome is a versatile vision foundation model, <em id="S1.p8.1.2" class="ltx_emph ltx_font_italic">Florence-2</em>, capable of performing a variety of tasks, such as object detection, captioning, and grounding, all within a single model governed by a uniform set of parameters. Task activation is achieved through textual prompts, reflecting the approach used by Large Language Models (LLMs)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib65" title="" class="ltx_ref">65</a>]</cite>.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our approach attains a universal representation, demonstrating broad applicability across various visual tasks. Key results include:</p>
</div>
<div id="S1.p10" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">As a versatile vision foundation model, <em id="S1.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> achieves new state-of-the-art zero-shot performance in tasks such as captioning on COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, visual grounding on Flick30k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, and referring expression comprehension on RefCOCO/+/g&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib31" title="" class="ltx_ref">31</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib93" title="" class="ltx_ref">93</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">After fine-tuning with public human-annotated data, <em id="S1.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em>, despite its compact size, competes with larger specialist models. Notably, the fine-tuned <em id="S1.I2.i2.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2</em> establishes new state-of-the-art results on the benchmarks on RefCOCO/+/g.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">The pre-trained <em id="S1.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> backbone enhances performance on downstream tasks, <em id="S1.I2.i3.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.I2.i3.p1.1.3" class="ltx_text"></span> COCO object detection and instance segmentation, and ADE20K semantic segmentation, surpassing both supervised and self-supervised models. Compared to pre-trained models on ImageNet, ours improves training efficiency by 4<math id="S1.I2.i3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.I2.i3.p1.1.m1.1a"><mo id="S1.I2.i3.p1.1.m1.1.1" xref="S1.I2.i3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.I2.i3.p1.1.m1.1b"><times id="S1.I2.i3.p1.1.m1.1.1.cmml" xref="S1.I2.i3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.I2.i3.p1.1.m1.1c">\times</annotation></semantics></math> and achieves substantial improvements of 6.9, 5.5, and 5.9 points on COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and ADE20K&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib98" title="" class="ltx_ref">98</a>]</cite> datasets, using Mask-RCNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, DINO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, and UperNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib82" title="" class="ltx_ref">82</a>]</cite> frameworks respectively.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Rethinking Vision Model Pre-training</h2>

<figure id="S2.F2" class="ltx_figure"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.5.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><em id="S2.F2.6.2" class="ltx_emph ltx_font_bold ltx_font_italic" style="font-size:90%;">Florence-2</em><span id="S2.F2.7.3" class="ltx_text" style="font-size:90%;"> consists of an image encoder and standard multi-modality encoder-decoder. We train <em id="S2.F2.7.3.1" class="ltx_emph ltx_font_bold ltx_font_italic">Florence-2</em> on our <em id="S2.F2.7.3.2" class="ltx_emph ltx_font_bold ltx_font_italic">FLD-5B</em> data in a unified multitask learning paradigm, resulting in a generaslist vision foundation model, which can perform various vision tasks.</span></figcaption>
</figure>
<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In pursuit of a versatile vision foundation model, we revisit three predominant pre-training paradigms: supervised (<em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.2" class="ltx_text"></span>, ImageNet classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib18" title="" class="ltx_ref">18</a>]</cite>), self-supervised (<em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.4" class="ltx_text"></span>, SimCLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, MoCo <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, BEiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, MAE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib24" title="" class="ltx_ref">24</a>]</cite>), and weakly supervised (<em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.p1.1.6" class="ltx_text"></span>, CLIP&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, Florence&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, SAM&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib32" title="" class="ltx_ref">32</a>]</cite>). Each paradigm captures unique aspects of visual data but is inherently limited by the constraints of single-task learning frameworks. Supervised pre-training excels in object recognition but lacks adaptability&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib38" title="" class="ltx_ref">38</a>]</cite>; self-supervised algorithms reveal intricate features but may overemphasize certain attributes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib8" title="" class="ltx_ref">8</a>]</cite>; weakly supervised methods leverage unstructured textual annotations but yield only image-level understanding&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. To build a unified vision foundation model suitable for various applications, we must explore innovative pre-training strategies that overcome single-task limitations and integrate both textual and visual semantics.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Image understanding necessitates capturing multiple levels of granularity, from global semantics to local details, and comprehending spatial relationships between objects and entities in their semantic context. To address these core aspects of image understanding, our approach incorporates a diverse set of annotations, effectively capturing visual understanding nuances and bridging the gap between vision and language understanding.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Comprehensive Multitask Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">To develop a versatile vision foundation model, we formulate a range of multitask learning objectives, each tailored to address specific aspects of visual comprehension. These objectives align with our predefined criteria: spatial hierarchy and semantic granularity, inspired by recent research on multitask learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib79" title="" class="ltx_ref">79</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib12" title="" class="ltx_ref">12</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Our multitask learning approach incorporates three distinct learning objectives, each addressing a different level of granularity and semantic understanding:</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Image-level understanding</span> tasks capture high-level semantics and foster a comprehensive understanding of images through linguistic descriptions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib13" title="" class="ltx_ref">13</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib91" title="" class="ltx_ref">91</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib18" title="" class="ltx_ref">18</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
They enable the model to comprehend the overall context of an image and grasp semantic relationships and contextual nuances in the language domain. Exemplar tasks include image classification, captioning, and visual question answering.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Region/pixel-level recognition</span> tasks facilitate detailed object and entity localization within images, capturing relationships between objects and their spatial context. Tasks include object detection, segmentation, and referring expression comprehension.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Fine-grained visual-semantic alignment</span> tasks require fine-grained understanding of both text and image. It involves locating the image regions that correspond to the text phrases, such as objects, attributes, or relations. These tasks challenge the ability to capture the local details of visual entities and their semantic contexts, as well as the interactions between textual and visual elements.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">By combining these three learning objectives in a multitask learning framework, our foundation model learns to handle different levels of detail and semantic understanding. This strategic alignment enables our model to deal with various spatial details, distinguish levels of detail in understanding, and go beyond surface-level recognition—ultimately learning a universal representation for vision understanding.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We present the foundation model <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em>, designed for universal representation learning, capable of handling various vision tasks with a single set of weights and a unified architecture. As depicted in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S2.F2" title="In 2 Rethinking Vision Model Pre-training ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, <em id="S3.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2</em> employs a sequence-to-sequence learning paradigm&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, integrating all tasks, described in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S2" title="2 Rethinking Vision Model Pre-training ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>, under a common language modeling objective. The model takes images coupled with task-prompt as task instructions, and generates the desirable results in text forms. It uses a vision encoder to convert images into visual token embeddings, which are then concatenated with text embeddings and processed by a transformer-based multi-modal encoder-decoder to generate the response. In the following sections, we will provide a detailed explanation of each model component.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Task formulation.</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We adopt a sequence-to-sequence framework&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib77" title="" class="ltx_ref">77</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to address various vision tasks in a unified manner. As shown in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#A1.T13" title="In Appendix A Supported Tasks and Annotations in Florence-2 ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">13</span></a>, we formulate each task as a translation problem: Given an input image and a task-specific prompt, we generate the corresponding output response. Depending on the task, the prompt and response can be either text or region:</p>
</div>
<div id="S3.SS0.SSS0.Px1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Text</span>: When the prompt or answer is plain text without special formatting, we maintain it in our final sequence-to-sequence format.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Region</span>:
For region-specific tasks, we add location tokens to the tokenizer’s vocabulary list, representing quantized coordinates. We create <math id="S3.I1.i2.p1.1.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S3.I1.i2.p1.1.m1.2a"><mrow id="S3.I1.i2.p1.1.m1.2.3.2" xref="S3.I1.i2.p1.1.m1.2.3.1.cmml"><mn id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">1</mn><mo id="S3.I1.i2.p1.1.m1.2.3.2.1" xref="S3.I1.i2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.I1.i2.p1.1.m1.2.2" xref="S3.I1.i2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.2b"><list id="S3.I1.i2.p1.1.m1.2.3.1.cmml" xref="S3.I1.i2.p1.1.m1.2.3.2"><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">1</cn><cn type="integer" id="S3.I1.i2.p1.1.m1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.2c">1,000</annotation></semantics></math> bins, similar to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib10" title="" class="ltx_ref">10</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib11" title="" class="ltx_ref">11</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, and represent regions using formats tailored to task requirements:</p>
</div>
<div id="S3.I1.i2.p2" class="ltx_para">
<ul id="S3.I1.i2.I1" class="ltx_itemize">
<li id="S3.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Box representation <math id="S3.I1.i2.I1.i1.p1.1.1.m1.4" class="ltx_Math" alttext="(x_{0},y_{0},x_{1},y_{1})" display="inline"><semantics id="S3.I1.i2.I1.i1.p1.1.1.m1.4a"><mrow id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml"><mo stretchy="false" id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.5" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml">(</mo><msub id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.cmml"><mi id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.2" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.2.cmml">x</mi><mn id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.3" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.6" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml">,</mo><msub id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.cmml"><mi id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.2" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.2.cmml">y</mi><mn id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.3" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.7" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml">,</mo><msub id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.cmml"><mi id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.2" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.2.cmml">x</mi><mn id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.3" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.3.cmml">1</mn></msub><mo id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.8" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml">,</mo><msub id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.cmml"><mi id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.2" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.2.cmml">y</mi><mn id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.3" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.3.cmml">1</mn></msub><mo stretchy="false" id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.9" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.I1.i1.p1.1.1.m1.4b"><vector id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.5.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4"><apply id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1">subscript</csymbol><ci id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.2.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.2">𝑥</ci><cn type="integer" id="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.3.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.1.1.1.1.3">0</cn></apply><apply id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.1.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2">subscript</csymbol><ci id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.2.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.2">𝑦</ci><cn type="integer" id="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.3.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.2.2.2.2.3">0</cn></apply><apply id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.1.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3">subscript</csymbol><ci id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.2.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.2">𝑥</ci><cn type="integer" id="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.3.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.3.3.3.3.3">1</cn></apply><apply id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.1.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.2.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.2">𝑦</ci><cn type="integer" id="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.3.cmml" xref="S3.I1.i2.I1.i1.p1.1.1.m1.4.4.4.4.3">1</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.I1.i1.p1.1.1.m1.4c">(x_{0},y_{0},x_{1},y_{1})</annotation></semantics></math></span>: Utilized in tasks such as object detection and dense region captioning, with location tokens corresponding to the box coordinates. The location tokens are the coordinates of the top-left and bottom-right corners of the box.</p>
</div>
</li>
<li id="S3.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Quad box representation <math id="S3.I1.i2.I1.i2.p1.1.1.m1.5" class="ltx_Math" alttext="(x_{0},y_{0},...,x_{3},y_{3})" display="inline"><semantics id="S3.I1.i2.I1.i2.p1.1.1.m1.5a"><mrow id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml"><mo stretchy="false" id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.5" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">(</mo><msub id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.cmml"><mi id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.2" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.2.cmml">x</mi><mn id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.3" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.6" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.cmml"><mi id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.2" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.2.cmml">y</mi><mn id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.3" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.7" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">,</mo><mi mathvariant="normal" id="S3.I1.i2.I1.i2.p1.1.1.m1.1.1" xref="S3.I1.i2.I1.i2.p1.1.1.m1.1.1.cmml">…</mi><mo id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.8" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.cmml"><mi id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.2" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.2.cmml">x</mi><mn id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.3" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.3.cmml">3</mn></msub><mo id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.9" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.cmml"><mi id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.2" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.2.cmml">y</mi><mn id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.3" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.3.cmml">3</mn></msub><mo stretchy="false" id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.10" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.I1.i2.p1.1.1.m1.5b"><vector id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.5.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4"><apply id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.2.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.2">𝑥</ci><cn type="integer" id="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.3.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.2.2.1.1.3">0</cn></apply><apply id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.2.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.2">𝑦</ci><cn type="integer" id="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.3.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.3.3.2.2.3">0</cn></apply><ci id="S3.I1.i2.I1.i2.p1.1.1.m1.1.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.1.1">…</ci><apply id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.2.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.2">𝑥</ci><cn type="integer" id="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.3.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.4.4.3.3.3">3</cn></apply><apply id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.1.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4">subscript</csymbol><ci id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.2.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.2">𝑦</ci><cn type="integer" id="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.3.cmml" xref="S3.I1.i2.I1.i2.p1.1.1.m1.5.5.4.4.3">3</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.I1.i2.p1.1.1.m1.5c">(x_{0},y_{0},...,x_{3},y_{3})</annotation></semantics></math></span>: For text detection and recognition tasks, using location tokens for each coordinate of the quadrilateral enclosing the text. The location tokens are the coordinates of each corner of the quad box, starting from the top-left and going clockwise.</p>
</div>
</li>
<li id="S3.I1.i2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S3.I1.i2.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S3.I1.i2.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Polygon Representation <math id="S3.I1.i2.I1.i3.p1.1.1.m1.5" class="ltx_Math" alttext="(x_{0},y_{0},...,x_{n},y_{n})" display="inline"><semantics id="S3.I1.i2.I1.i3.p1.1.1.m1.5a"><mrow id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml"><mo stretchy="false" id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.5" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">(</mo><msub id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.cmml"><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.2" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.2.cmml">x</mi><mn id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.3" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.6" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.cmml"><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.2" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.2.cmml">y</mi><mn id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.3" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.3.cmml">0</mn></msub><mo id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.7" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">,</mo><mi mathvariant="normal" id="S3.I1.i2.I1.i3.p1.1.1.m1.1.1" xref="S3.I1.i2.I1.i3.p1.1.1.m1.1.1.cmml">…</mi><mo id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.8" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.cmml"><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.2" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.2.cmml">x</mi><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.3" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.3.cmml">n</mi></msub><mo id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.9" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">,</mo><msub id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.cmml"><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.2" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.2.cmml">y</mi><mi id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.3" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.3.cmml">n</mi></msub><mo stretchy="false" id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.10" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.I1.i3.p1.1.1.m1.5b"><vector id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.5.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4"><apply id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.2.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.2">𝑥</ci><cn type="integer" id="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.3.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.2.2.1.1.3">0</cn></apply><apply id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.2.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.2">𝑦</ci><cn type="integer" id="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.3.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.3.3.2.2.3">0</cn></apply><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.1.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.1.1">…</ci><apply id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3">subscript</csymbol><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.2.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.2">𝑥</ci><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.3.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.4.4.3.3.3">𝑛</ci></apply><apply id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4"><csymbol cd="ambiguous" id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.1.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4">subscript</csymbol><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.2.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.2">𝑦</ci><ci id="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.3.cmml" xref="S3.I1.i2.I1.i3.p1.1.1.m1.5.5.4.4.3">𝑛</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.I1.i3.p1.1.1.m1.5c">(x_{0},y_{0},...,x_{n},y_{n})</annotation></semantics></math></span>: For referring segmentation tasks, with location tokens representing the vertices of the polygon. The location tokens are the coordinates of the vertices of the polygon, in clockwise order.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="S3.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p3.1" class="ltx_p">By extending the tokenizer’s vocabulary to include location tokens, we enable the model to process region-specific information in a unified learning format. This eliminates the need to design task-specific heads for different tasks and allows for a more data-centric approach.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Vision encoder.</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.6" class="ltx_p">We employ DaViT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib20" title="" class="ltx_ref">20</a>]</cite> as the vision encoder. It processes an input image <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{I}\in\mathbb{R}^{H\times W\times 3}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">𝐈</mi><mo id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1a" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1"><in id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.2">𝐈</ci><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.3">𝑊</ci><cn type="integer" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">\mathbf{I}\in\mathbb{R}^{H\times W\times 3}</annotation></semantics></math> (with <math id="S3.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">H</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.3.m3.1c">W</annotation></semantics></math> denoting height and width, respectively) into flattened visual token embeddings <math id="S3.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{V}\in\mathbb{R}^{N_{v}\times D_{v}}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S3.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">𝐕</mi><mo id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.cmml"><msub id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml"><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.2" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.2.cmml">D</mi><mi id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.3" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1"><in id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.1"></in><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.2">𝐕</ci><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3"><times id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.1"></times><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.2.3">𝑣</ci></apply><apply id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.2">𝐷</ci><ci id="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.3.cmml" xref="S3.SS0.SSS0.Px2.p1.4.m4.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.4.m4.1c">\mathbf{V}\in\mathbb{R}^{N_{v}\times D_{v}}</annotation></semantics></math>, where <math id="S3.SS0.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="N_{v}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.5.m5.1a"><msub id="S3.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">N</mi><mi id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.2">𝑁</ci><ci id="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.5.m5.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.5.m5.1c">N_{v}</annotation></semantics></math> and <math id="S3.SS0.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="D_{v}" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.6.m6.1a"><msub id="S3.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1.2">𝐷</ci><ci id="S3.SS0.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS0.SSS0.Px2.p1.6.m6.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.6.m6.1c">D_{v}</annotation></semantics></math> represent the number and dimensionality of vision tokens, respectively.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-modality encoder decoder.</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.4" class="ltx_p">We use a standard encoder-decoder transformer architecture to process visual and language token embeddings.
We first obtain prompt text embeddings <math id="S3.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{prompt}\in\mathbf{R}^{N_{t}\times D}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><msub id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.2.cmml">𝐓</mi><mrow id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.cmml"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1a" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.4" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1b" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.5" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1c" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.6" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1d" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.7" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.7.cmml">t</mi></mrow></msub><mo id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml">𝐑</mi><mrow id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml"><msub id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.2" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.3.cmml">t</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1"><in id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.1"></in><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.2">𝐓</ci><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3"><times id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.1"></times><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.2">𝑝</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.3">𝑟</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.4.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.4">𝑜</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.5.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.5">𝑚</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.6.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.6">𝑝</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.7.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.2.3.7">𝑡</ci></apply></apply><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.2">𝐑</ci><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.1"></times><apply id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.1.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.2.3">𝑡</ci></apply><ci id="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.1.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.1.m1.1c">\mathbf{T}_{prompt}\in\mathbf{R}^{N_{t}\times D}</annotation></semantics></math> using our extended language tokenizer and word embedding layer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
Then, we concatenate vision token embeddings with prompt embeddings to form the multi-modality encoder module input, <math id="S3.SS0.SSS0.Px3.p1.2.m2.2" class="ltx_Math" alttext="\mathbf{X}=[\mathbf{V}^{\prime},\mathbf{T}_{prompt}]" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.2.m2.2a"><mrow id="S3.SS0.SSS0.Px3.p1.2.m2.2.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.4" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.4.cmml">𝐗</mi><mo id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.3.cmml">[</mo><msup id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.2.cmml">𝐕</mi><mo id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.3.cmml">′</mo></msup><mo id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.4" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.2.cmml">𝐓</mi><mrow id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.cmml"><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.2" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.2.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.3" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1a" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.4" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1b" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.5" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.5.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1c" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.6" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.6.cmml">p</mi><mo lspace="0em" rspace="0em" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1d" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml">​</mo><mi id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.7" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.7.cmml">t</mi></mrow></msub><mo stretchy="false" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.5" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.2.m2.2b"><apply id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2"><eq id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.3"></eq><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.4.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.4">𝐗</ci><interval closure="closed" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2"><apply id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.2">𝐕</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.1.1.1.1.1.3">′</ci></apply><apply id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.2">𝐓</ci><apply id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3"><times id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.1"></times><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.2">𝑝</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.3">𝑟</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.4.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.4">𝑜</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.5.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.5">𝑚</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.6.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.6">𝑝</ci><ci id="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.7.cmml" xref="S3.SS0.SSS0.Px3.p1.2.m2.2.2.2.2.2.3.7">𝑡</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.2.m2.2c">\mathbf{X}=[\mathbf{V}^{\prime},\mathbf{T}_{prompt}]</annotation></semantics></math>, where <math id="S3.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{V}^{\prime}\in\mathbb{R}^{N_{v}\times D}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S3.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><msup id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.2" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.2.cmml">𝐕</mi><mo id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.3" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.3.cmml">′</mo></msup><mo id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.2" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml"><msub id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.cmml"><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.2" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.2.cmml">N</mi><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.3" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.3.cmml">v</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.1" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.3" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1"><in id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.1"></in><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.2">𝐕</ci><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.2.3">′</ci></apply><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3"><times id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.1"></times><apply id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.1.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2">subscript</csymbol><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.2.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.2">𝑁</ci><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.3.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.2.3">𝑣</ci></apply><ci id="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS0.SSS0.Px3.p1.3.m3.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.3.m3.1c">\mathbf{V}^{\prime}\in\mathbb{R}^{N_{v}\times D}</annotation></semantics></math> is obtained by applying a linear projection and LayerNorm layer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to <math id="S3.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{V}" display="inline"><semantics id="S3.SS0.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px3.p1.4.m4.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS0.SSS0.Px3.p1.4.m4.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px3.p1.4.m4.1c">\mathbf{V}</annotation></semantics></math> for dimensionality alignment.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Optimization objective.</h4>

<div id="S3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px4.p1.2" class="ltx_p">Given the input <math id="S3.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS0.SSS0.Px4.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px4.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px4.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px4.p1.1.m1.1c">x</annotation></semantics></math> combined from the image and the prompt, and the target <math id="S3.SS0.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS0.SSS0.Px4.p1.2.m2.1a"><mi id="S3.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px4.p1.2.m2.1b"><ci id="S3.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px4.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px4.p1.2.m2.1c">y</annotation></semantics></math>, we use the standard language modeling with cross-entropy loss for all the tasks.</p>
</div>
<div id="S3.SS0.SSS0.Px4.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\mathcal{L}=-\sum_{i=1}^{|y|}logP_{\theta}(y_{i}|y_{&lt;i},x)," display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml">ℒ</mi><mo id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mo id="S3.E1.m1.3.3.1.1.1a" xref="S3.E1.m1.3.3.1.1.1.cmml">−</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.E1.m1.3.3.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E1.m1.3.3.1.1.1.1.2.2.3.1" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.1.1.2.2.3.3" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">y</mi><mo stretchy="false" id="S3.E1.m1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.4" xref="S3.E1.m1.3.3.1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.5" xref="S3.E1.m1.3.3.1.1.1.1.1.5.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.2b" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.6" xref="S3.E1.m1.3.3.1.1.1.1.1.6.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.6.2" xref="S3.E1.m1.3.3.1.1.1.1.1.6.2.cmml">P</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.6.3" xref="S3.E1.m1.3.3.1.1.1.1.1.6.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.1.1.1.1.2c" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub><mo fence="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">x</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></eq><ci id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3">ℒ</ci><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2"></sum><apply id="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3"><eq id="S3.E1.m1.3.3.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.1"></eq><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.3.3.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.3"><abs id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></abs><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑦</ci></apply></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.2"></times><ci id="S3.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.3">𝑙</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.4.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.4">𝑜</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.5.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.5">𝑔</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.6.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.6.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.6">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.6.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.6.2">𝑃</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.6.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.6.3">𝜃</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑦</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply><list id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑦</ci><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑥</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathcal{L}=-\sum_{i=1}^{|y|}logP_{\theta}(y_{i}|y_{&lt;i},x),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS0.SSS0.Px4.p2.2" class="ltx_p">where <math id="S3.SS0.SSS0.Px4.p2.1.m1.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS0.SSS0.Px4.p2.1.m1.1a"><mi id="S3.SS0.SSS0.Px4.p2.1.m1.1.1" xref="S3.SS0.SSS0.Px4.p2.1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px4.p2.1.m1.1b"><ci id="S3.SS0.SSS0.Px4.p2.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px4.p2.1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px4.p2.1.m1.1c">\theta</annotation></semantics></math> are the network parameters, <math id="S3.SS0.SSS0.Px4.p2.2.m2.1" class="ltx_Math" alttext="|y|" display="inline"><semantics id="S3.SS0.SSS0.Px4.p2.2.m2.1a"><mrow id="S3.SS0.SSS0.Px4.p2.2.m2.1.2.2" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.2.1.cmml"><mo stretchy="false" id="S3.SS0.SSS0.Px4.p2.2.m2.1.2.2.1" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.2.1.1.cmml">|</mo><mi id="S3.SS0.SSS0.Px4.p2.2.m2.1.1" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS0.SSS0.Px4.p2.2.m2.1.2.2.2" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px4.p2.2.m2.1b"><apply id="S3.SS0.SSS0.Px4.p2.2.m2.1.2.1.cmml" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.2.2"><abs id="S3.SS0.SSS0.Px4.p2.2.m2.1.2.1.1.cmml" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.2.2.1"></abs><ci id="S3.SS0.SSS0.Px4.p2.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px4.p2.2.m2.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px4.p2.2.m2.1c">|y|</annotation></semantics></math> is the number of target tokens.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="138" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.10.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><em id="S3.F3.11.2" class="ltx_emph ltx_font_bold ltx_font_italic" style="font-size:90%;">Florence-2</em><span id="S3.F3.12.3" class="ltx_text ltx_font_bold" style="font-size:90%;"> data engine<span id="S3.F3.12.3.1" class="ltx_text ltx_font_medium"> consists of three essential phrases: (1) initial annotation employing specialist models, (2) data filtering to correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement. Our final dataset (<em id="S3.F3.12.3.1.1" class="ltx_emph ltx_font_bold ltx_font_italic">FLD-5B</em>) of over </span>5B<span id="S3.F3.12.3.2" class="ltx_text ltx_font_medium"> annotations contains </span>126M<span id="S3.F3.12.3.3" class="ltx_text ltx_font_medium"> images, </span>500M<span id="S3.F3.12.3.4" class="ltx_text ltx_font_medium"> text annotations, </span>1.3B<span id="S3.F3.12.3.5" class="ltx_text ltx_font_medium"> region-text annotations, and </span>3.6B<span id="S3.F3.12.3.6" class="ltx_text ltx_font_medium"> text-phrase-region annotations.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Engine</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">To train our <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> model, we require a comprehensive, large-scale, high-quality multitask dataset encompassing various image data aspects. Given the scarcity of such data, we have developed a new multitask image dataset. This dataset <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">FLD-5B</em> includes <span id="S4.p1.1.3" class="ltx_text ltx_font_bold">126M</span> images, <span id="S4.p1.1.4" class="ltx_text ltx_font_bold">500M</span> text annotations, and <span id="S4.p1.1.5" class="ltx_text ltx_font_bold">1.3B</span> text-region annotations, and <span id="S4.p1.1.6" class="ltx_text ltx_font_bold">3.6B</span> text-phrase-region annotations across different tasks. We extensively explain our data collection and annotation procedures, encompassing adaptations for various annotation types. The data engine pipeline, shown in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S3.F3" title="In Optimization objective. ‣ 3 Model ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a>, will be discussed in subsequent sections.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Image Collection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We construct our data by gathering a diverse collection of images from various sources.
We begin with the identification of three key tasks that act as primary sources for our image corpus: image classification, object detection, and image captioning.
Consequently, we curate and combine five distinct datasets originating from the aforementioned tasks: ImageNet-22k <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, Object 365 <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, Open Images <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, Conceptual Captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, and LAION <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib68" title="" class="ltx_ref">68</a>]</cite> filtered by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
This combination results in a dataset of 126 million images in total.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Data Annotation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our primary objective is to generate comprehensive annotations that can support multitask learning effectively.
Accordingly, our annotation endeavors span a comprehensive range of tasks, encapsulated within three discrete annotation categories: <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">text</span>, <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">region-text</span> pairs, and <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">text-phrase-region</span> triplets, which is illustrated in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4.F4" title="In Iterative data refinement. ‣ 4.2 Data Annotation ‣ 4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.
The data annotation workflow consists of three essential phases, each of which ensures the accuracy and quality of the annotations: (1) initial annotation employing specialist models, (2) data filtering to correct errors and remove irrelevant annotations, and (3) an iterative process for data refinement.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Initial annotation with specialist models.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">To initiate the annotation process for each annotation type, we employ synthetic labels obtained from specialist models.
These specialist models are a combination of offline models trained on a diverse range of publicly available datasets and online services hosted on cloud platforms. They are specifically tailored to excel in annotating their respective annotation types.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p2.1" class="ltx_p">It is worth noting that certain image datasets may already contain partial annotations for some annotation types. For instance, the Object 365&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a>]</cite> dataset already includes human-annotated bounding boxes and corresponding categories as region-text annotations. In such cases, we merge the pre-existing annotations with the synthetic labels generated by the specialist models. This approach enhances the coverage and diversity of the annotations.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p3.1" class="ltx_p">Moreover, specific annotations, such as detailed descriptions in the text annotation type, are represented by datasets of a considerably small size. This inherently poses challenges in obtaining high-performance specialist models. Consequently, we opt to omit these tasks during the initial annotation phase. Annotations for these tasks are generated later during the iterative data refinement process.</p>
</div>
<div id="S4.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p4.1" class="ltx_p">In summation, through the rigorous initial annotation procedures, we ensure that the aggregated dataset of 126 million images is comprehensively labeled across the majority of annotation types.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data filtering and enhancement.</h4>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">The initial annotations obtained from the specialist models, while comprehensive, are susceptible to noise and imprecision. In response to this challenge, we have implemented a multifaceted filtering process to refine and eliminate undesired annotations. Our general filtering protocol mainly focuses on two data types in the annotations: text and region data.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p2.1" class="ltx_p">First, pertaining to textual annotations, we are inspired by DiHT <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib63" title="" class="ltx_ref">63</a>]</cite> and develop a parsing tool based on SpaCy <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib28" title="" class="ltx_ref">28</a>]</cite> to extract objects, attributes, and actions. We filter out texts containing excessive objects, as they tend to introduce noise and may not accurately reflect the actual content in the corresponding images. Additionally, we assess the complexity of the actions and objects by measuring their degree of node in the dependency parsing tree. We retain texts with a certain minimum action and object complexity to ensure the richness of visual concepts in the images.</p>
</div>
<div id="S4.SS2.SSS0.Px2.p3" class="ltx_para">
<p id="S4.SS2.SSS0.Px2.p3.1" class="ltx_p">Second, in relation to the region annotations, specifically bounding boxes, we remove the noisy boxes under a confidence score threshold. Complementing this, we also employ non-maximum suppression to reduce redundant or overlapping bounding boxes.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Iterative data refinement.</h4>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">Using our filtered initial annotations, we trained a multitask model that processes sequences of data.
Upon evaluating this model against our training images, we discerned a marked enhancement in its predictions, particularly in instances where original labels were marred by inaccuracies or extraneous noise, such as in alt-texts. Motivated by these findings, we integrated these updated annotations with our original ones and subjected the model to another training iteration. This cyclical refinement process incrementally improves the quality of our training dataset.</p>
</div>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S4.SS2.SSS0.Px3.p2.1" class="ltx_p">In the case of tasks we initially bypassed due to insufficient data for the training of a robust specialist model, we leveraged the iteratively trained model for pre-training purposes. Subsequent fine-tuning of this pre-trained model with the sparse dataset showcased superior performance compared to a model trained from scratch on the same data. Thus, we harness the fine-tuned model as a specialist for annotating our expansive dataset comprising 126 million images, ensuring comprehensive annotation coverage.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="306" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.5.2" class="ltx_text" style="font-size:90%;">An illustrative example of an image and its corresponding annotations in <em id="S4.F4.5.2.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset. Each image in <em id="S4.F4.5.2.2" class="ltx_emph ltx_font_italic">FLD-5B</em> is annotated with text, region-text pairs, and text-phrase-region triplets by Florence data engine, which covers multiple spatial hierarchies, brief-to-detailed progressive granularity, and a wide semantics spectrum, enabling more comprehensive visual understanding from diverse perspectives.</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Annotation-specific Variations</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4.SS2" title="4.2 Data Annotation ‣ 4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.2</span></a>, we introduce our general annotation workflow. This section delves into each annotation type and the corresponding variations of the annotation procedure.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Text annotations categorize images using three types of granularities: brief, detailed, and more detailed. The brief text includes only one sentence that demonstrates the most salient objects and activities, which is similar to COCO caption&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In contrast, the detailed text and more detailed text contain multiple sentences that describe the image with richer objects, attributes, and actions.</p>
</div>
<div id="S4.SS3.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p2.1" class="ltx_p">For the brief text, a <em id="S4.SS3.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> model is trained as the specialist on publicly available image caption and image-text datasets, creating an image-to-text model for initial annotations. Iterative refinement is used to minimize noise in these texts. For the detailed text, prompts including existing image annotations like the brief text and region-text annotations, are fed to large language models (LLMs) or large multimodal models (LMMs) to generate comprehensive descriptions. Due to the high cost of the large models, only a small set of detailed text and more detailed text are generated. These are used to fine-tune the caption specialist, developing a detailed description specialist for further annotations.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Region-text pairs.</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">The region-text pairs provide descriptive textual annotation for semantic regions in the image. Semantic regions include regions of visual objects as well as text regions. The region is represented by a tight bounding box surrounds the region. Moreover, each region can be annotated with varying degrees of granularity, including phrases and sentences, that contribute to a richer understanding of the region.</p>
</div>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p2.1" class="ltx_p">Region-text pairs are annotated differently for text regions and visual object regions. Text regions are labeled using Azure AI Services’ OCR API&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, while visual objects are initially annotated with a DINO object detector&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite> trained on public datasets. Data filtering, including confidence thresholding and non-maximum suppression, removes noisy boxes. Textual annotations for the visual object regions are further enriched by brief text generated from an image-to-text model with cropped image regions. Each region then receives three textual annotations: phrase from object category, brief text, and noun phrase chunks from the brief text. The Florence-1&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite> model determines the most similar textual annotation to each image region.</p>
</div>
</section>
<section id="S4.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Text-phrase-region triplets.</h4>

<div id="S4.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px3.p1.1" class="ltx_p">Text-phrase-region triplets consist of a descriptive text of the image, noun phrases in this text related to image objects, and region annotations for these objects. The text includes brief, detailed, and more detailed text generated earlier. For each text, the Grounding DINO model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib50" title="" class="ltx_ref">50</a>]</cite> identifies noun phrases and creates bounding boxes for them. Additionally, the SAM model&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib32" title="" class="ltx_ref">32</a>]</cite> generates segmentation masks for each box, offering more precise object localization. During data filtering, a confidence score threshold is applied to both noun phrases and bounding boxes to ensure relevance. A blacklist is also used to exclude irrelevant noun phrases like pronouns and abstract concepts.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<td id="S4.T1.2.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">Dataset</td>
<td id="S4.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">Rep. Model</td>
<td id="S4.T1.2.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">#Images</td>
<td id="S4.T1.2.1.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">#Annotations</td>
<td id="S4.T1.2.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">Spatial hierarchy</td>
<td id="S4.T1.2.1.1.6" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:7.5pt;padding-right:7.5pt;">Semantics granularity</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">JFT300M&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">ViT</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">300M</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">300M</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">Image-level</td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:7.5pt;padding-right:7.5pt;">Coarse</td>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<td id="S4.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">WIT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>]</cite>
</td>
<td id="S4.T1.2.3.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">CLIP</td>
<td id="S4.T1.2.3.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">400M</td>
<td id="S4.T1.2.3.3.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">400M</td>
<td id="S4.T1.2.3.3.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Image-level</td>
<td id="S4.T1.2.3.3.6" class="ltx_td ltx_align_left" style="padding-left:7.5pt;padding-right:7.5pt;">Coarse</td>
</tr>
<tr id="S4.T1.2.4.4" class="ltx_tr">
<td id="S4.T1.2.4.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">SA-1B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib32" title="" class="ltx_ref">32</a>]</cite>
</td>
<td id="S4.T1.2.4.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">SAM</td>
<td id="S4.T1.2.4.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">11M</td>
<td id="S4.T1.2.4.4.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">1B</td>
<td id="S4.T1.2.4.4.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Region-level</td>
<td id="S4.T1.2.4.4.6" class="ltx_td ltx_align_left" style="padding-left:7.5pt;padding-right:7.5pt;">Non-semantic</td>
</tr>
<tr id="S4.T1.2.5.5" class="ltx_tr">
<td id="S4.T1.2.5.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">GrIT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
</td>
<td id="S4.T1.2.5.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Kosmos-2</td>
<td id="S4.T1.2.5.5.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">91M</td>
<td id="S4.T1.2.5.5.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">137M</td>
<td id="S4.T1.2.5.5.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Image &amp; Region-level</td>
<td id="S4.T1.2.5.5.6" class="ltx_td ltx_align_left" style="padding-left:7.5pt;padding-right:7.5pt;">Fine-grained</td>
</tr>
<tr id="S4.T1.2.6.6" class="ltx_tr">
<td id="S4.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">M3W&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S4.T1.2.6.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Flamingo</td>
<td id="S4.T1.2.6.6.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">185M</td>
<td id="S4.T1.2.6.6.4" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">43.3M*</td>
<td id="S4.T1.2.6.6.5" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">Multi-image-level</td>
<td id="S4.T1.2.6.6.6" class="ltx_td ltx_align_left" style="padding-left:7.5pt;padding-right:7.5pt;">Fine-grained</td>
</tr>
<tr id="S4.T1.2.7.7" class="ltx_tr" style="background-color:#E6E6E6;">
<td id="S4.T1.2.7.7.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">
<em id="S4.T1.2.7.7.1.1" class="ltx_emph ltx_font_italic" style="background-color:#E6E6E6;">FLD-5B</em><span id="S4.T1.2.7.7.1.2" class="ltx_text" style="background-color:#E6E6E6;"> (ours)</span>
</td>
<td id="S4.T1.2.7.7.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;">
<em id="S4.T1.2.7.7.2.1" class="ltx_emph ltx_font_italic" style="background-color:#E6E6E6;">Florence-2</em><span id="S4.T1.2.7.7.2.2" class="ltx_text" style="background-color:#E6E6E6;"> (ours)</span>
</td>
<td id="S4.T1.2.7.7.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;"><span id="S4.T1.2.7.7.3.1" class="ltx_text" style="background-color:#E6E6E6;">126M</span></td>
<td id="S4.T1.2.7.7.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;"><span id="S4.T1.2.7.7.4.1" class="ltx_text" style="background-color:#E6E6E6;">5B</span></td>
<td id="S4.T1.2.7.7.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:7.5pt;padding-right:7.5pt;"><span id="S4.T1.2.7.7.5.1" class="ltx_text" style="background-color:#E6E6E6;">Image &amp; Region-level</span></td>
<td id="S4.T1.2.7.7.6" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:7.5pt;padding-right:7.5pt;"><span id="S4.T1.2.7.7.6.1" class="ltx_text" style="background-color:#E6E6E6;">Coarse to fine-grained</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison with datasets in vision foundation model training. *Flamingo’s annotations are counted in the number of documents, where each document may have multiple images.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Dataset</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section introduces the statistics and analysis of <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em> that we built using the data engine in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4" title="4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>. We begin with an overview of the dataset and compare it with the recent works. We then show further analyses of detailed annotation statistics, semantic coverage and spatial coverage in the established dataset.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overview</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Following the data engine, we build a large-scale training set (<em id="S5.SS1.p1.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em>) of 126M images, more than <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_bold">500M</span> text annotations, <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_bold">1.3B</span> region-text annotations, and <span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_bold">3.6B</span> text-phrase-region annotations. Each image is annotated with text, region-text pairs, and text-phrase-region triplets and each annotation type has multiple instances varying in diverse granularity. An illustrative example of an image and its corresponding annotations can be found in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4.F4" title="In Iterative data refinement. ‣ 4.2 Data Annotation ‣ 4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">We provide a comparison between our data set and the existing data sets that are commonly used for training foundation models in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4.T1" title="In Text-phrase-region triplets. ‣ 4.3 Annotation-specific Variations ‣ 4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">1</span></a>. Our data set has several advantages over the previous ones, such as having more annotations in total and per image. Moreover, the annotations in our data set span multiple levels of spatial and semantic granularity, which allows for more diverse and comprehensive visual understanding tasks.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Data Analysis</h3>

<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Annotation statistics.</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">The statistics for each annotation type within our dataset are presented in <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.T2" title="In Annotation statistics. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">Firstly, we have around <span id="S5.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">500M</span> text annotations, including brief, detailed, and more detailed texts with different lengths. It is noteworthy that our detailed and more detailed text has 4x and 9x number of tokens compared with the brief text that is similar to COCO captions&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. These lengthy annotations provide much richer information for comphrensive visual understanding.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p3.1" class="ltx_p">In addition, our dataset has around <span id="S5.SS2.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">1.3B</span> region-text annotations, which is more than 30x larger than the academic object detection datasets such as OpenImages&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and Object 365&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a>]</cite>. On average, each image has around 5 regions, and each region is annotated with either a phrase or a relatively longer brief text. Note that the regional brief text (2.55 avg tokens) is shorter than typical brief text annotation (7.95 avg tokens), as the regional brief text annotation actually includes a mixture of phrase, noun chunks, and brief text based on the Florence-1 score. More details can be found from&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S4.SS3" title="4.3 Annotation-specific Variations ‣ 4 Data Engine ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span>&nbsp;<span class="ltx_text ltx_ref_tag">4.3</span></a> - region-text pairs.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p4.1" class="ltx_p">Moreover, we collect text-phrase-region annotations that include more than <span id="S5.SS2.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_bold">3.6B</span> phrase-region pairs for the <span id="S5.SS2.SSS0.Px1.p4.1.2" class="ltx_text ltx_font_bold">500M</span> text annotations. Specifically, the brief text annotation has 4.27 average phrase-region pairs, while detailed and more detailed text annotation has more than 10 pairs, indicating that the richer text annotation covers more objects and their corresponding phrases in the text.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.2.1.1" class="ltx_tr">
<th id="S5.T2.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Annotation Type</span></th>
<th id="S5.T2.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Text Type</span></th>
<th id="S5.T2.2.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.3.1" class="ltx_text" style="font-size:90%;">#Image Annotations</span></th>
<th id="S5.T2.2.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.4.1" class="ltx_text" style="font-size:90%;">#Avg Tokens</span></th>
<th id="S5.T2.2.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.5.1" class="ltx_text" style="font-size:90%;">#Regions</span></th>
<th id="S5.T2.2.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.6.1" class="ltx_text" style="font-size:90%;">#Avg Regions</span></th>
<th id="S5.T2.2.1.1.7" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.1.1.7.1" class="ltx_text" style="font-size:90%;">#Avg Regional Tokens</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.2.2.1" class="ltx_tr">
<th id="S5.T2.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">Text</span></th>
<th id="S5.T2.2.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.2.1" class="ltx_text" style="font-size:90%;">Brief</span></th>
<td id="S5.T2.2.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.3.1" class="ltx_text" style="font-size:90%;">235M</span></td>
<td id="S5.T2.2.2.1.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.4.1" class="ltx_text" style="font-size:90%;">7.95</span></td>
<td id="S5.T2.2.2.1.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.2.1.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.2.1.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.2.1.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T2.2.3.2" class="ltx_tr">
<th id="S5.T2.2.3.2.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"></th>
<th id="S5.T2.2.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.2.1" class="ltx_text" style="font-size:90%;">Detailed</span></th>
<td id="S5.T2.2.3.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.3.2.4" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.4.1" class="ltx_text" style="font-size:90%;">31.65</span></td>
<td id="S5.T2.2.3.2.5" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.3.2.6" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.3.2.7" class="ltx_td ltx_align_right" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.3.2.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T2.2.4.3" class="ltx_tr">
<th id="S5.T2.2.4.3.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"></th>
<th id="S5.T2.2.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.2.1" class="ltx_text" style="font-size:90%;">More detailed</span></th>
<td id="S5.T2.2.4.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.4.3.4" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.4.1" class="ltx_text" style="font-size:90%;">70.53</span></td>
<td id="S5.T2.2.4.3.5" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.4.3.6" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.4.3.7" class="ltx_td ltx_align_right" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.4.3.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S5.T2.2.5.4" class="ltx_tr">
<th id="S5.T2.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.1.1" class="ltx_text" style="font-size:90%;">Region-Text</span></th>
<th id="S5.T2.2.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.2.1" class="ltx_text" style="font-size:90%;">Phrase</span></th>
<td id="S5.T2.2.5.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.5.4.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.5.4.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.5.1" class="ltx_text" style="font-size:90%;">681M</span></td>
<td id="S5.T2.2.5.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.6.1" class="ltx_text" style="font-size:90%;">5.42</span></td>
<td id="S5.T2.2.5.4.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.5.4.7.1" class="ltx_text" style="font-size:90%;">1.19</span></td>
</tr>
<tr id="S5.T2.2.6.5" class="ltx_tr">
<th id="S5.T2.2.6.5.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"></th>
<th id="S5.T2.2.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.2.1" class="ltx_text" style="font-size:90%;">Brief</span></th>
<td id="S5.T2.2.6.5.3" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.6.5.4" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S5.T2.2.6.5.5" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.5.1" class="ltx_text" style="font-size:90%;">681M</span></td>
<td id="S5.T2.2.6.5.6" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.6.1" class="ltx_text" style="font-size:90%;">5.42</span></td>
<td id="S5.T2.2.6.5.7" class="ltx_td ltx_align_right" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.6.5.7.1" class="ltx_text" style="font-size:90%;">2.55</span></td>
</tr>
<tr id="S5.T2.2.7.6" class="ltx_tr">
<th id="S5.T2.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.1.1" class="ltx_text" style="font-size:90%;">Text-Phrase-Region</span></th>
<th id="S5.T2.2.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.2.1" class="ltx_text" style="font-size:90%;">Brief</span></th>
<td id="S5.T2.2.7.6.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.3.1" class="ltx_text" style="font-size:90%;">235M</span></td>
<td id="S5.T2.2.7.6.4" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.4.1" class="ltx_text" style="font-size:90%;">7.95</span></td>
<td id="S5.T2.2.7.6.5" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.5.1" class="ltx_text" style="font-size:90%;">1007M</span></td>
<td id="S5.T2.2.7.6.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.6.1" class="ltx_text" style="font-size:90%;">4.27</span></td>
<td id="S5.T2.2.7.6.7" class="ltx_td ltx_align_right ltx_border_t" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.7.6.7.1" class="ltx_text" style="font-size:90%;">1.93</span></td>
</tr>
<tr id="S5.T2.2.8.7" class="ltx_tr">
<th id="S5.T2.2.8.7.1" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"></th>
<th id="S5.T2.2.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.2.1" class="ltx_text" style="font-size:90%;">Detailed</span></th>
<td id="S5.T2.2.8.7.3" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.8.7.4" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.4.1" class="ltx_text" style="font-size:90%;">31.65</span></td>
<td id="S5.T2.2.8.7.5" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.5.1" class="ltx_text" style="font-size:90%;">1289M</span></td>
<td id="S5.T2.2.8.7.6" class="ltx_td ltx_align_right ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.6.1" class="ltx_text" style="font-size:90%;">10.25</span></td>
<td id="S5.T2.2.8.7.7" class="ltx_td ltx_align_right" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.8.7.7.1" class="ltx_text" style="font-size:90%;">1.49</span></td>
</tr>
<tr id="S5.T2.2.9.8" class="ltx_tr">
<th id="S5.T2.2.9.8.1" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"></th>
<th id="S5.T2.2.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.2.1" class="ltx_text" style="font-size:90%;">More detailed</span></th>
<td id="S5.T2.2.9.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T2.2.9.8.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.4.1" class="ltx_text" style="font-size:90%;">70.53</span></td>
<td id="S5.T2.2.9.8.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.5.1" class="ltx_text" style="font-size:90%;">1278M</span></td>
<td id="S5.T2.2.9.8.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.6.1" class="ltx_text" style="font-size:90%;">10.17</span></td>
<td id="S5.T2.2.9.8.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding:1.6pt 5.5pt;"><span id="S5.T2.2.9.8.7.1" class="ltx_text" style="font-size:90%;">1.35</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Annotation statistics of <em id="S5.T2.7.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset.</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Semantic coverage.</h4>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">Our text annotations comprise various text types, addressing different levels of detail. To assess semantic coverage, we employ SpaCy&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib28" title="" class="ltx_ref">28</a>]</cite> for tokenization and parsing, inspired by DiHT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. This process yields part-of-speech (POS) tags and the dependency parsing tree among tokens. We establish heuristic rules based on POS tags, categorizing tokens into semantic element types, <em id="S5.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS2.SSS0.Px2.p1.1.2" class="ltx_text"></span>, objects, attributes, actions, and proper nouns. Additionally, we introduce the concept of <span id="S5.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_italic">token complexity</span>, measured by the total degrees of the token in the dependency parsing tree when treated as an undirected graph. This complexity reflects the richness of semantic connections. In our study, we focus on measuring the complexity of objects and actions.</p>
</div>
<div id="S5.SS2.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p2.2" class="ltx_p"><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.T3" title="In Semantic coverage. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">3</span></a> presents the statistics on the average number of semantic elements and their corresponding complexity. The results indicate that all measurements increase with the inclusion of more details in text annotations. Notably, average actions experience the most significant boost, with detailed and more detailed text exhibiting 7<math id="S5.SS2.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.SSS0.Px2.p2.1.m1.1a"><mo id="S5.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S5.SS2.SSS0.Px2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p2.1.m1.1b"><times id="S5.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p2.1.m1.1c">\times</annotation></semantics></math> and 15<math id="S5.SS2.SSS0.Px2.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.SSS0.Px2.p2.2.m2.1a"><mo id="S5.SS2.SSS0.Px2.p2.2.m2.1.1" xref="S5.SS2.SSS0.Px2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p2.2.m2.1b"><times id="S5.SS2.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS0.Px2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p2.2.m2.1c">\times</annotation></semantics></math> increases, respectively, compared to brief text. This highlights the limitations of traditional brief text annotations in describing image actions. Conversely, the increment in proper nouns is relatively low, potentially because specialists often describe objects more generally than using specific proper nouns. In terms of complexity measurements, both objects and actions show more semantic connections in detailed text annotations. The complexity of actions exhibits a higher improvement, aligning with our observation of the increasing number of actions.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.2.1.1" class="ltx_tr">
<th id="S5.T3.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Text Type</span></th>
<th id="S5.T3.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Brief</span></th>
<th id="S5.T3.2.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Detailed</span></th>
<th id="S5.T3.2.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.1.1.4.1" class="ltx_text" style="font-size:90%;">More detailed</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.2.2.1" class="ltx_tr">
<th id="S5.T3.2.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">#Image Annotations</span></th>
<td id="S5.T3.2.2.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.2.1.2.1" class="ltx_text" style="font-size:90%;">235M</span></td>
<td id="S5.T3.2.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.2.1.3.1" class="ltx_text" style="font-size:90%;">126M</span></td>
<td id="S5.T3.2.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.2.1.4.1" class="ltx_text" style="font-size:90%;">126M</span></td>
</tr>
<tr id="S5.T3.2.3.2" class="ltx_tr">
<th id="S5.T3.2.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.3.2.1.1" class="ltx_text" style="font-size:90%;">#Avg Tokens</span></th>
<td id="S5.T3.2.3.2.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.3.2.2.1" class="ltx_text" style="font-size:90%;">7.95</span></td>
<td id="S5.T3.2.3.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.3.2.3.1" class="ltx_text" style="font-size:90%;">31.65</span></td>
<td id="S5.T3.2.3.2.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.3.2.4.1" class="ltx_text" style="font-size:90%;">70.53</span></td>
</tr>
<tr id="S5.T3.2.4.3" class="ltx_tr">
<th id="S5.T3.2.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.4.3.1.1" class="ltx_text" style="font-size:90%;">#Avg Objects</span></th>
<td id="S5.T3.2.4.3.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.4.3.2.1" class="ltx_text" style="font-size:90%;">3.23</span></td>
<td id="S5.T3.2.4.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.4.3.3.1" class="ltx_text" style="font-size:90%;">13.31</span></td>
<td id="S5.T3.2.4.3.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.4.3.4.1" class="ltx_text" style="font-size:90%;">28.06</span></td>
</tr>
<tr id="S5.T3.2.5.4" class="ltx_tr">
<th id="S5.T3.2.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.5.4.1.1" class="ltx_text" style="font-size:90%;">#Avg Attributes</span></th>
<td id="S5.T3.2.5.4.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.5.4.2.1" class="ltx_text" style="font-size:90%;">2.80</span></td>
<td id="S5.T3.2.5.4.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.5.4.3.1" class="ltx_text" style="font-size:90%;">7.27</span></td>
<td id="S5.T3.2.5.4.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.5.4.4.1" class="ltx_text" style="font-size:90%;">16.25</span></td>
</tr>
<tr id="S5.T3.2.6.5" class="ltx_tr">
<th id="S5.T3.2.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.6.5.1.1" class="ltx_text" style="font-size:90%;">#Avg Actions</span></th>
<td id="S5.T3.2.6.5.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.6.5.2.1" class="ltx_text" style="font-size:90%;">0.58</span></td>
<td id="S5.T3.2.6.5.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.6.5.3.1" class="ltx_text" style="font-size:90%;">4.21</span></td>
<td id="S5.T3.2.6.5.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.6.5.4.1" class="ltx_text" style="font-size:90%;">8.76</span></td>
</tr>
<tr id="S5.T3.2.7.6" class="ltx_tr">
<th id="S5.T3.2.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.7.6.1.1" class="ltx_text" style="font-size:90%;">#Proper Nouns</span></th>
<td id="S5.T3.2.7.6.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.7.6.2.1" class="ltx_text" style="font-size:90%;">1.10</span></td>
<td id="S5.T3.2.7.6.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.7.6.3.1" class="ltx_text" style="font-size:90%;">2.40</span></td>
<td id="S5.T3.2.7.6.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.7.6.4.1" class="ltx_text" style="font-size:90%;">2.41</span></td>
</tr>
<tr id="S5.T3.2.8.7" class="ltx_tr">
<th id="S5.T3.2.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.8.7.1.1" class="ltx_text" style="font-size:90%;">Avg Object Complexity</span></th>
<td id="S5.T3.2.8.7.2" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.8.7.2.1" class="ltx_text" style="font-size:90%;">2.80</span></td>
<td id="S5.T3.2.8.7.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.8.7.3.1" class="ltx_text" style="font-size:90%;">4.00</span></td>
<td id="S5.T3.2.8.7.4" class="ltx_td ltx_align_right" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.8.7.4.1" class="ltx_text" style="font-size:90%;">4.02</span></td>
</tr>
<tr id="S5.T3.2.9.8" class="ltx_tr">
<th id="S5.T3.2.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.9.8.1.1" class="ltx_text" style="font-size:90%;">Avg Action Complexity</span></th>
<td id="S5.T3.2.9.8.2" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.9.8.2.1" class="ltx_text" style="font-size:90%;">1.14</span></td>
<td id="S5.T3.2.9.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.9.8.3.1" class="ltx_text" style="font-size:90%;">3.63</span></td>
<td id="S5.T3.2.9.8.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:5.1pt;padding-right:5.1pt;"><span id="S5.T3.2.9.8.4.1" class="ltx_text" style="font-size:90%;">4.38</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Statistics of the average number of semantic elements
and corresponding complexity in <em id="S5.T3.7.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset.</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Spatial coverage.</h4>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p1.1" class="ltx_p">Our region-text and text-phrase-region annotations, represented by bounding boxes and masks, capture the location of visual concepts within images. The distribution of box areas, as shown in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.F5.sf1" title="In Figure 5 ‣ Spatial coverage. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5(a)</span></a>, reveals more small boxes in region-text pairs and a uniform box size distribution in text-phrase-region triplets. This difference stems from the the divergent origins of these boxes: object detectors for region-text pairs and a grounding model for text-phrase-region triplets, which aligns boxes to textual phrases representing both localized and overarching image concepts. In&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.F5.sf2" title="In Figure 5 ‣ Spatial coverage. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">5(b)</span></a>, the log-format distribution of aspect ratios is illustrated. Region-text pairs and text-phrase-region triplets exhibit similar symmetric distributions, covering a wide range of aspect ratios. Heatmaps of the box center for each annotation type, shown in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.F5.sf3" title="In Figure 5 ‣ Spatial coverage. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures.</span>&nbsp;<span class="ltx_text ltx_ref_tag">5(c)</span></a> and&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S5.F5.sf4" title="Figure 5(d) ‣ Figure 5 ‣ Spatial coverage. ‣ 5.2 Data Analysis ‣ 5 Dataset ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(d)</span></a>, indicate a center bias, with region-text pairs displaying a more uniform distribution than text-phrase-region triplets.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x5.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="502" height="376" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x6.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="502" height="376" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x7.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_img_square" width="502" height="417" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x8.png" id="S5.F5.sf4.g1" class="ltx_graphics ltx_img_square" width="502" height="417" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text" style="font-size:90%;">Distributions of bounding boxes in <em id="S5.F5.4.2.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Experiments</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> models are trained on&nbsp;<em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">FLD-5B</em> to learn a universal image representation. We conduct our experiments in three main parts:
(1) We evaluate the <em id="S6.p1.1.3" class="ltx_emph ltx_font_bold ltx_font_italic">zero-shot</em> performance of our method on various tasks to show its inherent ability to handle multiple tasks without any extra fine-tuning on task-specific data using <em id="S6.p1.1.4" class="ltx_emph ltx_font_bold ltx_font_italic">one single generalist</em> model.
(2) We show the adaptability of our method by further training <em id="S6.p1.1.5" class="ltx_emph ltx_font_bold ltx_font_italic">one single generalist</em> model with additional supervised data on a wide range of tasks, achieving competitive state-of-the-art performance.
(3) We examine the performance of the learned visual representation on the downstream tasks as the backbone to show the superiority of our pre-training method over previous approaches.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Setup</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We investigate two model variants with different sizes: <em id="S6.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2-B</em> model with 232 million parameters and <em id="S6.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2-L</em> model with 771 million parameters. The detailed architectures of each model are given in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#A3.T15" title="In Appendix C Model Configuration ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">15</span></a>. We initialize the weights of the image encoder and multi-modality encoder-decoder from UniCL&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib87" title="" class="ltx_ref">87</a>]</cite> and BART&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, respectively.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.2" class="ltx_p">We adopt AdamW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib54" title="" class="ltx_ref">54</a>]</cite> with cosine learning rate decay&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib53" title="" class="ltx_ref">53</a>]</cite> for training our models.
We leverage Deepspeed&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib67" title="" class="ltx_ref">67</a>]</cite> and mixed precision to improve the training efficiency.
The maximum learning rate is set at <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="1e-4" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mrow id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml"><mn id="S6.SS1.p2.1.m1.1.1.2.2" xref="S6.SS1.p2.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.SS1.p2.1.m1.1.1.2.1" xref="S6.SS1.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p2.1.m1.1.1.2.3" xref="S6.SS1.p2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">−</mo><mn id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><minus id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1"></minus><apply id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2"><times id="S6.SS1.p2.1.m1.1.1.2.1.cmml" xref="S6.SS1.p2.1.m1.1.1.2.1"></times><cn type="integer" id="S6.SS1.p2.1.m1.1.1.2.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2.2">1</cn><ci id="S6.SS1.p2.1.m1.1.1.2.3.cmml" xref="S6.SS1.p2.1.m1.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">1e-4</annotation></semantics></math> for the base model and <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="1e-5" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><mrow id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mrow id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml"><mn id="S6.SS1.p2.2.m2.1.1.2.2" xref="S6.SS1.p2.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S6.SS1.p2.2.m2.1.1.2.1" xref="S6.SS1.p2.2.m2.1.1.2.1.cmml">​</mo><mi id="S6.SS1.p2.2.m2.1.1.2.3" xref="S6.SS1.p2.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S6.SS1.p2.2.m2.1.1.1" xref="S6.SS1.p2.2.m2.1.1.1.cmml">−</mo><mn id="S6.SS1.p2.2.m2.1.1.3" xref="S6.SS1.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><minus id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1.1"></minus><apply id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2"><times id="S6.SS1.p2.2.m2.1.1.2.1.cmml" xref="S6.SS1.p2.2.m2.1.1.2.1"></times><cn type="integer" id="S6.SS1.p2.2.m2.1.1.2.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2.2">1</cn><ci id="S6.SS1.p2.2.m2.1.1.2.3.cmml" xref="S6.SS1.p2.2.m2.1.1.2.3">𝑒</ci></apply><cn type="integer" id="S6.SS1.p2.2.m2.1.1.3.cmml" xref="S6.SS1.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">1e-5</annotation></semantics></math> for the large model.
A linear warm-up to the maximum learning rate is applied during the first 5,000 optimization steps.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.2" class="ltx_p">We train our models with a mini-batch size of 2048/3072 (base/large) and an image size of 384<math id="S6.SS1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><times id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\times</annotation></semantics></math>384 until reaching 3 billion effective training samples. Similar to&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib29" title="" class="ltx_ref">29</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, we further conduct high-resolution tuning with an image size of 768<math id="S6.SS1.p3.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS1.p3.2.m2.1a"><mo id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><times id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\times</annotation></semantics></math>768 for 0.5 billion samples for the base model and 0.1 billion samples for the large model.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Zero-shot Evaluation Across Tasks</h3>

<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T4.2.1.1" class="ltx_tr">
<th id="S6.T4.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;" rowspan="3"><span id="S6.T4.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S6.T4.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;" rowspan="3"><span id="S6.T4.2.1.1.2.1" class="ltx_text" style="font-size:90%;">#params</span></th>
<th id="S6.T4.2.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.4.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:48.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:48.5pt;transform:translate(-20.3pt,-19.43pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.4.1.1" class="ltx_p"><span id="S6.T4.2.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">COCO Cap.</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:30.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:30.8pt;transform:translate(-11.45pt,-10.58pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.6.1.1" class="ltx_p"><span id="S6.T4.2.1.1.6.1.1.1" class="ltx_text" style="font-size:90%;">NoCaps</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.7" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.8.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.9pt;height:37.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.6pt;transform:translate(-14.83pt,-13.95pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.8.1.1" class="ltx_p"><span id="S6.T4.2.1.1.8.1.1.1" class="ltx_text" style="font-size:90%;">TextCaps</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.9" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.10.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:46.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-20.36pt,-20.36pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.10.1.1" class="ltx_p"><span id="S6.T4.2.1.1.10.1.1.1" class="ltx_text" style="font-size:90%;">COCO Det.</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.11" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.12.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:36.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:36.7pt;transform:translate(-15.2pt,-15.2pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.12.1.1" class="ltx_p"><span id="S6.T4.2.1.1.12.1.1.1" class="ltx_text" style="font-size:90%;">Flickr30k</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.13" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.14" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="3">
<div id="S6.T4.2.1.1.14.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:30.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:30.6pt;transform:translate(-12.19pt,-12.19pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.14.1.1" class="ltx_p"><span id="S6.T4.2.1.1.14.1.1.1" class="ltx_text" style="font-size:90%;">Refcoco</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.15" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.16" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="3">
<div id="S6.T4.2.1.1.16.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:37.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.6pt;transform:translate(-15.31pt,-14.94pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.16.1.1" class="ltx_p"><span id="S6.T4.2.1.1.16.1.1.1" class="ltx_text" style="font-size:90%;">Refcoco+</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.17" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.18" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="2">
<div id="S6.T4.2.1.1.18.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:35.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.1pt;transform:translate(-13.56pt,-12.69pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.18.1.1" class="ltx_p"><span id="S6.T4.2.1.1.18.1.1.1" class="ltx_text" style="font-size:90%;">Refcocog</span></p>
</span></div>
</th>
<th id="S6.T4.2.1.1.19" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;"></th>
<th id="S6.T4.2.1.1.20" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.4pt;padding-right:2.4pt;">
<div id="S6.T4.2.1.1.20.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.3pt;height:51.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.4pt;transform:translate(-22.56pt,-22.56pt) rotate(-90deg) ;">
<p id="S6.T4.2.1.1.20.1.1" class="ltx_p"><span id="S6.T4.2.1.1.20.1.1.1" class="ltx_text" style="font-size:90%;">Refcoco RES</span></p>
</span></div>
</th>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<td id="S6.T4.2.2.2.1" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.2.1" class="ltx_text" style="font-size:90%;">test</span></td>
<td id="S6.T4.2.2.2.3" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.4.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T4.2.2.2.5" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.6.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T4.2.2.2.7" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.8.1" class="ltx_text" style="font-size:90%;">val2017</span></td>
<td id="S6.T4.2.2.2.9" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.10.1" class="ltx_text" style="font-size:90%;">test</span></td>
<td id="S6.T4.2.2.2.11" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.12.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T4.2.2.2.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.13.1" class="ltx_text" style="font-size:90%;">test-A</span></td>
<td id="S6.T4.2.2.2.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.14.1" class="ltx_text" style="font-size:90%;">test-B</span></td>
<td id="S6.T4.2.2.2.15" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.16.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T4.2.2.2.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.17.1" class="ltx_text" style="font-size:90%;">test-A</span></td>
<td id="S6.T4.2.2.2.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.18.1" class="ltx_text" style="font-size:90%;">test-B</span></td>
<td id="S6.T4.2.2.2.19" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.20.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T4.2.2.2.21" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.21.1" class="ltx_text" style="font-size:90%;">test</span></td>
<td id="S6.T4.2.2.2.22" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.2.2.23" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.2.2.23.1" class="ltx_text" style="font-size:90%;">val</span></td>
</tr>
<tr id="S6.T4.2.3.3" class="ltx_tr">
<td id="S6.T4.2.3.3.1" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.2" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.2.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T4.2.3.3.3" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.4" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.4.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T4.2.3.3.5" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.6" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.6.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T4.2.3.3.7" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.8" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.8.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
<td id="S6.T4.2.3.3.9" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.10" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.10.1" class="ltx_text" style="font-size:90%;">R@1</span></td>
<td id="S6.T4.2.3.3.11" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.12" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="3"><span id="S6.T4.2.3.3.12.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T4.2.3.3.13" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.14" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="3"><span id="S6.T4.2.3.3.14.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T4.2.3.3.15" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.16" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;" colspan="2"><span id="S6.T4.2.3.3.16.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T4.2.3.3.17" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.3.3.18" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.3.3.18.1" class="ltx_text" style="font-size:90%;">mIoU</span></td>
</tr>
<tr id="S6.T4.2.4.4" class="ltx_tr">
<th id="S6.T4.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S6.T4.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Flamingo&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.4.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a><span id="S6.T4.2.4.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T4.2.4.4.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.2.1" class="ltx_text" style="font-size:90%;">80B</span></th>
<td id="S6.T4.2.4.4.3" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.4.1" class="ltx_text" style="font-size:90%;">84.3</span></td>
<td id="S6.T4.2.4.4.5" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.7" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.9" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.11" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.12.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.13" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.14.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.15.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.16.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.17" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.18.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.19.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.20.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.21" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.22.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.23" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.23.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.4.4.24" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.4.4.25" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.4.4.25.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T4.2.5.5" class="ltx_tr">
<th id="S6.T4.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;">
<span id="S6.T4.2.5.5.1.1" class="ltx_text" style="font-size:90%;">Kosmos-2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T4.2.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib60" title="" class="ltx_ref">60</a><span id="S6.T4.2.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T4.2.5.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.2.1" class="ltx_text" style="font-size:90%;">1.6B</span></th>
<td id="S6.T4.2.5.5.3" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.5.5.5" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.6" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.5.5.7" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.8" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.5.5.9" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.10" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T4.2.5.5.11" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.12" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.12.1" class="ltx_text" style="font-size:90%;">78.7</span></td>
<td id="S6.T4.2.5.5.13" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.14" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.14.1" class="ltx_text" style="font-size:90%;">52.3</span></td>
<td id="S6.T4.2.5.5.15" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.15.1" class="ltx_text" style="font-size:90%;">57.4</span></td>
<td id="S6.T4.2.5.5.16" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.16.1" class="ltx_text" style="font-size:90%;">47.3</span></td>
<td id="S6.T4.2.5.5.17" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.18" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.18.1" class="ltx_text" style="font-size:90%;">45.5</span></td>
<td id="S6.T4.2.5.5.19" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.19.1" class="ltx_text" style="font-size:90%;">50.7</span></td>
<td id="S6.T4.2.5.5.20" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.20.1" class="ltx_text" style="font-size:90%;">42.2</span></td>
<td id="S6.T4.2.5.5.21" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.22" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.22.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S6.T4.2.5.5.23" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.23.1" class="ltx_text" style="font-size:90%;">61.7</span></td>
<td id="S6.T4.2.5.5.24" class="ltx_td" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.5.5.25" class="ltx_td ltx_align_center" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.5.5.25.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T4.2.6.6" class="ltx_tr">
<th id="S6.T4.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><em id="S6.T4.2.6.6.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-B</em></th>
<th id="S6.T4.2.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.2.1" class="ltx_text" style="font-size:90%;">0.23B</span></th>
<td id="S6.T4.2.6.6.3" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.4.1" class="ltx_text" style="font-size:90%;">133.0</span></td>
<td id="S6.T4.2.6.6.5" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.6.1" class="ltx_text" style="font-size:90%;">118.7</span></td>
<td id="S6.T4.2.6.6.7" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.8.1" class="ltx_text" style="font-size:90%;">70.1</span></td>
<td id="S6.T4.2.6.6.9" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.10.1" class="ltx_text" style="font-size:90%;">34.7</span></td>
<td id="S6.T4.2.6.6.11" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.12.1" class="ltx_text" style="font-size:90%;">83.6</span></td>
<td id="S6.T4.2.6.6.13" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.14.1" class="ltx_text" style="font-size:90%;">53.9</span></td>
<td id="S6.T4.2.6.6.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.15.1" class="ltx_text" style="font-size:90%;">58.4</span></td>
<td id="S6.T4.2.6.6.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.16.1" class="ltx_text" style="font-size:90%;">49.7</span></td>
<td id="S6.T4.2.6.6.17" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.18" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.18.1" class="ltx_text" style="font-size:90%;">51.5</span></td>
<td id="S6.T4.2.6.6.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.19.1" class="ltx_text" style="font-size:90%;">56.4</span></td>
<td id="S6.T4.2.6.6.20" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.20.1" class="ltx_text" style="font-size:90%;">47.9</span></td>
<td id="S6.T4.2.6.6.21" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.22" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.22.1" class="ltx_text" style="font-size:90%;">66.3</span></td>
<td id="S6.T4.2.6.6.23" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.23.1" class="ltx_text" style="font-size:90%;">65.1</span></td>
<td id="S6.T4.2.6.6.24" class="ltx_td ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.6.6.25" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.6.6.25.1" class="ltx_text" style="font-size:90%;">34.6</span></td>
</tr>
<tr id="S6.T4.2.7.7" class="ltx_tr">
<th id="S6.T4.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><em id="S6.T4.2.7.7.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-L</em></th>
<th id="S6.T4.2.7.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.2.1" class="ltx_text" style="font-size:90%;">0.77B</span></th>
<td id="S6.T4.2.7.7.3" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.4.1" class="ltx_text" style="font-size:90%;">135.6</span></td>
<td id="S6.T4.2.7.7.5" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.6.1" class="ltx_text" style="font-size:90%;">120.8</span></td>
<td id="S6.T4.2.7.7.7" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.8.1" class="ltx_text" style="font-size:90%;">72.8</span></td>
<td id="S6.T4.2.7.7.9" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.10.1" class="ltx_text" style="font-size:90%;">37.5</span></td>
<td id="S6.T4.2.7.7.11" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.12.1" class="ltx_text" style="font-size:90%;">84.4</span></td>
<td id="S6.T4.2.7.7.13" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.14" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.14.1" class="ltx_text" style="font-size:90%;">56.3</span></td>
<td id="S6.T4.2.7.7.15" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.15.1" class="ltx_text" style="font-size:90%;">61.6</span></td>
<td id="S6.T4.2.7.7.16" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.16.1" class="ltx_text" style="font-size:90%;">51.4</span></td>
<td id="S6.T4.2.7.7.17" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.18" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.18.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S6.T4.2.7.7.19" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.19.1" class="ltx_text" style="font-size:90%;">57.9</span></td>
<td id="S6.T4.2.7.7.20" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.20.1" class="ltx_text" style="font-size:90%;">49.9</span></td>
<td id="S6.T4.2.7.7.21" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.22" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.22.1" class="ltx_text" style="font-size:90%;">68.0</span></td>
<td id="S6.T4.2.7.7.23" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.23.1" class="ltx_text" style="font-size:90%;">67.0</span></td>
<td id="S6.T4.2.7.7.24" class="ltx_td ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"></td>
<td id="S6.T4.2.7.7.25" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.4pt;padding-right:2.4pt;"><span id="S6.T4.2.7.7.25.1" class="ltx_text" style="font-size:90%;">35.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S6.T4.10.1" class="ltx_text ltx_font_bold">Zero-shot</span> performance of generalist vision foundation models. The models do not see the training data of the evaluation tasks during training. <em id="S6.T4.11.2" class="ltx_emph ltx_font_italic">Florence-2</em> models are pre-trained on <em id="S6.T4.12.3" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset.
Karpathy test split is used for COCO caption evaluation. </figcaption>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">We present a powerful vision foundation model that does not require task-specific supervised annotations for fine-tuning.
The <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_bold">zero-shot</span> performance of our model is shown in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T4" title="In 6.2 Zero-shot Evaluation Across Tasks ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">4</span></a>.
For image-level tasks, <em id="S6.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2-L</em> achieves a 135.6 CIDEr score on the COCO caption benchmark&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, utilizing less than 1% of the parameters compared to the 80B Flamingo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>]</cite> model (which has an 84.3 CIDEr score). For region-level grounding and referring expression comprehension tasks, <em id="S6.SS2.p1.1.3" class="ltx_emph ltx_font_italic">Florence-2-L</em> establishes a new record in zero-shot performance achieving a 5.7 improvement in Flickr30k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib61" title="" class="ltx_ref">61</a>]</cite> Recall@1, and approximately 4%, 8%, and 8% absolute improvements on Refcoco, Refcoco+, and Refcocog&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, respectively, compared to the Kosmos-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib60" title="" class="ltx_ref">60</a>]</cite> model, which has 1.6B parameters.
Additionally, our pre-trained model attains a 35.8% mIOU in the Refcoco referring expression segmentation (RES)&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib94" title="" class="ltx_ref">94</a>]</cite> task, a capability not supported by prior foundation models.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Generalist Model with Public Supervised Data</h3>

<figure id="S6.T5" class="ltx_table">
<table id="S6.T5.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T5.6.7.1" class="ltx_tr">
<th id="S6.T5.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;" rowspan="3"><span id="S6.T5.6.7.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S6.T5.6.7.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;" rowspan="3"><span id="S6.T5.6.7.1.2.1" class="ltx_text" style="font-size:90%;">#params</span></th>
<th id="S6.T5.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.3.1" class="ltx_text" style="font-size:90%;">COCO Caption</span></th>
<th id="S6.T5.6.7.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.4.1" class="ltx_text" style="font-size:90%;">NoCaps</span></th>
<th id="S6.T5.6.7.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.5.1" class="ltx_text" style="font-size:90%;">TextCaps</span></th>
<th id="S6.T5.6.7.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.6.1" class="ltx_text" style="font-size:90%;">VQAv2</span></th>
<th id="S6.T5.6.7.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.7.1" class="ltx_text" style="font-size:90%;">TextVQA</span></th>
<th id="S6.T5.6.7.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.7.1.8.1" class="ltx_text" style="font-size:90%;">VizWiz VQA</span></th>
</tr>
<tr id="S6.T5.6.8.2" class="ltx_tr">
<td id="S6.T5.6.8.2.1" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.1.1" class="ltx_text" style="font-size:90%;">Karpathy test</span></td>
<td id="S6.T5.6.8.2.2" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.2.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T5.6.8.2.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.3.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T5.6.8.2.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.4.1" class="ltx_text" style="font-size:90%;">test-dev</span></td>
<td id="S6.T5.6.8.2.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.5.1" class="ltx_text" style="font-size:90%;">test-dev</span></td>
<td id="S6.T5.6.8.2.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.8.2.6.1" class="ltx_text" style="font-size:90%;">test-dev</span></td>
</tr>
<tr id="S6.T5.6.9.3" class="ltx_tr">
<td id="S6.T5.6.9.3.1" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.1.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T5.6.9.3.2" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.2.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T5.6.9.3.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.3.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T5.6.9.3.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.4.1" class="ltx_text" style="font-size:90%;">Acc</span></td>
<td id="S6.T5.6.9.3.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.5.1" class="ltx_text" style="font-size:90%;">Acc</span></td>
<td id="S6.T5.6.9.3.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.9.3.6.1" class="ltx_text" style="font-size:90%;">Acc</span></td>
</tr>
<tr id="S6.T5.6.10.4" class="ltx_tr">
<th id="S6.T5.6.10.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;" colspan="8"><span id="S6.T5.6.10.4.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Specialist Models</span></th>
</tr>
<tr id="S6.T5.6.11.5" class="ltx_tr">
<th id="S6.T5.6.11.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.11.5.1.1" class="ltx_text" style="font-size:90%;">CoCa&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.11.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib92" title="" class="ltx_ref">92</a><span id="S6.T5.6.11.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.11.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.2.1" class="ltx_text" style="font-size:90%;">2.1B</span></th>
<td id="S6.T5.6.11.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.3.1" class="ltx_text" style="font-size:90%;">143.6</span></td>
<td id="S6.T5.6.11.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.4.1" class="ltx_text" style="font-size:90%;">122.4</span></td>
<td id="S6.T5.6.11.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.11.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.6.1" class="ltx_text" style="font-size:90%;">82.3</span></td>
<td id="S6.T5.6.11.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.11.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.11.5.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T5.6.12.6" class="ltx_tr">
<th id="S6.T5.6.12.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.12.6.1.1" class="ltx_text" style="font-size:90%;">BLIP-2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.12.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib44" title="" class="ltx_ref">44</a><span id="S6.T5.6.12.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.12.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.2.1" class="ltx_text" style="font-size:90%;">7.8B</span></th>
<td id="S6.T5.6.12.6.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.3.1" class="ltx_text" style="font-size:90%;">144.5</span></td>
<td id="S6.T5.6.12.6.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.4.1" class="ltx_text" style="font-size:90%;">121.6</span></td>
<td id="S6.T5.6.12.6.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.12.6.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.6.1" class="ltx_text" style="font-size:90%;">82.2</span></td>
<td id="S6.T5.6.12.6.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.12.6.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.12.6.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T5.6.13.7" class="ltx_tr">
<th id="S6.T5.6.13.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.13.7.1.1" class="ltx_text" style="font-size:90%;">GIT2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.13.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib78" title="" class="ltx_ref">78</a><span id="S6.T5.6.13.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.13.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.2.1" class="ltx_text" style="font-size:90%;">5.1B</span></th>
<td id="S6.T5.6.13.7.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.3.1" class="ltx_text" style="font-size:90%;">145</span></td>
<td id="S6.T5.6.13.7.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.4.1" class="ltx_text" style="font-size:90%;">126.9</span></td>
<td id="S6.T5.6.13.7.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.5.1" class="ltx_text" style="font-size:90%;">148.6</span></td>
<td id="S6.T5.6.13.7.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.6.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
<td id="S6.T5.6.13.7.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.7.1" class="ltx_text" style="font-size:90%;">67.3</span></td>
<td id="S6.T5.6.13.7.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.13.7.8.1" class="ltx_text" style="font-size:90%;">71.0</span></td>
</tr>
<tr id="S6.T5.6.14.8" class="ltx_tr">
<th id="S6.T5.6.14.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.14.8.1.1" class="ltx_text" style="font-size:90%;">Flamingo&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.14.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a><span id="S6.T5.6.14.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.14.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.2.1" class="ltx_text" style="font-size:90%;">80B</span></th>
<td id="S6.T5.6.14.8.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.3.1" class="ltx_text" style="font-size:90%;">138.1</span></td>
<td id="S6.T5.6.14.8.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.14.8.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T5.6.14.8.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.6.1" class="ltx_text" style="font-size:90%;">82.0</span></td>
<td id="S6.T5.6.14.8.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.7.1" class="ltx_text" style="font-size:90%;">54.1</span></td>
<td id="S6.T5.6.14.8.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.14.8.8.1" class="ltx_text" style="font-size:90%;">65.7</span></td>
</tr>
<tr id="S6.T5.3.3" class="ltx_tr">
<th id="S6.T5.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.3.3.4.1" class="ltx_text" style="font-size:90%;">PaLI&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.3.3.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a><span id="S6.T5.3.3.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.3.3.5" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.3.3.5.1" class="ltx_text" style="font-size:90%;">17B</span></th>
<td id="S6.T5.3.3.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.3.3.6.1" class="ltx_text" style="font-size:90%;">149.1</span></td>
<td id="S6.T5.3.3.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.3.3.7.1" class="ltx_text" style="font-size:90%;">127.0</span></td>
<td id="S6.T5.1.1.1" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.1.1.1.1" class="ltx_text" style="font-size:90%;">160.0</span><sup id="S6.T5.1.1.1.2" class="ltx_sup"><span id="S6.T5.1.1.1.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
<td id="S6.T5.3.3.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.3.3.8.1" class="ltx_text" style="font-size:90%;">84.3</span></td>
<td id="S6.T5.2.2.2" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.2.2.2.1" class="ltx_text" style="font-size:90%;">58.8 / 73.1</span><sup id="S6.T5.2.2.2.2" class="ltx_sup"><span id="S6.T5.2.2.2.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
<td id="S6.T5.3.3.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.3.3.3.1" class="ltx_text" style="font-size:90%;">71.6 / 74.4</span><sup id="S6.T5.3.3.3.2" class="ltx_sup"><span id="S6.T5.3.3.3.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
</tr>
<tr id="S6.T5.6.6" class="ltx_tr">
<th id="S6.T5.6.6.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.6.4.1" class="ltx_text" style="font-size:90%;">PaLI-X&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.6.4.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib12" title="" class="ltx_ref">12</a><span id="S6.T5.6.6.4.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.6.5" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.6.5.1" class="ltx_text" style="font-size:90%;">55B</span></th>
<td id="S6.T5.6.6.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.6.6.1" class="ltx_text" style="font-size:90%;">149.2</span></td>
<td id="S6.T5.6.6.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.6.7.1" class="ltx_text" style="font-size:90%;">126.3</span></td>
<td id="S6.T5.4.4.1" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.4.4.1.1" class="ltx_text" style="font-size:90%;">147 / 163.7</span><sup id="S6.T5.4.4.1.2" class="ltx_sup"><span id="S6.T5.4.4.1.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
<td id="S6.T5.6.6.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.6.8.1" class="ltx_text" style="font-size:90%;">86.0</span></td>
<td id="S6.T5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.5.5.2.1" class="ltx_text" style="font-size:90%;">71.4 / 80.8</span><sup id="S6.T5.5.5.2.2" class="ltx_sup"><span id="S6.T5.5.5.2.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
<td id="S6.T5.6.6.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.6.3.1" class="ltx_text" style="font-size:90%;">70.9 / 74.6</span><sup id="S6.T5.6.6.3.2" class="ltx_sup"><span id="S6.T5.6.6.3.2.1" class="ltx_text" style="font-size:90%;">△</span></sup>
</td>
</tr>
<tr id="S6.T5.6.15.9" class="ltx_tr">
<th id="S6.T5.6.15.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;" colspan="8"><span id="S6.T5.6.15.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Generalist Models</span></th>
</tr>
<tr id="S6.T5.6.16.10" class="ltx_tr">
<th id="S6.T5.6.16.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;">
<span id="S6.T5.6.16.10.1.1" class="ltx_text" style="font-size:90%;">Unified-IO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T5.6.16.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a><span id="S6.T5.6.16.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T5.6.16.10.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.2.1" class="ltx_text" style="font-size:90%;">2.9B</span></th>
<th id="S6.T5.6.16.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.3.1" class="ltx_text" style="font-size:90%;">-</span></th>
<th id="S6.T5.6.16.10.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.4.1" class="ltx_text" style="font-size:90%;">100</span></th>
<th id="S6.T5.6.16.10.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.5.1" class="ltx_text" style="font-size:90%;">-</span></th>
<th id="S6.T5.6.16.10.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.6.1" class="ltx_text" style="font-size:90%;">77.9</span></th>
<th id="S6.T5.6.16.10.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.7.1" class="ltx_text" style="font-size:90%;">-</span></th>
<th id="S6.T5.6.16.10.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.16.10.8.1" class="ltx_text" style="font-size:90%;">57.4</span></th>
</tr>
<tr id="S6.T5.6.17.11" class="ltx_tr">
<th id="S6.T5.6.17.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><em id="S6.T5.6.17.11.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-B</em></th>
<th id="S6.T5.6.17.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.2.1" class="ltx_text" style="font-size:90%;">0.23B</span></th>
<td id="S6.T5.6.17.11.3" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.3.1" class="ltx_text" style="font-size:90%;">140.0</span></td>
<td id="S6.T5.6.17.11.4" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.4.1" class="ltx_text" style="font-size:90%;">116.7</span></td>
<td id="S6.T5.6.17.11.5" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.5.1" class="ltx_text" style="font-size:90%;">143.9</span></td>
<td id="S6.T5.6.17.11.6" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.6.1" class="ltx_text" style="font-size:90%;">79.7</span></td>
<td id="S6.T5.6.17.11.7" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.7.1" class="ltx_text" style="font-size:90%;">63.6</span></td>
<td id="S6.T5.6.17.11.8" class="ltx_td ltx_align_center" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.17.11.8.1" class="ltx_text" style="font-size:90%;">63.6</span></td>
</tr>
<tr id="S6.T5.6.18.12" class="ltx_tr">
<th id="S6.T5.6.18.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><em id="S6.T5.6.18.12.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-L</em></th>
<th id="S6.T5.6.18.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.2.1" class="ltx_text" style="font-size:90%;">0.77B</span></th>
<td id="S6.T5.6.18.12.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.3.1" class="ltx_text" style="font-size:90%;">143.3</span></td>
<td id="S6.T5.6.18.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.4.1" class="ltx_text" style="font-size:90%;">124.9</span></td>
<td id="S6.T5.6.18.12.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.5.1" class="ltx_text" style="font-size:90%;">151.1</span></td>
<td id="S6.T5.6.18.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.6.1" class="ltx_text" style="font-size:90%;">81.7</span></td>
<td id="S6.T5.6.18.12.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.7.1" class="ltx_text" style="font-size:90%;">73.5</span></td>
<td id="S6.T5.6.18.12.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:9.4pt;padding-right:9.4pt;"><span id="S6.T5.6.18.12.8.1" class="ltx_text" style="font-size:90%;">72.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance of specialist and generalist models on captioning and VQA tasks. <span id="S6.T5.18.1" class="ltx_text ltx_font_bold ltx_font_italic">Specialist Models</span> refer to those that are fine-tuned specifically for each task, while <span id="S6.T5.19.2" class="ltx_text ltx_font_bold ltx_font_italic">Generalist Models</span> denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks. <sup id="S6.T5.20.3" class="ltx_sup">△</sup> indicates usage of external OCR as input.</figcaption>
</figure>
<figure id="S6.T6" class="ltx_table">
<table id="S6.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T6.2.1.1" class="ltx_tr">
<th id="S6.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" rowspan="3"><span id="S6.T6.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Method</span></th>
<th id="S6.T6.2.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" rowspan="3"><span id="S6.T6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">#params</span></th>
<td id="S6.T6.2.1.1.3" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.1.1.4.1" class="ltx_text" style="font-size:90%;">COCO Det.</span></td>
<td id="S6.T6.2.1.1.5" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.1.1.6.1" class="ltx_text" style="font-size:90%;">Flickr30k</span></td>
<td id="S6.T6.2.1.1.7" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="3"><span id="S6.T6.2.1.1.8.1" class="ltx_text" style="font-size:90%;">Refcoco</span></td>
<td id="S6.T6.2.1.1.9" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="3"><span id="S6.T6.2.1.1.10.1" class="ltx_text" style="font-size:90%;">Refcoco+</span></td>
<td id="S6.T6.2.1.1.11" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="2"><span id="S6.T6.2.1.1.12.1" class="ltx_text" style="font-size:90%;">Refcocog</span></td>
<td id="S6.T6.2.1.1.13" class="ltx_td ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.1.1.14" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.1.1.14.1" class="ltx_text" style="font-size:90%;">Refcoco RES</span></td>
</tr>
<tr id="S6.T6.2.2.2" class="ltx_tr">
<td id="S6.T6.2.2.2.1" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.2.1" class="ltx_text" style="font-size:90%;">val2017</span></td>
<td id="S6.T6.2.2.2.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.4.1" class="ltx_text" style="font-size:90%;">test</span></td>
<td id="S6.T6.2.2.2.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.6.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T6.2.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.7.1" class="ltx_text" style="font-size:90%;">test-A</span></td>
<td id="S6.T6.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.8.1" class="ltx_text" style="font-size:90%;">test-B</span></td>
<td id="S6.T6.2.2.2.9" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.10.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T6.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.11.1" class="ltx_text" style="font-size:90%;">test-A</span></td>
<td id="S6.T6.2.2.2.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.12.1" class="ltx_text" style="font-size:90%;">test-B</span></td>
<td id="S6.T6.2.2.2.13" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.14.1" class="ltx_text" style="font-size:90%;">val</span></td>
<td id="S6.T6.2.2.2.15" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.15.1" class="ltx_text" style="font-size:90%;">test</span></td>
<td id="S6.T6.2.2.2.16" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.2.2.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.2.2.17.1" class="ltx_text" style="font-size:90%;">val</span></td>
</tr>
<tr id="S6.T6.2.3.3" class="ltx_tr">
<td id="S6.T6.2.3.3.1" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.2" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.3.3.2.1" class="ltx_text" style="font-size:90%;">mAP</span></td>
<td id="S6.T6.2.3.3.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.3.3.4.1" class="ltx_text" style="font-size:90%;">R@1</span></td>
<td id="S6.T6.2.3.3.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="3"><span id="S6.T6.2.3.3.6.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T6.2.3.3.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="3"><span id="S6.T6.2.3.3.8.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T6.2.3.3.9" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="2"><span id="S6.T6.2.3.3.10.1" class="ltx_text" style="font-size:90%;">Accuracy</span></td>
<td id="S6.T6.2.3.3.11" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.3.3.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.3.3.12.1" class="ltx_text" style="font-size:90%;">mIoU</span></td>
</tr>
<tr id="S6.T6.2.4.4" class="ltx_tr">
<th id="S6.T6.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="19"><span id="S6.T6.2.4.4.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Specialist Models</span></th>
</tr>
<tr id="S6.T6.2.5.5" class="ltx_tr">
<th id="S6.T6.2.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">
<span id="S6.T6.2.5.5.1.1" class="ltx_text" style="font-size:90%;">SeqTR&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.5.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib99" title="" class="ltx_ref">99</a><span id="S6.T6.2.5.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T6.2.5.5.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.2.1" class="ltx_text" style="font-size:90%;">-</span></th>
<td id="S6.T6.2.5.5.3" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.5.5.5" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.5.5.7" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.8.1" class="ltx_text" style="font-size:90%;">83.7</span></td>
<td id="S6.T6.2.5.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.9.1" class="ltx_text" style="font-size:90%;">86.5</span></td>
<td id="S6.T6.2.5.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.10.1" class="ltx_text" style="font-size:90%;">81.2</span></td>
<td id="S6.T6.2.5.5.11" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.12.1" class="ltx_text" style="font-size:90%;">71.5</span></td>
<td id="S6.T6.2.5.5.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.13.1" class="ltx_text" style="font-size:90%;">76.3</span></td>
<td id="S6.T6.2.5.5.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.14.1" class="ltx_text" style="font-size:90%;">64.9</span></td>
<td id="S6.T6.2.5.5.15" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.16.1" class="ltx_text" style="font-size:90%;">74.9</span></td>
<td id="S6.T6.2.5.5.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.17.1" class="ltx_text" style="font-size:90%;">74.2</span></td>
<td id="S6.T6.2.5.5.18" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.5.5.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.5.5.19.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T6.2.6.6" class="ltx_tr">
<th id="S6.T6.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;">
<span id="S6.T6.2.6.6.1.1" class="ltx_text" style="font-size:90%;">PolyFormer&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.6.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib49" title="" class="ltx_ref">49</a><span id="S6.T6.2.6.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T6.2.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.2.1" class="ltx_text" style="font-size:90%;">-</span></th>
<td id="S6.T6.2.6.6.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.6.6.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.6.6.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.8.1" class="ltx_text" style="font-size:90%;">90.4</span></td>
<td id="S6.T6.2.6.6.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.9.1" class="ltx_text" style="font-size:90%;">92.9</span></td>
<td id="S6.T6.2.6.6.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.10.1" class="ltx_text" style="font-size:90%;">87.2</span></td>
<td id="S6.T6.2.6.6.11" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.12.1" class="ltx_text" style="font-size:90%;">85.0</span></td>
<td id="S6.T6.2.6.6.13" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.13.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S6.T6.2.6.6.14" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.14.1" class="ltx_text" style="font-size:90%;">78.0</span></td>
<td id="S6.T6.2.6.6.15" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.16" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.16.1" class="ltx_text" style="font-size:90%;">85.8</span></td>
<td id="S6.T6.2.6.6.17" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.17.1" class="ltx_text" style="font-size:90%;">85.9</span></td>
<td id="S6.T6.2.6.6.18" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.6.6.19" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.6.6.19.1" class="ltx_text" style="font-size:90%;">76.9</span></td>
</tr>
<tr id="S6.T6.2.7.7" class="ltx_tr">
<th id="S6.T6.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;">
<span id="S6.T6.2.7.7.1.1" class="ltx_text" style="font-size:90%;">UNINEXT&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.7.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib84" title="" class="ltx_ref">84</a><span id="S6.T6.2.7.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T6.2.7.7.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.2.1" class="ltx_text" style="font-size:90%;">0.74B</span></th>
<td id="S6.T6.2.7.7.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.4.1" class="ltx_text" style="font-size:90%;">60.6</span></td>
<td id="S6.T6.2.7.7.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.7.7.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.8.1" class="ltx_text" style="font-size:90%;">92.6</span></td>
<td id="S6.T6.2.7.7.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.9.1" class="ltx_text" style="font-size:90%;">94.3</span></td>
<td id="S6.T6.2.7.7.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.10.1" class="ltx_text" style="font-size:90%;">91.5</span></td>
<td id="S6.T6.2.7.7.11" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.12.1" class="ltx_text" style="font-size:90%;">85.2</span></td>
<td id="S6.T6.2.7.7.13" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.13.1" class="ltx_text" style="font-size:90%;">89.6</span></td>
<td id="S6.T6.2.7.7.14" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.14.1" class="ltx_text" style="font-size:90%;">79.8</span></td>
<td id="S6.T6.2.7.7.15" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.16" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.16.1" class="ltx_text" style="font-size:90%;">88.7</span></td>
<td id="S6.T6.2.7.7.17" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.17.1" class="ltx_text" style="font-size:90%;">89.4</span></td>
<td id="S6.T6.2.7.7.18" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.7.7.19" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.7.7.19.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T6.2.8.8" class="ltx_tr">
<th id="S6.T6.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;">
<span id="S6.T6.2.8.8.1.1" class="ltx_text" style="font-size:90%;">Ferret&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.8.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib90" title="" class="ltx_ref">90</a><span id="S6.T6.2.8.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T6.2.8.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.2.1" class="ltx_text" style="font-size:90%;">13B</span></th>
<td id="S6.T6.2.8.8.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.8.8.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.8.8.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.8.1" class="ltx_text" style="font-size:90%;">89.5</span></td>
<td id="S6.T6.2.8.8.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.9.1" class="ltx_text" style="font-size:90%;">92.4</span></td>
<td id="S6.T6.2.8.8.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.10.1" class="ltx_text" style="font-size:90%;">84.4</span></td>
<td id="S6.T6.2.8.8.11" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.12.1" class="ltx_text" style="font-size:90%;">82.8</span></td>
<td id="S6.T6.2.8.8.13" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.13.1" class="ltx_text" style="font-size:90%;">88.1</span></td>
<td id="S6.T6.2.8.8.14" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.14.1" class="ltx_text" style="font-size:90%;">75.2</span></td>
<td id="S6.T6.2.8.8.15" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.16" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.16.1" class="ltx_text" style="font-size:90%;">85.8</span></td>
<td id="S6.T6.2.8.8.17" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.17.1" class="ltx_text" style="font-size:90%;">86.3</span></td>
<td id="S6.T6.2.8.8.18" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.8.8.19" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.8.8.19.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T6.2.9.9" class="ltx_tr">
<th id="S6.T6.2.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="19"><span id="S6.T6.2.9.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Generalist Models</span></th>
</tr>
<tr id="S6.T6.2.10.10" class="ltx_tr">
<th id="S6.T6.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;">
<span id="S6.T6.2.10.10.1.1" class="ltx_text" style="font-size:90%;">UniTAB&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T6.2.10.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib88" title="" class="ltx_ref">88</a><span id="S6.T6.2.10.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T6.2.10.10.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<td id="S6.T6.2.10.10.3" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.10.10.5" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T6.2.10.10.7" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.8.1" class="ltx_text" style="font-size:90%;">88.6</span></td>
<td id="S6.T6.2.10.10.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.9.1" class="ltx_text" style="font-size:90%;">91.1</span></td>
<td id="S6.T6.2.10.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.10.1" class="ltx_text" style="font-size:90%;">83.8</span></td>
<td id="S6.T6.2.10.10.11" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.12.1" class="ltx_text" style="font-size:90%;">81.0</span></td>
<td id="S6.T6.2.10.10.13" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.13.1" class="ltx_text" style="font-size:90%;">85.4</span></td>
<td id="S6.T6.2.10.10.14" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.14.1" class="ltx_text" style="font-size:90%;">71.6</span></td>
<td id="S6.T6.2.10.10.15" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.16" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.16.1" class="ltx_text" style="font-size:90%;">84.6</span></td>
<td id="S6.T6.2.10.10.17" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.17.1" class="ltx_text" style="font-size:90%;">84.7</span></td>
<td id="S6.T6.2.10.10.18" class="ltx_td ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.10.10.19" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.10.10.19.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T6.2.11.11" class="ltx_tr">
<th id="S6.T6.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><em id="S6.T6.2.11.11.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-B</em></th>
<th id="S6.T6.2.11.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.2.1" class="ltx_text" style="font-size:90%;">0.23B</span></th>
<td id="S6.T6.2.11.11.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.4.1" class="ltx_text" style="font-size:90%;">41.4</span></td>
<td id="S6.T6.2.11.11.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.6" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.6.1" class="ltx_text" style="font-size:90%;">84.0</span></td>
<td id="S6.T6.2.11.11.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.8" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.8.1" class="ltx_text" style="font-size:90%;">92.6</span></td>
<td id="S6.T6.2.11.11.9" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.9.1" class="ltx_text" style="font-size:90%;">94.8</span></td>
<td id="S6.T6.2.11.11.10" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.10.1" class="ltx_text" style="font-size:90%;">91.5</span></td>
<td id="S6.T6.2.11.11.11" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.12" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.12.1" class="ltx_text" style="font-size:90%;">86.8</span></td>
<td id="S6.T6.2.11.11.13" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.13.1" class="ltx_text" style="font-size:90%;">91.7</span></td>
<td id="S6.T6.2.11.11.14" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.14.1" class="ltx_text" style="font-size:90%;">82.2</span></td>
<td id="S6.T6.2.11.11.15" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.16" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.16.1" class="ltx_text" style="font-size:90%;">89.8</span></td>
<td id="S6.T6.2.11.11.17" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.17.1" class="ltx_text" style="font-size:90%;">82.2</span></td>
<td id="S6.T6.2.11.11.18" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.11.11.19" class="ltx_td ltx_align_center" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.11.11.19.1" class="ltx_text" style="font-size:90%;">78.0</span></td>
</tr>
<tr id="S6.T6.2.12.12" class="ltx_tr">
<th id="S6.T6.2.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><em id="S6.T6.2.12.12.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-L</em></th>
<th id="S6.T6.2.12.12.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.2.1" class="ltx_text" style="font-size:90%;">0.77B</span></th>
<td id="S6.T6.2.12.12.3" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.4.1" class="ltx_text" style="font-size:90%;">43.4</span></td>
<td id="S6.T6.2.12.12.5" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.6.1" class="ltx_text" style="font-size:90%;">85.2</span></td>
<td id="S6.T6.2.12.12.7" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.8.1" class="ltx_text" style="font-size:90%;">93.4</span></td>
<td id="S6.T6.2.12.12.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.9.1" class="ltx_text" style="font-size:90%;">95.3</span></td>
<td id="S6.T6.2.12.12.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.10.1" class="ltx_text" style="font-size:90%;">92.0</span></td>
<td id="S6.T6.2.12.12.11" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.12" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.12.1" class="ltx_text" style="font-size:90%;">88.3</span></td>
<td id="S6.T6.2.12.12.13" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.13.1" class="ltx_text" style="font-size:90%;">92.9</span></td>
<td id="S6.T6.2.12.12.14" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.14.1" class="ltx_text" style="font-size:90%;">83.6</span></td>
<td id="S6.T6.2.12.12.15" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.16" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.16.1" class="ltx_text" style="font-size:90%;">91.2</span></td>
<td id="S6.T6.2.12.12.17" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.17.1" class="ltx_text" style="font-size:90%;">91.7</span></td>
<td id="S6.T6.2.12.12.18" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T6.2.12.12.19" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T6.2.12.12.19.1" class="ltx_text" style="font-size:90%;">80.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>Performance of specialist and generalist models on region-level tasks. <span id="S6.T6.9.1" class="ltx_text ltx_font_bold ltx_font_italic">Specialist Models</span> refer to those that are fine-tuned specifically for each task, while <span id="S6.T6.10.2" class="ltx_text ltx_font_bold ltx_font_italic">Generalist Models</span> denote a single model fine-tuned in a task-agnostic manner, applicable across all tasks.</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">We demonstrate the versatility and effectiveness of our model as a vision foundation that can be transferred to various downstream tasks. We fine-tune <em id="S6.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> models by adding a collection of public datasets that cover image-level, region-level, pixel-level tasks, yielding <span id="S6.SS3.p1.1.2" class="ltx_text ltx_font_italic">one</span> generalist model for various vision tasks. The details of the dataset collection are provided in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#A2.T14" title="In Appendix B Supervised Data Collection for Generalist Model Fine-tuning ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">14</span></a>. &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T5" title="In 6.3 Generalist Model with Public Supervised Data ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span>&nbsp;<span class="ltx_text ltx_ref_tag">5</span></a> and&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T6" title="Table 6 ‣ 6.3 Generalist Model with Public Supervised Data ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compare our model with other state-of-the-art models. Our key findings are:</p>
</div>
<section id="S6.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Simple design for strong performance.</h4>

<div id="S6.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px1.p1.1" class="ltx_p"><em id="S6.SS3.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> demonstrates <em id="S6.SS3.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">strong</em> performance with <em id="S6.SS3.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">standard</em> multi-modality Transformer encoder-decoder without special designs, particularly for region-level and pixel-level tasks.
For example, <em id="S6.SS3.SSS0.Px1.p1.1.4" class="ltx_emph ltx_font_italic">Florence-2-L</em> outperforms PolyFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib49" title="" class="ltx_ref">49</a>]</cite> on both RefCOCO REC task and RES task by 3.0 Accuracy@0.5 and 3.54 mIOU respectively, where PolyFormer&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib49" title="" class="ltx_ref">49</a>]</cite> adapts specifically designed regression-based prediction head for coordinates. <em id="S6.SS3.SSS0.Px1.p1.1.5" class="ltx_emph ltx_font_italic">Florence-2-L</em> also outperforms previous SOTA method UNINEXT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib84" title="" class="ltx_ref">84</a>]</cite> on RefCOCO by 0.8 Accuracy@0.5, where UNINEXT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib84" title="" class="ltx_ref">84</a>]</cite> is based on advanced object detector Deformable DETR&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib100" title="" class="ltx_ref">100</a>]</cite> and DINO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite>.</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Competitive performance with fewer parameters.</h4>

<div id="S6.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px2.p1.1" class="ltx_p"><em id="S6.SS3.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2-L</em> achieves competitive performance without the need for LLMs, showcasing efficiency in handling diverse tasks while maintaining a compact size. For instance, <em id="S6.SS3.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2-L</em> attains a CIDEr score of 140.0 on the COCO Caption karpathy test split&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, outperforming models with significantly more parameters, such as Flamingo (80B parameters, 138.1 CIDEr score).</p>
</div>
</section>
<section id="S6.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Adaptable generalization across task levels.</h4>

<div id="S6.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS3.SSS0.Px3.p1.1" class="ltx_p"><em id="S6.SS3.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> demonstrates competitive performance across image-level, pixel-level, and region-level tasks, emphasizing its adaptability and effectiveness in addressing various challenges in computer vision and natural language processing. For example, in the TextVQA task, <em id="S6.SS3.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2-L</em> sets a new state-of-the-art performance with an accuracy of 81.5 without any external OCR token input, surpassing previous SOTA methods&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S6.SS3.SSS0.Px3.p2" class="ltx_para">
<p id="S6.SS3.SSS0.Px3.p2.1" class="ltx_p">These achievements emphasize <em id="S6.SS3.SSS0.Px3.p2.1.1" class="ltx_emph ltx_font_italic">Florence-2</em>’s efficiency in handling diverse tasks while maintaining a compact size, making it a unique and valuable asset in the ever-evolving landscape of AI research and applications.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Downstream Tasks Fine-tuning</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In this section, we investigate the performance of our single model fine-tuning on downstream tasks. This experiment highlights the superiority of <em id="S6.SS4.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-training over previous approaches, as it demonstrates the effectiveness of the learned universal image representation. We use the base size model with about 80M parameters in our experiments to ensure fair comparison with other methods.</p>
</div>
<figure id="S6.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x9.png" id="S6.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="528" height="401" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Mask-RCNN on COCO detection.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x10.png" id="S6.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="528" height="401" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">DINO on COCO detection.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S6.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x11.png" id="S6.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="528" height="401" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S6.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">UpperNet on ADE20K.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S6.F6.3.2" class="ltx_text" style="font-size:90%;">Training efficiency on COCO object detection and segmentation, and ADE20K semantic segmentation tasks.</span></figcaption>
</figure>
<section id="S6.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Object detection and segmentation.</h4>

<div id="S6.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p1.1" class="ltx_p">We conduct COCO object detection and instance segmentation&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite> experiments with Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and COCO object detection&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>]</cite> experiments with DINO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite> to further demonstrate the effectiveness of <em id="S6.SS4.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-training.
We train on the <em id="S6.SS4.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">train2017</em> split and evaluate on the <em id="S6.SS4.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">val2017</em> split.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p2.1" class="ltx_p">For Mask R-CNN&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib26" title="" class="ltx_ref">26</a>]</cite> experiments, we follow the common setup used in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite>, we use the standard 1<math id="S6.SS4.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px1.p2.1.m1.1a"><mo id="S6.SS4.SSS0.Px1.p2.1.m1.1.1" xref="S6.SS4.SSS0.Px1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p2.1.m1.1b"><times id="S6.SS4.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p2.1.m1.1c">\times</annotation></semantics></math> (12 epochs) schedule with multi-scale training for all experiments. The learning rate is stepped down by a factor of 0.1 at the 67% and 89% of training epochs.
We do not use any additional augmentation (such as random crop, mosaic, etc) or optimization techniques (such as EMA, weight normalization) during training to ensure a fair comparison. We do not use any test time augmentation (TTA) either.
Thanks to the strong universal representation learned by <em id="S6.SS4.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-training, we do not require longer training epochs, such as 36 epochs in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib86" title="" class="ltx_ref">86</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, or 100 epochs in&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, to achieve better results.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p3" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p3.1" class="ltx_p">For DINO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite> experiments, we train DINO-4scale&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib97" title="" class="ltx_ref">97</a>]</cite> detector for 12 epochs (1<math id="S6.SS4.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px1.p3.1.m1.1a"><mo id="S6.SS4.SSS0.Px1.p3.1.m1.1.1" xref="S6.SS4.SSS0.Px1.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p3.1.m1.1b"><times id="S6.SS4.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p3.1.m1.1c">\times</annotation></semantics></math>) using the same data augmentation strategy as employed by&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p4" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p4.3" class="ltx_p">First, our base model achieves a strong performance improvement compared to other approaches. As shown in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T7" title="In Object detection and segmentation. ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>, our DaViT-B model pre-trained by <em id="S6.SS4.SSS0.Px1.p4.3.1" class="ltx_emph ltx_font_italic">Florence-2</em> surpasses previous best base model (ConvNext v2-B), which is pre-trained by FCMAE&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, by 0.7 <math id="S6.SS4.SSS0.Px1.p4.1.m1.1" class="ltx_Math" alttext="AP_{b}" display="inline"><semantics id="S6.SS4.SSS0.Px1.p4.1.m1.1a"><mrow id="S6.SS4.SSS0.Px1.p4.1.m1.1.1" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.cmml"><mi id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.2" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.1" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.1.cmml">​</mo><msub id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.cmml"><mi id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.2" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.2.cmml">P</mi><mi id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.3" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.3.cmml">b</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p4.1.m1.1b"><apply id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1"><times id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.1.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.1"></times><ci id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.2.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.2">𝐴</ci><apply id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.1.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3">subscript</csymbol><ci id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.2.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.2">𝑃</ci><ci id="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.3.cmml" xref="S6.SS4.SSS0.Px1.p4.1.m1.1.1.3.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p4.1.m1.1c">AP_{b}</annotation></semantics></math> using Mask RCNN. Importantly, while ConvNeXt v2-B leverages a 3<math id="S6.SS4.SSS0.Px1.p4.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px1.p4.2.m2.1a"><mo id="S6.SS4.SSS0.Px1.p4.2.m2.1.1" xref="S6.SS4.SSS0.Px1.p4.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p4.2.m2.1b"><times id="S6.SS4.SSS0.Px1.p4.2.m2.1.1.cmml" xref="S6.SS4.SSS0.Px1.p4.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p4.2.m2.1c">\times</annotation></semantics></math> schedule (36 epochs), our model efficiently employs a 1<math id="S6.SS4.SSS0.Px1.p4.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px1.p4.3.m3.1a"><mo id="S6.SS4.SSS0.Px1.p4.3.m3.1.1" xref="S6.SS4.SSS0.Px1.p4.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px1.p4.3.m3.1b"><times id="S6.SS4.SSS0.Px1.p4.3.m3.1.1.cmml" xref="S6.SS4.SSS0.Px1.p4.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px1.p4.3.m3.1c">\times</annotation></semantics></math> schedule (12 epochs) thanks to our powerful pre-trained universal representation. For DINO framework, our model significantly outperforms the ViT-B, achieving a notable improvement of 4.2 AP.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p5" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p5.1" class="ltx_p">Second, our pre-training demonstrates higher training efficiency. As shown in <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T8" title="In Object detection and segmentation. ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a> and <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.F6" title="In 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">6</span></a>, compared to the model with supervised ImageNet-1k pre-training, our model with <em id="S6.SS4.SSS0.Px1.p5.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-training achieves 4x efficiency and a significant improvement of 6.9 AP and 5.5 AP with Mask-RCNN and DINO framework, respectively.</p>
</div>
<div id="S6.SS4.SSS0.Px1.p6" class="ltx_para">
<p id="S6.SS4.SSS0.Px1.p6.1" class="ltx_p">Third, our pre-training provides a good generic representation without extensive fine-tuning. &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T8" title="In Object detection and segmentation. ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a> indicates that the models with <em id="S6.SS4.SSS0.Px1.p6.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-training maintains competitive performances when the first two stages are frozen with only 0.3 and 0.2 drops for Mask-RCNN and DINO, respectively. Moreover, our approach with completely frozen backbone can outperform the model with supervised ImageNet-1k pre-training by 1.6 and 2.4 for Mask-RCNN and DINO.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T7.2.3.1" class="ltx_tr">
<th id="S6.T7.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.2.3.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.2.3.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2"><span id="S6.T7.2.3.1.4.1" class="ltx_text" style="font-size:90%;">Mask R-CNN</span></th>
<th id="S6.T7.2.3.1.5" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.3.1.6.1" class="ltx_text" style="font-size:90%;">DINO</span></th>
</tr>
<tr id="S6.T7.2.2" class="ltx_tr">
<th id="S6.T7.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.2.3.1" class="ltx_text" style="font-size:90%;">Backbone</span></th>
<th id="S6.T7.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.2.4.1" class="ltx_text" style="font-size:90%;">Pretrain</span></th>
<th id="S6.T7.2.2.5" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.1.1.1.1" class="ltx_text" style="font-size:90%;">AP</span><sub id="S6.T7.1.1.1.2" class="ltx_sub"><span id="S6.T7.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">b</span></sub>
</th>
<th id="S6.T7.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.2.2.1" class="ltx_text" style="font-size:90%;">AP</span><sub id="S6.T7.2.2.2.2" class="ltx_sub"><span id="S6.T7.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">m</span></sub>
</th>
<th id="S6.T7.2.2.6" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.0pt;padding-right:3.0pt;"></th>
<th id="S6.T7.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.2.7.1" class="ltx_text" style="font-size:90%;">AP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T7.2.4.1" class="ltx_tr">
<th id="S6.T7.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.4.1.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.4.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib46" title="" class="ltx_ref">46</a><span id="S6.T7.2.4.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.4.1.2.1" class="ltx_text" style="font-size:90%;">MAE, IN-1k</span></th>
<td id="S6.T7.2.4.1.3" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.4.1.4.1" class="ltx_text" style="font-size:90%;">51.6</span></td>
<td id="S6.T7.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.4.1.5.1" class="ltx_text" style="font-size:90%;">45.9</span></td>
<td id="S6.T7.2.4.1.6" class="ltx_td ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.4.1.7.1" class="ltx_text" style="font-size:90%;">55.0</span></td>
</tr>
<tr id="S6.T7.2.5.2" class="ltx_tr">
<th id="S6.T7.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.5.2.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.5.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a><span id="S6.T7.2.5.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.5.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.5.2.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T7.2.5.2.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.5.2.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.5.2.4.1" class="ltx_text" style="font-size:90%;">50.2</span></td>
<td id="S6.T7.2.5.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.5.2.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T7.2.5.2.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.5.2.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.5.2.7.1" class="ltx_text" style="font-size:90%;">53.4</span></td>
</tr>
<tr id="S6.T7.2.6.3" class="ltx_tr">
<th id="S6.T7.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.6.3.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.6.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a><span id="S6.T7.2.6.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.6.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.6.3.2.1" class="ltx_text" style="font-size:90%;">SimMIM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.6.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib83" title="" class="ltx_ref">83</a><span id="S6.T7.2.6.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T7.2.6.3.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.6.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.6.3.4.1" class="ltx_text" style="font-size:90%;">52.3</span></td>
<td id="S6.T7.2.6.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.6.3.5.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T7.2.6.3.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.6.3.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.6.3.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T7.2.7.4" class="ltx_tr">
<th id="S6.T7.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.7.4.1.1" class="ltx_text" style="font-size:90%;">FocalAtt-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.7.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib86" title="" class="ltx_ref">86</a><span id="S6.T7.2.7.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.7.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.7.4.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T7.2.7.4.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.7.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.7.4.4.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S6.T7.2.7.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.7.4.5.1" class="ltx_text" style="font-size:90%;">43.7</span></td>
<td id="S6.T7.2.7.4.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.7.4.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.7.4.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T7.2.8.5" class="ltx_tr">
<th id="S6.T7.2.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.8.5.1.1" class="ltx_text" style="font-size:90%;">FocalNet-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.8.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib85" title="" class="ltx_ref">85</a><span id="S6.T7.2.8.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.8.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.8.5.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T7.2.8.5.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.8.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.8.5.4.1" class="ltx_text" style="font-size:90%;">49.8</span></td>
<td id="S6.T7.2.8.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.8.5.5.1" class="ltx_text" style="font-size:90%;">44.1</span></td>
<td id="S6.T7.2.8.5.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.8.5.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.8.5.7.1" class="ltx_text" style="font-size:90%;">54.4</span></td>
</tr>
<tr id="S6.T7.2.9.6" class="ltx_tr">
<th id="S6.T7.2.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.9.6.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v1-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.9.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib52" title="" class="ltx_ref">52</a><span id="S6.T7.2.9.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.9.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.9.6.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T7.2.9.6.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.9.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.9.6.4.1" class="ltx_text" style="font-size:90%;">50.3</span></td>
<td id="S6.T7.2.9.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.9.6.5.1" class="ltx_text" style="font-size:90%;">44.9</span></td>
<td id="S6.T7.2.9.6.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.9.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.9.6.7.1" class="ltx_text" style="font-size:90%;">52.6</span></td>
</tr>
<tr id="S6.T7.2.10.7" class="ltx_tr">
<th id="S6.T7.2.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.10.7.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v2-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.10.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a><span id="S6.T7.2.10.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.10.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.10.7.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T7.2.10.7.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.10.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.10.7.4.1" class="ltx_text" style="font-size:90%;">51.0</span></td>
<td id="S6.T7.2.10.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.10.7.5.1" class="ltx_text" style="font-size:90%;">45.6</span></td>
<td id="S6.T7.2.10.7.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.10.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.10.7.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T7.2.11.8" class="ltx_tr">
<th id="S6.T7.2.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S6.T7.2.11.8.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v2-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T7.2.11.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a><span id="S6.T7.2.11.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T7.2.11.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.11.8.2.1" class="ltx_text" style="font-size:90%;">FCMAE</span></th>
<td id="S6.T7.2.11.8.3" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.11.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.11.8.4.1" class="ltx_text" style="font-size:90%;">52.9</span></td>
<td id="S6.T7.2.11.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.11.8.5.1" class="ltx_text" style="font-size:90%;">46.6</span></td>
<td id="S6.T7.2.11.8.6" class="ltx_td" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.11.8.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.11.8.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T7.2.12.9" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S6.T7.2.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.12.9.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">DaViT-B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></th>
<th id="S6.T7.2.12.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><em id="S6.T7.2.12.9.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th>
<td id="S6.T7.2.12.9.3" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.12.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.12.9.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">53.6</span></td>
<td id="S6.T7.2.12.9.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.12.9.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">46.4</span></td>
<td id="S6.T7.2.12.9.6" class="ltx_td ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T7.2.12.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T7.2.12.9.7.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">59.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="S6.T7.18.1" class="ltx_text ltx_font_bold">COCO object detection and instance segmentation results</span> using Mask-RCNN framework, and <span id="S6.T7.19.2" class="ltx_text ltx_font_bold">COCO object detection results</span> using DINO-4scale framework. All the entries use a base size model to ensure a fair comparison. For Mask-RCNN experiments, our method utilizes 1<math id="S6.T7.6.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T7.6.m1.1b"><mo id="S6.T7.6.m1.1.1" xref="S6.T7.6.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T7.6.m1.1c"><times id="S6.T7.6.m1.1.1.cmml" xref="S6.T7.6.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.6.m1.1d">\times</annotation></semantics></math> schedule (12 epochs), ViT-B use 100 epochs, all others use 3<math id="S6.T7.7.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T7.7.m2.1b"><mo id="S6.T7.7.m2.1.1" xref="S6.T7.7.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T7.7.m2.1c"><times id="S6.T7.7.m2.1.1.cmml" xref="S6.T7.7.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.7.m2.1d">\times</annotation></semantics></math> (36 epochs). For DINO experiments, all the entries use 1<math id="S6.T7.8.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T7.8.m3.1b"><mo id="S6.T7.8.m3.1.1" xref="S6.T7.8.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T7.8.m3.1c"><times id="S6.T7.8.m3.1.1.cmml" xref="S6.T7.8.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.8.m3.1d">\times</annotation></semantics></math> schedule except for ViT-B which uses 50 epochs.</figcaption>
</figure>
<figure id="S6.T8" class="ltx_table">
<table id="S6.T8.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T8.2.3.1" class="ltx_tr">
<th id="S6.T8.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" rowspan="2"><span id="S6.T8.2.3.1.1.1" class="ltx_text" style="font-size:90%;">Pretrain</span></th>
<th id="S6.T8.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" rowspan="2"><span id="S6.T8.2.3.1.2.1" class="ltx_text" style="font-size:90%;">Frozen stages</span></th>
<td id="S6.T8.2.3.1.3" class="ltx_td ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.3.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;" colspan="2"><span id="S6.T8.2.3.1.4.1" class="ltx_text" style="font-size:90%;">Mask R-CNN</span></td>
<td id="S6.T8.2.3.1.5" class="ltx_td ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.3.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.3.1.6.1" class="ltx_text" style="font-size:90%;">DINO</span></td>
<td id="S6.T8.2.3.1.7" class="ltx_td ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.3.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.3.1.8.1" class="ltx_text" style="font-size:90%;">UperNet</span></td>
</tr>
<tr id="S6.T8.2.2" class="ltx_tr">
<td id="S6.T8.2.2.3" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S6.T8.1.1.1.1" class="ltx_text" style="font-size:90%;">AP</span><sub id="S6.T8.1.1.1.2" class="ltx_sub"><span id="S6.T8.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">b</span></sub>
</td>
<td id="S6.T8.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S6.T8.2.2.2.1" class="ltx_text" style="font-size:90%;">AP</span><sub id="S6.T8.2.2.2.2" class="ltx_sub"><span id="S6.T8.2.2.2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">m</span></sub>
</td>
<td id="S6.T8.2.2.4" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.2.5.1" class="ltx_text" style="font-size:90%;">AP</span></td>
<td id="S6.T8.2.2.6" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.2.7.1" class="ltx_text" style="font-size:90%;">mIoU</span></td>
</tr>
<tr id="S6.T8.2.4.2" class="ltx_tr">
<th id="S6.T8.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.1.1" class="ltx_text" style="font-size:90%;">Sup IN1k</span></th>
<th id="S6.T8.2.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.2.1" class="ltx_text" style="font-size:90%;">n/a</span></th>
<td id="S6.T8.2.4.2.3" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.4.1" class="ltx_text" style="font-size:90%;">46.7</span></td>
<td id="S6.T8.2.4.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.5.1" class="ltx_text" style="font-size:90%;">42.0</span></td>
<td id="S6.T8.2.4.2.6" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.7.1" class="ltx_text" style="font-size:90%;">53.7</span></td>
<td id="S6.T8.2.4.2.8" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.4.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.4.2.9.1" class="ltx_text" style="font-size:90%;">49</span></td>
</tr>
<tr id="S6.T8.2.5.3" class="ltx_tr">
<th id="S6.T8.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="S6.T8.2.5.3.1.1" class="ltx_text" style="font-size:90%;">UniCL&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T8.2.5.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib87" title="" class="ltx_ref">87</a><span id="S6.T8.2.5.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T8.2.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.5.3.2.1" class="ltx_text" style="font-size:90%;">n/a</span></th>
<td id="S6.T8.2.5.3.3" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.5.3.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.5.3.4.1" class="ltx_text" style="font-size:90%;">50.4</span></td>
<td id="S6.T8.2.5.3.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.5.3.5.1" class="ltx_text" style="font-size:90%;">45.0</span></td>
<td id="S6.T8.2.5.3.6" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.5.3.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.5.3.7.1" class="ltx_text" style="font-size:90%;">57.3</span></td>
<td id="S6.T8.2.5.3.8" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.5.3.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.5.3.9.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
</tr>
<tr id="S6.T8.2.6.4" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S6.T8.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><em id="S6.T8.2.6.4.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th>
<th id="S6.T8.2.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.6.4.2.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">n/a</span></th>
<td id="S6.T8.2.6.4.3" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.6.4.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.6.4.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">53.6</span></td>
<td id="S6.T8.2.6.4.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.6.4.5.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">46.4</span></td>
<td id="S6.T8.2.6.4.6" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.6.4.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.6.4.7.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">59.2</span></td>
<td id="S6.T8.2.6.4.8" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.6.4.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.6.4.9.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">54.9</span></td>
</tr>
<tr id="S6.T8.2.7.5" class="ltx_tr">
<th id="S6.T8.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><em id="S6.T8.2.7.5.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2</em></th>
<th id="S6.T8.2.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.7.5.2.1" class="ltx_text" style="font-size:90%;">[1]</span></th>
<td id="S6.T8.2.7.5.3" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.7.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.7.5.4.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S6.T8.2.7.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.7.5.5.1" class="ltx_text" style="font-size:90%;">46.3</span></td>
<td id="S6.T8.2.7.5.6" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.7.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.7.5.7.1" class="ltx_text" style="font-size:90%;">59.2</span></td>
<td id="S6.T8.2.7.5.8" class="ltx_td ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.7.5.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.7.5.9.1" class="ltx_text" style="font-size:90%;">54.1</span></td>
</tr>
<tr id="S6.T8.2.8.6" class="ltx_tr">
<th id="S6.T8.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><em id="S6.T8.2.8.6.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2</em></th>
<th id="S6.T8.2.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.8.6.2.1" class="ltx_text" style="font-size:90%;">[1, 2]</span></th>
<td id="S6.T8.2.8.6.3" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.8.6.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.8.6.4.1" class="ltx_text" style="font-size:90%;">53.3</span></td>
<td id="S6.T8.2.8.6.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.8.6.5.1" class="ltx_text" style="font-size:90%;">46.1</span></td>
<td id="S6.T8.2.8.6.6" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.8.6.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.8.6.7.1" class="ltx_text" style="font-size:90%;">59.0</span></td>
<td id="S6.T8.2.8.6.8" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.8.6.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.8.6.9.1" class="ltx_text" style="font-size:90%;">54.4</span></td>
</tr>
<tr id="S6.T8.2.9.7" class="ltx_tr">
<th id="S6.T8.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><em id="S6.T8.2.9.7.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2</em></th>
<th id="S6.T8.2.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.9.7.2.1" class="ltx_text" style="font-size:90%;">[1, 2, 3]</span></th>
<td id="S6.T8.2.9.7.3" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.9.7.4" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.9.7.4.1" class="ltx_text" style="font-size:90%;">49.5</span></td>
<td id="S6.T8.2.9.7.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.9.7.5.1" class="ltx_text" style="font-size:90%;">42.9</span></td>
<td id="S6.T8.2.9.7.6" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.9.7.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.9.7.7.1" class="ltx_text" style="font-size:90%;">56.7</span></td>
<td id="S6.T8.2.9.7.8" class="ltx_td" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.9.7.9" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.9.7.9.1" class="ltx_text" style="font-size:90%;">49.6</span></td>
</tr>
<tr id="S6.T8.2.10.8" class="ltx_tr">
<th id="S6.T8.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><em id="S6.T8.2.10.8.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2</em></th>
<th id="S6.T8.2.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.10.8.2.1" class="ltx_text" style="font-size:90%;">[1, 2, 3, 4]</span></th>
<td id="S6.T8.2.10.8.3" class="ltx_td ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.10.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.10.8.4.1" class="ltx_text" style="font-size:90%;">48.3</span></td>
<td id="S6.T8.2.10.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.10.8.5.1" class="ltx_text" style="font-size:90%;">44.5</span></td>
<td id="S6.T8.2.10.8.6" class="ltx_td ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.10.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.10.8.7.1" class="ltx_text" style="font-size:90%;">56.1</span></td>
<td id="S6.T8.2.10.8.8" class="ltx_td ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"></td>
<td id="S6.T8.2.10.8.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S6.T8.2.10.8.9.1" class="ltx_text" style="font-size:90%;">45.9</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Downstream task fine-tuning on COCO and ADE20K dataset. <span id="S6.T8.13.1" class="ltx_text ltx_font_bold">COCO object detection</span> using Mask R-CNN and DINO. <span id="S6.T8.14.2" class="ltx_text ltx_font_bold">ADE20K semantic segmentation</span> using UperNet. All entries use DaViT-B with 80M parameters as the backbone and standard 1<math id="S6.T8.4.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.T8.4.m1.1b"><mo id="S6.T8.4.m1.1.1" xref="S6.T8.4.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.T8.4.m1.1c"><times id="S6.T8.4.m1.1.1.cmml" xref="S6.T8.4.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.T8.4.m1.1d">\times</annotation></semantics></math> schedule. </figcaption>
</figure>
</section>
<section id="S6.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Semantic segmentation.</h4>

<div id="S6.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS4.SSS0.Px2.p1.1" class="ltx_p">We conduct semantic segmentation experiments with UperNet&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib82" title="" class="ltx_ref">82</a>]</cite> framework on ADE20k&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib98" title="" class="ltx_ref">98</a>]</cite> dataset.
We mostly follow the training and evaluation protocols from Swin&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. Specifically, we use input size 512<math id="S6.SS4.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px2.p1.1.m1.1a"><mo id="S6.SS4.SSS0.Px2.p1.1.m1.1.1" xref="S6.SS4.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px2.p1.1.m1.1b"><times id="S6.SS4.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>512 and train the model for 40k iterations with a batch size of 64. We adopt the AdamW&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib54" title="" class="ltx_ref">54</a>]</cite> optimizer with the optimal learning rate searched from {8e-4,4e-4,2e-4,1e-4}.</p>
</div>
<div id="S6.SS4.SSS0.Px2.p2" class="ltx_para">
<p id="S6.SS4.SSS0.Px2.p2.1" class="ltx_p">Our results show a similar trend to the object detection experiments. As illustrated in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T9" title="In Semantic segmentation. ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">9</span></a>, our base model outperforms the previous SoTA model, which is BEiT pre-trained ViT-B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, by 1.3 and 1.4 points in single-scale and multi-scale testing protocol, respectively. With the same backbone architecture of DaViT-B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, <em id="S6.SS4.SSS0.Px2.p2.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> pre-trained model achieves a remarkable improvement of 4.9 points and 4<math id="S6.SS4.SSS0.Px2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS4.SSS0.Px2.p2.1.m1.1a"><mo id="S6.SS4.SSS0.Px2.p2.1.m1.1.1" xref="S6.SS4.SSS0.Px2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS0.Px2.p2.1.m1.1b"><times id="S6.SS4.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S6.SS4.SSS0.Px2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS0.Px2.p2.1.m1.1c">\times</annotation></semantics></math> efficiency compared to the ImageNet-1k pre-trained counterpart as demonstrated in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T8" title="In Object detection and segmentation. ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tables</span>&nbsp;<span class="ltx_text ltx_ref_tag">8</span></a> and&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.F6" title="Figure 6 ‣ 6.4 Downstream Tasks Fine-tuning ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S6.T9" class="ltx_table">
<table id="S6.T9.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T9.6.1.1" class="ltx_tr">
<th id="S6.T9.6.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Backbone</span></th>
<th id="S6.T9.6.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.1.1.2.1" class="ltx_text" style="font-size:90%;">Pretrain</span></th>
<th id="S6.T9.6.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.1.1.3.1" class="ltx_text" style="font-size:90%;">mIoU</span></th>
<th id="S6.T9.6.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.1.1.4.1" class="ltx_text" style="font-size:90%;">ms-mIoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T9.6.2.1" class="ltx_tr">
<th id="S6.T9.6.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.2.1.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.2.1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib24" title="" class="ltx_ref">24</a><span id="S6.T9.6.2.1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.2.1.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.2.1.3.1" class="ltx_text" style="font-size:90%;">47.4</span></td>
<td id="S6.T9.6.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.2.1.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T9.6.3.2" class="ltx_tr">
<th id="S6.T9.6.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.3.2.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.3.2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib24" title="" class="ltx_ref">24</a><span id="S6.T9.6.3.2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.3.2.2.1" class="ltx_text" style="font-size:90%;">MAE IN-1k</span></th>
<td id="S6.T9.6.3.2.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.3.2.3.1" class="ltx_text" style="font-size:90%;">48.1</span></td>
<td id="S6.T9.6.3.2.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.3.2.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T9.6.4.3" class="ltx_tr">
<th id="S6.T9.6.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.4.3.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.4.3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib4" title="" class="ltx_ref">4</a><span id="S6.T9.6.4.3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.4.3.2.1" class="ltx_text" style="font-size:90%;">BEiT</span></th>
<td id="S6.T9.6.4.3.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.4.3.3.1" class="ltx_text" style="font-size:90%;">53.6</span></td>
<td id="S6.T9.6.4.3.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.4.3.4.1" class="ltx_text" style="font-size:90%;">54.1</span></td>
</tr>
<tr id="S6.T9.6.5.4" class="ltx_tr">
<th id="S6.T9.6.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.5.4.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.5.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib59" title="" class="ltx_ref">59</a><span id="S6.T9.6.5.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.5.4.2.1" class="ltx_text" style="font-size:90%;">BEiTv2 IN-1k</span></th>
<td id="S6.T9.6.5.4.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.5.4.3.1" class="ltx_text" style="font-size:90%;">53.1</span></td>
<td id="S6.T9.6.5.4.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.5.4.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T9.6.6.5" class="ltx_tr">
<th id="S6.T9.6.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.6.5.1.1" class="ltx_text" style="font-size:90%;">ViT-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.6.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib59" title="" class="ltx_ref">59</a><span id="S6.T9.6.6.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.6.5.2.1" class="ltx_text" style="font-size:90%;">BEiTv2 IN-22k</span></th>
<td id="S6.T9.6.6.5.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.6.5.3.1" class="ltx_text" style="font-size:90%;">53.5</span></td>
<td id="S6.T9.6.6.5.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.6.5.4.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="S6.T9.6.7.6" class="ltx_tr">
<th id="S6.T9.6.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.7.6.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.7.6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a><span id="S6.T9.6.7.6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.7.6.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.7.6.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.7.6.3.1" class="ltx_text" style="font-size:90%;">48.1</span></td>
<td id="S6.T9.6.7.6.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.7.6.4.1" class="ltx_text" style="font-size:90%;">49.7</span></td>
</tr>
<tr id="S6.T9.6.8.7" class="ltx_tr">
<th id="S6.T9.6.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.8.7.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.8.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a><span id="S6.T9.6.8.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.8.7.2.1" class="ltx_text" style="font-size:90%;">Sup IN-22k</span></th>
<td id="S6.T9.6.8.7.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.8.7.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T9.6.8.7.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.8.7.4.1" class="ltx_text" style="font-size:90%;">51.8</span></td>
</tr>
<tr id="S6.T9.6.9.8" class="ltx_tr">
<th id="S6.T9.6.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.9.8.1.1" class="ltx_text" style="font-size:90%;">Swin-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.9.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib51" title="" class="ltx_ref">51</a><span id="S6.T9.6.9.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.9.8.2.1" class="ltx_text" style="font-size:90%;">SimMIM&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.9.8.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib83" title="" class="ltx_ref">83</a><span id="S6.T9.6.9.8.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<td id="S6.T9.6.9.8.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.9.8.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T9.6.9.8.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.9.8.4.1" class="ltx_text" style="font-size:90%;">52.8</span></td>
</tr>
<tr id="S6.T9.6.10.9" class="ltx_tr">
<th id="S6.T9.6.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.10.9.1.1" class="ltx_text" style="font-size:90%;">FocalAtt-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.10.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib86" title="" class="ltx_ref">86</a><span id="S6.T9.6.10.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.10.9.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.10.9.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.10.9.3.1" class="ltx_text" style="font-size:90%;">49.0</span></td>
<td id="S6.T9.6.10.9.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.10.9.4.1" class="ltx_text" style="font-size:90%;">50.5</span></td>
</tr>
<tr id="S6.T9.6.11.10" class="ltx_tr">
<th id="S6.T9.6.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.11.10.1.1" class="ltx_text" style="font-size:90%;">FocalNet-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.11.10.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib85" title="" class="ltx_ref">85</a><span id="S6.T9.6.11.10.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.11.10.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.11.10.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.11.10.3.1" class="ltx_text" style="font-size:90%;">50.5</span></td>
<td id="S6.T9.6.11.10.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.11.10.4.1" class="ltx_text" style="font-size:90%;">51.4</span></td>
</tr>
<tr id="S6.T9.6.12.11" class="ltx_tr">
<th id="S6.T9.6.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.12.11.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v1-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.12.11.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib52" title="" class="ltx_ref">52</a><span id="S6.T9.6.12.11.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.12.11.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.12.11.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.12.11.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T9.6.12.11.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.12.11.4.1" class="ltx_text" style="font-size:90%;">49.9</span></td>
</tr>
<tr id="S6.T9.6.13.12" class="ltx_tr">
<th id="S6.T9.6.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.13.12.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v2-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.13.12.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a><span id="S6.T9.6.13.12.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.13.12.2.1" class="ltx_text" style="font-size:90%;">Sup IN-1k</span></th>
<td id="S6.T9.6.13.12.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.13.12.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T9.6.13.12.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.13.12.4.1" class="ltx_text" style="font-size:90%;">50.5</span></td>
</tr>
<tr id="S6.T9.6.14.13" class="ltx_tr">
<th id="S6.T9.6.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;">
<span id="S6.T9.6.14.13.1.1" class="ltx_text" style="font-size:90%;">ConvNeXt v2-B&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S6.T9.6.14.13.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib81" title="" class="ltx_ref">81</a><span id="S6.T9.6.14.13.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</th>
<th id="S6.T9.6.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.14.13.2.1" class="ltx_text" style="font-size:90%;">FCMAE</span></th>
<td id="S6.T9.6.14.13.3" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.14.13.3.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="S6.T9.6.14.13.4" class="ltx_td ltx_align_center" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.14.13.4.1" class="ltx_text" style="font-size:90%;">52.1</span></td>
</tr>
<tr id="S6.T9.6.15.14" class="ltx_tr" style="background-color:#E6E6E6;">
<th id="S6.T9.6.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.15.14.1.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">DaViT-B&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span></th>
<th id="S6.T9.6.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:5.5pt;padding-right:5.5pt;"><em id="S6.T9.6.15.14.2.1" class="ltx_emph ltx_font_italic" style="font-size:90%;background-color:#E6E6E6;">Florence-2</em></th>
<td id="S6.T9.6.15.14.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.15.14.3.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">54.9</span></td>
<td id="S6.T9.6.15.14.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.5pt;padding-right:5.5pt;"><span id="S6.T9.6.15.14.4.1" class="ltx_text" style="font-size:90%;background-color:#E6E6E6;">55.5</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="S6.T9.12.1" class="ltx_text ltx_font_bold">ADE20K semantic segmentation results</span> using UperNet. The input size is <math id="S6.T9.3.m1.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S6.T9.3.m1.1b"><mrow id="S6.T9.3.m1.1.1" xref="S6.T9.3.m1.1.1.cmml"><mn id="S6.T9.3.m1.1.1.2" xref="S6.T9.3.m1.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T9.3.m1.1.1.1" xref="S6.T9.3.m1.1.1.1.cmml">×</mo><mn id="S6.T9.3.m1.1.1.3" xref="S6.T9.3.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T9.3.m1.1c"><apply id="S6.T9.3.m1.1.1.cmml" xref="S6.T9.3.m1.1.1"><times id="S6.T9.3.m1.1.1.1.cmml" xref="S6.T9.3.m1.1.1.1"></times><cn type="integer" id="S6.T9.3.m1.1.1.2.cmml" xref="S6.T9.3.m1.1.1.2">512</cn><cn type="integer" id="S6.T9.3.m1.1.1.3.cmml" xref="S6.T9.3.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T9.3.m1.1d">512\times 512</annotation></semantics></math> for all the entries, except for models with BEiT pre-trained, which use the input size of <math id="S6.T9.4.m2.1" class="ltx_Math" alttext="640\times 640" display="inline"><semantics id="S6.T9.4.m2.1b"><mrow id="S6.T9.4.m2.1.1" xref="S6.T9.4.m2.1.1.cmml"><mn id="S6.T9.4.m2.1.1.2" xref="S6.T9.4.m2.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="S6.T9.4.m2.1.1.1" xref="S6.T9.4.m2.1.1.1.cmml">×</mo><mn id="S6.T9.4.m2.1.1.3" xref="S6.T9.4.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.T9.4.m2.1c"><apply id="S6.T9.4.m2.1.1.cmml" xref="S6.T9.4.m2.1.1"><times id="S6.T9.4.m2.1.1.1.cmml" xref="S6.T9.4.m2.1.1.1"></times><cn type="integer" id="S6.T9.4.m2.1.1.2.cmml" xref="S6.T9.4.m2.1.1.2">640</cn><cn type="integer" id="S6.T9.4.m2.1.1.3.cmml" xref="S6.T9.4.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T9.4.m2.1d">640\times 640</annotation></semantics></math>.</figcaption>
</figure>
</section>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Ablation Studies</h3>

<figure id="S6.F7" class="ltx_figure"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x12.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F7.4.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S6.F7.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Multitask transfer.<span id="S6.F7.5.2.1" class="ltx_text ltx_font_medium"> We conduct experiments with three different versions of <em id="S6.F7.5.2.1.1" class="ltx_emph ltx_font_italic">Florence-2</em> models, each trained on a different level of image annotation: image level, image and region level, and image, region, and pixel level. We then evaluate the transfer learning performance of these models on four downstream tasks: COCO caption, COCO object detection, Flickr30k grounding, and Refcoco referring segmentation.</span></span></figcaption>
</figure>
<section id="S6.SS5.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multitask transfer.</h4>

<div id="S6.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p1.1" class="ltx_p">In this study, we aimed to identify the most effective pre-trained model for transfer learning across various downstream tasks in computer vision. We compared three different models, each pre-trained on a different combination of tasks:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Image-level Model: pre-trained on image-level tasks only</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">Image-Region Model: pre-trained on image-level and region-level tasks</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">Image-Region-Pixel Model: pre-trained on image-level, region-level, and pixel-level tasks</p>
</div>
</li>
</ul>
</div>
<div id="S6.SS5.SSS0.Px1.p2" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p2.1" class="ltx_p">For pre-training, we optimize all models for the same number of effective samples (72M) on a subset of our <em id="S6.SS5.SSS0.Px1.p2.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset.</p>
</div>
<div id="S6.SS5.SSS0.Px1.p3" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p3.1" class="ltx_p">These models are then transferred to a combined dataset with four downstream tasks, each representing a different level of task granularity: COCO caption (image-level task), COCO object detection (region-level task), Flickr30k grounding (region-level task), RefCOCO referring segmentation (pixel-level task).</p>
</div>
<div id="S6.SS5.SSS0.Px1.p4" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p4.1" class="ltx_p">The results are shown in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.F7" title="In 6.5 Ablation Studies ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span>&nbsp;<span class="ltx_text ltx_ref_tag">7</span></a>. The results demonstrate that Image-Region-Pixel Model, pre-trained on all three levels of tasks, consistently demonstrated competitive performance across the four downstream tasks.</p>
</div>
<div id="S6.SS5.SSS0.Px1.p5" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p5.1" class="ltx_p">For the COCO caption task, Image-Region-Pixel Model initially performs worse than Image-level Model and Image-Region Model but eventually achieve a final performance (133.4 CIDEr) that is only slightly worse than the other models (134.6 CIDEr).</p>
</div>
<div id="S6.SS5.SSS0.Px1.p6" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p6.1" class="ltx_p">For the COCO object detection task, Image-Region-Pixel Model outperforms Image-level Model by a significant margin (28.3 vs. 0.1) and was only slightly worse than Image-Region Model (29.7).</p>
</div>
<div id="S6.SS5.SSS0.Px1.p7" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p7.1" class="ltx_p">For the Flickr30k grounding task, Image-Region-Pixel Model shows strong performance (78.1 recall@1), comparable to Image-Region Model (79.1 recall@1) and significantly better than Image-level Model (62.0 recall@1).</p>
</div>
<div id="S6.SS5.SSS0.Px1.p8" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p8.1" class="ltx_p">For the RefCOCO referring segmentation task, Image-Region-Pixel Model clearly outperforms both Image-level Model and Image-Region Model, achieving the highest performance (31.6 mIoU) compared to the other models (28.4 and 18.2 mIoU).</p>
</div>
<div id="S6.SS5.SSS0.Px1.p9" class="ltx_para">
<p id="S6.SS5.SSS0.Px1.p9.1" class="ltx_p">Our findings suggest that the Image-Region-Pixel Model, which is pre-trained on tasks at the image, region, and pixel levels, is the most effective base model for transfer learning across various computer vision tasks. This model shows strong performance on all four downstream tasks we evaluated, and consistently outperforms the Image-level Model and matches or exceeds the Image-Region Model in performance. By pre-training a model on tasks at different levels of granularity, we can ensure that the base model is better prepared to handle a diverse range of downstream tasks, offering a versatile and robust solution for transfer learning in computer vision.</p>
</div>
</section>
<section id="S6.SS5.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model scaling.</h4>

<div id="S6.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS5.SSS0.Px2.p1.1" class="ltx_p">We aimed to investigate the impact of increasing model capacity on zero-shot performance on various downstream tasks in computer vision. We compared two models:
<em id="S6.SS5.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Florence-2-B</em> and <em id="S6.SS5.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2-L</em>, which have 232M and 771M parameters, respectively. The model architectures are described in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#A3.T15" title="In Appendix C Model Configuration ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">15</span></a>.
We show the zero-shot performance on four downstream tasks in&nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T10" title="In Model scaling. ‣ 6.5 Ablation Studies ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">10</span></a>. The large model clearly outperforms the base model across various downstream tasks.</p>
</div>
<figure id="S6.T10" class="ltx_table">
<table id="S6.T10.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T10.2.1.1" class="ltx_tr">
<th id="S6.T10.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" rowspan="2"><span id="S6.T10.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="S6.T10.2.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Caption</span></th>
<th id="S6.T10.2.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.1.1.5.1" class="ltx_text" style="font-size:90%;">Detection</span></th>
<th id="S6.T10.2.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.1.1.7.1" class="ltx_text" style="font-size:90%;">Grounding</span></th>
<th id="S6.T10.2.1.1.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.3pt;padding-right:3.3pt;" colspan="2"><span id="S6.T10.2.1.1.9.1" class="ltx_text" style="font-size:90%;">RES</span></th>
</tr>
<tr id="S6.T10.2.2.2" class="ltx_tr">
<td id="S6.T10.2.2.2.1" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.2.2.2.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T10.2.2.2.3" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.2.2.4.1" class="ltx_text" style="font-size:90%;">AP</span></td>
<td id="S6.T10.2.2.2.5" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.2.2.6.1" class="ltx_text" style="font-size:90%;">Recall@1</span></td>
<td id="S6.T10.2.2.2.7" class="ltx_td" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.2.2.8.1" class="ltx_text" style="font-size:90%;">mIOU</span></td>
<td id="S6.T10.2.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.2.2.9.1" class="ltx_text" style="font-size:90%;">oIOU</span></td>
</tr>
<tr id="S6.T10.2.3.3" class="ltx_tr">
<th id="S6.T10.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.1.1" class="ltx_text" style="font-size:90%;">Base</span></th>
<th id="S6.T10.2.3.3.2" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.3.1" class="ltx_text" style="font-size:90%;">118.7</span></th>
<th id="S6.T10.2.3.3.4" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.5.1" class="ltx_text" style="font-size:90%;">19.7</span></th>
<th id="S6.T10.2.3.3.6" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.7.1" class="ltx_text" style="font-size:90%;">76.3</span></th>
<th id="S6.T10.2.3.3.8" class="ltx_td ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"></th>
<th id="S6.T10.2.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.9.1" class="ltx_text" style="font-size:90%;">18.6</span></th>
<th id="S6.T10.2.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.3.3.10.1" class="ltx_text" style="font-size:90%;">17.8</span></th>
</tr>
<tr id="S6.T10.2.4.4" class="ltx_tr">
<th id="S6.T10.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.1.1" class="ltx_text" style="font-size:90%;">Large</span></th>
<td id="S6.T10.2.4.4.2" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">124.4</span></td>
<td id="S6.T10.2.4.4.4" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.4.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">22.6</span></td>
<td id="S6.T10.2.4.4.6" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.4.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">78.2</span></td>
<td id="S6.T10.2.4.4.8" class="ltx_td ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"></td>
<td id="S6.T10.2.4.4.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">21.5</span></td>
<td id="S6.T10.2.4.4.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.3pt;padding-right:3.3pt;"><span id="S6.T10.2.4.4.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 10: </span><span id="S6.T10.6.1" class="ltx_text ltx_font_bold">Model scaling.</span> Zero-shot performance on COCO caption and COCO object detection, Flickr30k grounding, RefCOCO referring expression segmentation(RES).</figcaption>
</figure>
</section>
<section id="S6.SS5.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data scaling.</h4>

<div id="S6.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p1.1" class="ltx_p">We conducted experiments to study how zero-shot performance on various computer vision tasks is affected by the scale of pre-training data.
We used four different data sizes for pre-training: 0.12M, 0.36M, 1.2M, and 12M images. All models were trained with the same effective sample size (72M) on a subset of <em id="S6.SS5.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em> data.</p>
</div>
<figure id="S6.T11" class="ltx_table">
<table id="S6.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T11.2.1.1" class="ltx_tr">
<th id="S6.T11.2.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Data</span></th>
<th id="S6.T11.2.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Caption</span></th>
<th id="S6.T11.2.1.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.1.1.5.1" class="ltx_text" style="font-size:90%;">Detection</span></th>
<th id="S6.T11.2.1.1.6" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.1.1.7.1" class="ltx_text" style="font-size:90%;">Grounding</span></th>
<th id="S6.T11.2.1.1.8" class="ltx_td ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.1pt;padding-right:3.1pt;" colspan="2"><span id="S6.T11.2.1.1.9.1" class="ltx_text" style="font-size:90%;">RES</span></th>
</tr>
<tr id="S6.T11.2.2.2" class="ltx_tr">
<th id="S6.T11.2.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.1.1" class="ltx_text" style="font-size:90%;">size</span></th>
<th id="S6.T11.2.2.2.2" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.3.1" class="ltx_text" style="font-size:90%;">CIDEr</span></th>
<th id="S6.T11.2.2.2.4" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.5.1" class="ltx_text" style="font-size:90%;">AP</span></th>
<th id="S6.T11.2.2.2.6" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.7.1" class="ltx_text" style="font-size:90%;">Recall@1</span></th>
<th id="S6.T11.2.2.2.8" class="ltx_td ltx_th ltx_th_column" style="padding-left:3.1pt;padding-right:3.1pt;"></th>
<th id="S6.T11.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.9.1" class="ltx_text" style="font-size:90%;">mIOU</span></th>
<th id="S6.T11.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.2.2.10.1" class="ltx_text" style="font-size:90%;">oIOU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T11.2.3.1" class="ltx_tr">
<th id="S6.T11.2.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.1.1" class="ltx_text" style="font-size:90%;">0.12M</span></th>
<td id="S6.T11.2.3.1.2" class="ltx_td ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.3.1" class="ltx_text" style="font-size:90%;">102.8</span></td>
<td id="S6.T11.2.3.1.4" class="ltx_td ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.5.1" class="ltx_text" style="font-size:90%;">16.1</span></td>
<td id="S6.T11.2.3.1.6" class="ltx_td ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.7.1" class="ltx_text" style="font-size:90%;">74.0</span></td>
<td id="S6.T11.2.3.1.8" class="ltx_td ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.9.1" class="ltx_text" style="font-size:90%;">15.9</span></td>
<td id="S6.T11.2.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.3.1.10.1" class="ltx_text" style="font-size:90%;">16.6</span></td>
</tr>
<tr id="S6.T11.2.4.2" class="ltx_tr">
<th id="S6.T11.2.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.1.1" class="ltx_text" style="font-size:90%;">0.36M</span></th>
<td id="S6.T11.2.4.2.2" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.4.2.3" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.3.1" class="ltx_text" style="font-size:90%;">114.3</span></td>
<td id="S6.T11.2.4.2.4" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.4.2.5" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.5.1" class="ltx_text" style="font-size:90%;">18.7</span></td>
<td id="S6.T11.2.4.2.6" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.4.2.7" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.7.1" class="ltx_text" style="font-size:90%;">75.8</span></td>
<td id="S6.T11.2.4.2.8" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.4.2.9" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.9.1" class="ltx_text" style="font-size:90%;">16.6</span></td>
<td id="S6.T11.2.4.2.10" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.4.2.10.1" class="ltx_text" style="font-size:90%;">16.4</span></td>
</tr>
<tr id="S6.T11.2.5.3" class="ltx_tr">
<th id="S6.T11.2.5.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.1.1" class="ltx_text" style="font-size:90%;">1.2M</span></th>
<td id="S6.T11.2.5.3.2" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.5.3.3" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.3.1" class="ltx_text" style="font-size:90%;">118.1</span></td>
<td id="S6.T11.2.5.3.4" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.5.3.5" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.5.1" class="ltx_text" style="font-size:90%;">18.9</span></td>
<td id="S6.T11.2.5.3.6" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.5.3.7" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.7.1" class="ltx_text" style="font-size:90%;">76.3</span></td>
<td id="S6.T11.2.5.3.8" class="ltx_td" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.5.3.9" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.3</span></td>
<td id="S6.T11.2.5.3.10" class="ltx_td ltx_align_center" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.5.3.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">18.4</span></td>
</tr>
<tr id="S6.T11.2.6.4" class="ltx_tr">
<th id="S6.T11.2.6.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.1.1" class="ltx_text" style="font-size:90%;">12M</span></th>
<td id="S6.T11.2.6.4.2" class="ltx_td ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.6.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">118.7</span></td>
<td id="S6.T11.2.6.4.4" class="ltx_td ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.6.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.7</span></td>
<td id="S6.T11.2.6.4.6" class="ltx_td ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.6.4.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">76.3</span></td>
<td id="S6.T11.2.6.4.8" class="ltx_td ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"></td>
<td id="S6.T11.2.6.4.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.9.1" class="ltx_text" style="font-size:90%;">18.6</span></td>
<td id="S6.T11.2.6.4.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.1pt;padding-right:3.1pt;"><span id="S6.T11.2.6.4.10.1" class="ltx_text" style="font-size:90%;">17.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 11: </span><span id="S6.T11.6.1" class="ltx_text ltx_font_bold">Data scaling.</span> Zero-shot performance on COCO caption, COCO object detection, Flickr30k grounding, COCORef referring segmentation.</figcaption>
</figure>
<div id="S6.SS5.SSS0.Px3.p2" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p2.1" class="ltx_p"><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T11" title="In Data scaling. ‣ 6.5 Ablation Studies ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">11</span></a> presents the zero-shot performance results on COCO caption, COCO object detection, Flickr30k grounding, and RefCoco referring segmentation (RES) tasks.
We can observe a trend of improved zero-shot performance on the downstream tasks as the pre-training data size increases (except for RES, 1.2M data has slightly better performance compared to 12M).</p>
</div>
<div id="S6.SS5.SSS0.Px3.p3" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p3.1" class="ltx_p">Our experiments on data scaling demonstrate that larger pre-training data sizes generally lead to improved zero-shot performance across a variety of downstream tasks in computer vision. This finding suggests that investing in larger pre-training datasets can provide a more effective and versatile foundation for handling a wide range of downstream tasks.</p>
</div>
<div id="S6.SS5.SSS0.Px3.p4" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p4.1" class="ltx_p">Our approach to scaling data is significantly more efficient than relying solely on human annotations, as most of the annotation generation is performed using model inference. By leveraging specialist models to generate annotations, we can substantially reduce the time and cost associated with manual annotation efforts, which often involve labor-intensive processes and may be subject to human errors or inconsistencies.</p>
</div>
<div id="S6.SS5.SSS0.Px3.p5" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p5.1" class="ltx_p">Furthermore, utilizing model-generated annotations enables us to scale the pre-training datasets more rapidly and efficiently, allowing us to explore the impact of larger data sizes on model performance across various downstream tasks in computer vision. This not only facilitates the development of more effective and versatile foundation models but also ensures that the annotation process remains sustainable and scalable as the need for high-quality labeled data continues to grow.</p>
</div>
<div id="S6.SS5.SSS0.Px3.p6" class="ltx_para">
<p id="S6.SS5.SSS0.Px3.p6.1" class="ltx_p">In summary, our data scaling approach offers a more efficient alternative to traditional human annotation methods by harnessing the power of specialist models for annotation generation. This strategy enables us to accelerate the pre-training process, optimize model performance, and effectively manage the ever-increasing demand for labeled data in the field of computer vision.</p>
</div>
</section>
<section id="S6.SS5.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training settings.</h4>

<div id="S6.SS5.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS5.SSS0.Px4.p1.1" class="ltx_p">We analyze the basic model training settings for the two primary components of our model, namely the vision encoder and the multi-modality encoder-decoder. The experiment results are presented in &nbsp;<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#S6.T12" title="In Training settings. ‣ 6.5 Ablation Studies ‣ 6 Experiments ‣ Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span>&nbsp;<span class="ltx_text ltx_ref_tag">12</span></a></p>
</div>
<figure id="S6.T12" class="ltx_table">
<table id="S6.T12.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T12.2.1.1" class="ltx_tr">
<th id="S6.T12.2.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<th id="S6.T12.2.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<td id="S6.T12.2.1.1.3" class="ltx_td ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.1.1.4.1" class="ltx_text" style="font-size:90%;">Caption</span></td>
<td id="S6.T12.2.1.1.5" class="ltx_td ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.1.1.6.1" class="ltx_text" style="font-size:90%;">Detection</span></td>
<td id="S6.T12.2.1.1.7" class="ltx_td ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.1.1.8.1" class="ltx_text" style="font-size:90%;">Grounding</span></td>
<td id="S6.T12.2.1.1.9" class="ltx_td ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.1.1.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.1pt;padding-right:2.1pt;" colspan="2"><span id="S6.T12.2.1.1.10.1" class="ltx_text" style="font-size:90%;">RES</span></td>
</tr>
<tr id="S6.T12.2.2.2" class="ltx_tr">
<th id="S6.T12.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.1.1" class="ltx_text" style="font-size:90%;">V Pre</span></th>
<th id="S6.T12.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.2.1" class="ltx_text" style="font-size:90%;">L Pre</span></th>
<td id="S6.T12.2.2.2.3" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.4.1" class="ltx_text" style="font-size:90%;">CIDEr</span></td>
<td id="S6.T12.2.2.2.5" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.6.1" class="ltx_text" style="font-size:90%;">AP</span></td>
<td id="S6.T12.2.2.2.7" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.8.1" class="ltx_text" style="font-size:90%;">Recall@1</span></td>
<td id="S6.T12.2.2.2.9" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.10.1" class="ltx_text" style="font-size:90%;">mIOU</span></td>
<td id="S6.T12.2.2.2.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.2.2.11.1" class="ltx_text" style="font-size:90%;">oIOU</span></td>
</tr>
<tr id="S6.T12.2.3.3" class="ltx_tr">
<th id="S6.T12.2.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;" colspan="11"><span id="S6.T12.2.3.3.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Freeze Vision Encoder</span></th>
</tr>
<tr id="S6.T12.2.4.4" class="ltx_tr">
<th id="S6.T12.2.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<th id="S6.T12.2.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.2.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S6.T12.2.4.4.3" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.4.1" class="ltx_text" style="font-size:90%;">120.0</span></td>
<td id="S6.T12.2.4.4.5" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.6.1" class="ltx_text" style="font-size:90%;">6.9</span></td>
<td id="S6.T12.2.4.4.7" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.8.1" class="ltx_text" style="font-size:90%;">66.3</span></td>
<td id="S6.T12.2.4.4.9" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.4.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.10.1" class="ltx_text" style="font-size:90%;">9.9</span></td>
<td id="S6.T12.2.4.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.4.4.11.1" class="ltx_text" style="font-size:90%;">13.6</span></td>
</tr>
<tr id="S6.T12.2.5.5" class="ltx_tr">
<th id="S6.T12.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;" colspan="11"><span id="S6.T12.2.5.5.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Unfreeze Vision Encoder</span></th>
</tr>
<tr id="S6.T12.2.6.6" class="ltx_tr">
<th id="S6.T12.2.6.6.1" class="ltx_td ltx_th ltx_th_row ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<th id="S6.T12.2.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.2.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S6.T12.2.6.6.3" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.4.1" class="ltx_text" style="font-size:90%;">81.3</span></td>
<td id="S6.T12.2.6.6.5" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.6.1" class="ltx_text" style="font-size:90%;">4.9</span></td>
<td id="S6.T12.2.6.6.7" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.8.1" class="ltx_text" style="font-size:90%;">69.0</span></td>
<td id="S6.T12.2.6.6.9" class="ltx_td ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.10.1" class="ltx_text" style="font-size:90%;">15.3</span></td>
<td id="S6.T12.2.6.6.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.6.6.11.1" class="ltx_text" style="font-size:90%;">15.6</span></td>
</tr>
<tr id="S6.T12.2.7.7" class="ltx_tr">
<th id="S6.T12.2.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<th id="S6.T12.2.7.7.2" class="ltx_td ltx_th ltx_th_row ltx_border_r" style="padding-left:2.1pt;padding-right:2.1pt;"></th>
<td id="S6.T12.2.7.7.3" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.4.1" class="ltx_text" style="font-size:90%;">117.4</span></td>
<td id="S6.T12.2.7.7.5" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.7.7.6" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.6.1" class="ltx_text" style="font-size:90%;">19.6</span></td>
<td id="S6.T12.2.7.7.7" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.7.7.8" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.8.1" class="ltx_text" style="font-size:90%;">75.2</span></td>
<td id="S6.T12.2.7.7.9" class="ltx_td" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.7.7.10" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">21.5</span></td>
<td id="S6.T12.2.7.7.11" class="ltx_td ltx_align_center" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.7.7.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.3</span></td>
</tr>
<tr id="S6.T12.2.8.8" class="ltx_tr">
<th id="S6.T12.2.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.1.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<th id="S6.T12.2.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.2.1" class="ltx_text" style="font-size:90%;">✓</span></th>
<td id="S6.T12.2.8.8.3" class="ltx_td ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.8.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">118.7</span></td>
<td id="S6.T12.2.8.8.5" class="ltx_td ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.8.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">19.7</span></td>
<td id="S6.T12.2.8.8.7" class="ltx_td ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.8.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">76.3</span></td>
<td id="S6.T12.2.8.8.9" class="ltx_td ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"></td>
<td id="S6.T12.2.8.8.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.10.1" class="ltx_text" style="font-size:90%;">18.6</span></td>
<td id="S6.T12.2.8.8.11" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.1pt;padding-right:2.1pt;"><span id="S6.T12.2.8.8.11.1" class="ltx_text" style="font-size:90%;">17.8</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 12: </span><span id="S6.T12.6.1" class="ltx_text ltx_font_bold">Basic components.</span> Zero-shot performance on COCO caption, COCO object detection, Flickr30k grounding, and COCORef referring segmentation. V Pre and L Pre indicate that using vision and language pre-training initialization, respectively.</figcaption>
</figure>
<div id="S6.SS5.SSS0.Px4.p2" class="ltx_para">
<p id="S6.SS5.SSS0.Px4.p2.1" class="ltx_p">We observe that freezing the vision encoders does not affect the performance on tasks that require image-level understanding, but it significantly degrades the performance on tasks that require region-level or pixel-level understanding (e.g., AP on COCO object detection drops from 19.7 to 6.9). Previous methods for pre-training vision foundation models mainly focus on image-level tasks (e.g., image classification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib38" title="" class="ltx_ref">38</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, image-text contrastive learning&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite>), which may not provide them with sufficient region-level and pixel-level skills for downstream tasks. Therefore, it is important to unfreeze the vision backbone, enabling it to learn region-level and pixel-level features for various downstream tasks.</p>
</div>
<div id="S6.SS5.SSS0.Px4.p3" class="ltx_para">
<p id="S6.SS5.SSS0.Px4.p3.1" class="ltx_p">The effect of language pre-training weights on multi-modal encoder-decoder tasks varies depending on the task. Tasks that require more text understanding, such as captioning and grounding, benefit slightly from using language pre-training weights (e.g., COCO caption, Flickr30k grounding). Tasks that are mostly vision-focused, such as object detection and region segmentation, do not gain much from using language pre-training weights (for COCO object detection, the gain is only 0.1; for RES tasks, which use only localization tokens, the drop is 2.91 mIOU).</p>
</div>
<div id="S6.SS5.SSS0.Px4.p4" class="ltx_para">
<p id="S6.SS5.SSS0.Px4.p4.1" class="ltx_p">We investigate the effects of different training configurations on the performance of a foundation model in region-level and pixel-level tasks. We find that unfreezing the vision backbone is crucial for enhancing the model’s ability to learn from regions and pixels, which is beneficial for transferring to various downstream tasks. Moreover, we observe that using language pre-training weights can help the model in tasks that require text understanding, but have less impact on tasks that are purely vision-based. These results offer useful guidance for choosing the best training settings for different computer vision tasks.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Related Works</h2>

<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Vision-Language Foundation Models</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Recent vision-language pre-training models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib29" title="" class="ltx_ref">29</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite> have demonstrated impressive zero-shot transfer abilities to vision-language alignment and image classification tasks, thanks to the alignment of vision and text embeddings extracted from respective encoders through contrastive learning objectives&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib74" title="" class="ltx_ref">74</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib58" title="" class="ltx_ref">58</a>]</cite>. These models (<em id="S7.SS1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS1.p1.1.2" class="ltx_text"></span>,&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib95" title="" class="ltx_ref">95</a>]</cite>), trained on weakly large-scale image-text data, have been further extended to more downstream tasks such as object detection, achieving state-of-the-art performance with task-specific adaptation heads.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">In contrast, other studies&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib92" title="" class="ltx_ref">92</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib45" title="" class="ltx_ref">45</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib78" title="" class="ltx_ref">78</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>]</cite> propose using a multi-modality decoder to predict text in an autoregressive manner with language modeling pre-training objectives. Techniques for fusing vision and language embeddings vary: GIT&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib78" title="" class="ltx_ref">78</a>]</cite> concatenates vision and text tokens as decoder input and designs a casual attention mask, CoCa&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib92" title="" class="ltx_ref">92</a>]</cite> uses attentional poolers with learnable queries to select task-specific vision representations which are then cross-attended via the decoder, and Flamingo&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>]</cite> pools a fixed number of vision tokens with a Perceiver Resampler and adds new learnable cross-attention layers to the decoder while freezing the pre-trained vision encoder and text decoder.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">Beyond image captioning pre-training task, some research&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib79" title="" class="ltx_ref">79</a>]</cite> attempts to formulate more vision tasks in a unified sequence-to-sequence learning paradigm, including object detection and image segmentation. Customized special tokens accommodate representations beyond pure text, such as bounding boxes&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib10" title="" class="ltx_ref">10</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. This approach uses the same architecture for pre-training and downstream tasks, potentially using the same set of weights for all tasks. Our method, which falls into this category, aims to obtain foundation models that understand dense information beyond simple image-level captions. It shares the same encoder-decoder design as other multi-modality encoder-decoder models&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib15" title="" class="ltx_ref">15</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib55" title="" class="ltx_ref">55</a>]</cite> adapted for sequence-to-sequence learning, but uses our built large-scale comprehensive annotation data instead of combining existing sparse annotated data.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Vision Datasets</h3>

<section id="S7.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comprehensive annotations.</h4>

<div id="S7.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS2.SSS0.Px1.p1.1" class="ltx_p">The quest for comprehensive understanding of visual scenes, the holy grail of computer vision&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, has evolved from focusing on individual datasets each targeting a single perspective, <em id="S7.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S7.SS2.SSS0.Px1.p1.1.2" class="ltx_text"></span>, image classification&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, to providing multi-perspective&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib36" title="" class="ltx_ref">36</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, comprehensive annotations for every visual data point. Notable datasets like MS-COCO&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib48" title="" class="ltx_ref">48</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and Visual Genome&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib36" title="" class="ltx_ref">36</a>]</cite> integrate various types of annotations, enabling richer understanding in spatial and semantic granularities and better model interactions across annotations. However, due to the high cost of human verification, these annotations are limited in size. Our datasets, while large-scale, maintain comprehensive annotations covering text, region-text pairs, and text-phrase-region triplets, with reduced human involvement.</p>
</div>
</section>
<section id="S7.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Scalable annotations.</h4>

<div id="S7.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS2.SSS0.Px2.p1.1" class="ltx_p">: Over the past decade, vision datasets have rapidly scaled up from thousands <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib42" title="" class="ltx_ref">42</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to billion examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib29" title="" class="ltx_ref">29</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib96" title="" class="ltx_ref">96</a>]</cite> to encompass more visual concepts for better generalization. This shift is evident in recent foundation models that employ massive quantities of data&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. These large datasets typically collect images from the web and parse noisy annotations from the corresponding metadata, such as category label from query&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib75" title="" class="ltx_ref">75</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib96" title="" class="ltx_ref">96</a>]</cite>, short description from alt-text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib64" title="" class="ltx_ref">64</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, as well as detailed description from interleaved text&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib2" title="" class="ltx_ref">2</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Despite their diversity, these annotations suffer from randomness and limited types (<em id="S7.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S7.SS2.SSS0.Px2.p1.1.2" class="ltx_text"></span>, texts only). Some works&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib32" title="" class="ltx_ref">32</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib45" title="" class="ltx_ref">45</a>]</cite> attempt to scale up annotations using pseudo-label generation with iteratively trained models, which offer higher quality without significant diversity loss. Our data pipeline extends these large-scale, web-crawled noisy annotations with higher-quality, autonomous annotations generated from multiple specialist models. The pipeline iteratively refines labels and completes missing pieces, resulting in a scalable and comprehensive dataset for learning a unified visual representation.</p>
</div>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">The Florence Project endeavors to develop a foundational vision model endowed with a diverse array of perceptual capabilities, encompassing spatial hierarchy and semantic granularity. To this end, we construct <em id="S8.p1.1.1" class="ltx_emph ltx_font_italic">FLD-5B</em> dataset containing an extensive collection of 126M images paired with 5B comprehensive annotations, which are collected by the Florence data engine. Subsequently, we pre-train <em id="S8.p1.1.2" class="ltx_emph ltx_font_italic">Florence-2</em> on this rich dataset through comprehensive multitask learning in a unified manner. <em id="S8.p1.1.3" class="ltx_emph ltx_font_italic">Florence-2</em> has exhibited remarkable zero-shot capabilities that extend across a wide spectrum of visual tasks, such as captioning, object detection, visual grounding, and referring segmentation, among others. The experimental findings underscore the potency of the universal representation pre-trained by <em id="S8.p1.1.4" class="ltx_emph ltx_font_italic">Florence-2</em>, revealing its substantial contributions to the enhancement of a multitude of downstream tasks.</p>
</div>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgment.</h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p">We would like to express our heartfelt gratitude to all the contributors from the Azure AI team who worked on the Florence project. We sincerely appreciate Misha Bilenko for the invaluable guidance and support. Our thanks are extended to Yi-Ling Chen, Mengchen Liu, Yen-Chun Chen and Dongdong Chen for engaging in helpful discussions and to Yunsheng Li for their assistance with segmentation annotations. Deep appreciation is also expressed to Qingfen Lin, Ryan Menezes, Kuan Lu, Gabe Blanco, Shohei Ono, Ping Jin, Jiahe Zhou, Xiong Qiao, Tong Bai, Xingchao Peng, Pei Guo, Lihang Li for providing valuable feedback in downstream applications discussions. Special thanks to Cha Zhang, Jinyu Li, Min Gao, Christina Sun, Oliver Ernst, Kevin Pan, Mei Gao for their work on data annotation support and insightful discussions in data pipeline. Furthermore, we would like to thank Thomas Soemo, Nguyen Bach for their constructive feedback.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Azure ai services.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://azure.microsoft.com/en-us/products/ai-services?activetab=pivot:azureopenaiservicetab" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://azure.microsoft.com/en-us/products/ai-services?activetab=pivot:azureopenaiservicetab</a><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">Accessed: 2023-10-13.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Flamingo: a visual language model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">,
35:23716–23736, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Jimmy&nbsp;Lei Ba, Jamie&nbsp;Ryan Kiros, and Geoffrey&nbsp;E. Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Layer normalization, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">BEiT: BERT pre-training of image transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Rishi Bommasani, Drew&nbsp;A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael&nbsp;S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">On the opportunities and risks of foundation models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2108.07258</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared&nbsp;D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Language models are few-shot learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, volume&nbsp;33,
pages 1877–1901. Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 213–229.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual features by contrasting cluster
assignments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">,
volume&nbsp;33, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A simple framework for contrastive learning of visual
representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages
1597–1607. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Saurabh Saxena, Lala Li, David&nbsp;J. Fleet, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Pix2seq: A language modeling framework for object detection, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David&nbsp;J Fleet, and Geoffrey&nbsp;E
Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">A unified sequence interface for vision tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">,
35:31333–31346, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo,
Jialin Wu, Carlos&nbsp;Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Pali-x: On scaling up a multilingual vision and language model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2305.18565</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C&nbsp;Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul
Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr
Padlewski, Daniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran Rong,
Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Pali-3 vision language models: Smaller, faster, stronger, 2023.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski,
Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer,
Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,
Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo,
Mojtaba Seyedhosseini, Chao Jia, Burcu&nbsp;Karagol Ayan, Carlos Riquelme, Andreas
Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Pali: A jointly-scaled multilingual language-image model, 2022.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Bowen Cheng, Ishan Misra, Alexander&nbsp;G. Schwing, Alexander Kirillov, and Rohit
Girdhar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Masked-attention mask transformer for universal image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Kyunghyun Cho, Bart Van&nbsp;Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Learning phrase representations using rnn encoder-decoder for
statistical machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1406.1078</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding, 2019.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Davit: Dual attention vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, pages 74–92.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale&nbsp;J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey&nbsp;P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 3608–3617, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 16000–16009, 2022.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 9729–9738, 2020.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Matthew Honnibal, Ines Montani, Sofie Van&nbsp;Landeghem, Adriane Boyd, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">spacy: Industrial-strength natural language processing in python.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc&nbsp;V.
Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Scaling up visual and vision-language representation learning with
noisy text supervision, 2021.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Andrej Karpathy and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Deep visual-semantic alignments for generating image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2015 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, pages 3128–3137, 2014.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Referitgame: Referring to objects in photographs of natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 787–798, 2014.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander&nbsp;C Berg, Wan-Yen Lo, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2304.02643</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Aniket Kittur, Ed Chi, Bryan&nbsp;A Pendleton, Bongwon Suh, and Todd Mytkowicz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Power of the few vs. wisdom of the crowd: Wikipedia and the rise of
the bourgeoisie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">World wide web</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 1(2):19, 2007.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">A hierarchical approach for generating descriptive image paragraphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 317–325, 2017.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">A hierarchical approach for generating descriptive image paragraphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Patterm Recognition (CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David&nbsp;A Shamma, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International journal of computer vision</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 123:32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Geoffrey Hinton, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Learning multiple layers of features from tiny images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">2009.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey&nbsp;E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Imagenet classification with deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages
1097–1105, 2012.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, Tom Duerig, and Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">The open images dataset v4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 128(7):1956–1981,
mar 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">The open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 128(7):1956–1981,
2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet
Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander&nbsp;M Rush,
Douwe Kiela, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Obelisc: An open web-scale filtered dataset of interleaved image-text
documents.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.16527</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Yann LeCun, Corinna Cortes, and CJ Burges.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Mnist handwritten digit database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">,
2, 2010.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Bart: Denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension, 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</span><span id="bib.bib44.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages
12888–12900. PMLR, 2022.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Exploring plain vision transformer backbones for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 280–296.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,
James Hays, Pietro Perona, Deva Ramanan, C.&nbsp;Lawrence Zitnick, and Piotr
Dollár.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C&nbsp;Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi&nbsp;Kumar Satzoda, Vijay
Mahadevan, and R Manmatha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Polyformer: Referring image segmentation as sequential polygon
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 18653–18663, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
Li, Jianwei Yang, Hang Su, Jun Zhu, et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2303.05499</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
and Saining Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">A convnet for the 2020s.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, pages 11976–11986, 2022.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Sgdr: Stochastic gradient descent with warm restarts, 2017.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Ilya Loshchilov and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Decoupled weight decay regularization, 2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha
Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Unified-io: A unified model for vision, language, and multi-modal
tasks, 2022.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan&nbsp;L Yuille, and
Kevin Murphy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Generation and comprehension of unambiguous object descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages 11–20, 2016.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Ok-vqa: A visual question answering benchmark requiring external
knowledge, 2019.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Aaron van&nbsp;den Oord, Yazhe Li, and Oriol Vinyals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Representation learning with contrastive predictive coding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.03748</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">BEiT v2: Masked image modeling with vector-quantized visual
tokenizers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">2022.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and
Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Kosmos-2: Grounding multimodal large language models to the world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2306.14824</span><span id="bib.bib60.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Bryan&nbsp;A Plummer, Liwei Wang, Chris&nbsp;M Cervantes, Juan&nbsp;C Caicedo, Julia
Hockenmaier, and Svetlana Lazebnik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Flickr30k entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, pages 2641–2649, 2015.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and
Vittorio Ferrari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Connecting vision and language with localized narratives.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Filip Radenovic, Abhimanyu Dubey, Abhishek Kadian, Todor Mihaylov, Simon
Vandenhende, Yash Patel, Yi Wen, Vignesh Ramanathan, and Dhruv Mahajan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Filtering, distillation, and hard negatives for vision-language
pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.02280</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jong&nbsp;Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et&nbsp;al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, pages
8748–8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Language models are unsupervised multitask learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter&nbsp;J Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">Exploring the limits of transfer learning with a unified text-to-text
transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The Journal of Machine Learning Research</span><span id="bib.bib66.4.2" class="ltx_text" style="font-size:90%;">, 21(1):5485–5551,
2020.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Deepspeed: System optimizations enable training deep learning models
with over 100 billion parameters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib67.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</span><span id="bib.bib67.5.3" class="ltx_text" style="font-size:90%;">, pages 3505–3506, 2020.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
Komatsuzaki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Laion-400m: Open dataset of clip-filtered 400 million image-text
pairs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.02114</span><span id="bib.bib68.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">A-okvqa: A benchmark for visual question answering using world
knowledge, 2022.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Objects365: A large-scale, high-quality dataset for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, pages 8430–8439, 2019.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text" style="font-size:90%;">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib71.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ACL</span><span id="bib.bib71.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">Textcaps: a dataset for image captioning with reading comprehension,
2020.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib73.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span><span id="bib.bib73.5.3" class="ltx_text" style="font-size:90%;">, pages 8317–8326, 2019.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Kihyuk Sohn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Improved deep metric learning with multi-class n-pair loss objective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib74.4.2" class="ltx_text" style="font-size:90%;">, 29, 2016.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Revisiting unreasonable effectiveness of data in deep learning era.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, pages 843–852, 2017.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Ilya Sutskever, Oriol Vinyals, and Quoc&nbsp;V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Sequence to sequence learning with neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib76.4.2" class="ltx_text" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan&nbsp;N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib77.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib77.5.3" class="ltx_text" style="font-size:90%;">, pages
5998–6008, 2017.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan,
Zicheng Liu, Ce Liu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Git: A generative image-to-text transformer for vision and language,
2022.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
Chang Zhou, Jingren Zhou, and Hongxia Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Ofa: Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework, 2022.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Nic&nbsp;M Weststrate, Susan Bluck, and Judith Glück.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Wisdom of the crowd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The Cambridge handbook of wisdom</span><span id="bib.bib80.4.2" class="ltx_text" style="font-size:90%;">, pages 97–121, 2019.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In&nbsp;So
Kweon, and Saining Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">Convnext v2: Co-designing and scaling convnets with masked
autoencoders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib81.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib81.5.3" class="ltx_text" style="font-size:90%;">, pages 16133–16142, 2023.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">Unified perceptual parsing for scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib82.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European conference on computer vision
(ECCV)</span><span id="bib.bib82.5.3" class="ltx_text" style="font-size:90%;">, pages 418–434, 2018.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi
Dai, and Han Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">Simmim: A simple framework for masked image modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, pages 9653–9663, 2022.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan
Lu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.2.1" class="ltx_text" style="font-size:90%;">Universal instance perception as object discovery and retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib84.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib84.5.3" class="ltx_text" style="font-size:90%;">, pages 15325–15336, 2023.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.2.1" class="ltx_text" style="font-size:90%;">Focal modulation networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib85.4.2" class="ltx_text" style="font-size:90%;">,
35:4203–4217, 2022.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and
Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.2.1" class="ltx_text" style="font-size:90%;">Focal self-attention for local-global interactions in vision
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.00641</span><span id="bib.bib86.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and
Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.2.1" class="ltx_text" style="font-size:90%;">Unified contrastive learning in image-text-label space, 2022.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text" style="font-size:90%;">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu,
Yumao Lu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.2.1" class="ltx_text" style="font-size:90%;">Unitab: Unifying text and box outputs for grounded vision-language
modeling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib88.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib88.5.3" class="ltx_text" style="font-size:90%;">, pages 521–539.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text" style="font-size:90%;">
Sheng Kung&nbsp;Michael Yi, Mark Steyvers, Michael&nbsp;D Lee, and Matthew&nbsp;J Dry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.2.1" class="ltx_text" style="font-size:90%;">The wisdom of the crowd in combinatorial problems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Cognitive science</span><span id="bib.bib89.4.2" class="ltx_text" style="font-size:90%;">, 36(3):452–470, 2012.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text" style="font-size:90%;">
Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang,
Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.2.1" class="ltx_text" style="font-size:90%;">Ferret: Refer and ground anything anywhere at any granularity, 2023.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text" style="font-size:90%;">
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.2.1" class="ltx_text" style="font-size:90%;">From image descriptions to visual denotations: New similarity metrics
for semantic inference over event descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Transactions of the Association for Computational Linguistics</span><span id="bib.bib91.4.2" class="ltx_text" style="font-size:90%;">,
2:67–78, 2014.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text" style="font-size:90%;">
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.2.1" class="ltx_text" style="font-size:90%;">Coca: Contrastive captioners are image-text foundation models, 2022.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Patrick Poirson, Shan Yang, Alexander&nbsp;C Berg, and Tamara&nbsp;L Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.2.1" class="ltx_text" style="font-size:90%;">Modeling context in referring expressions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib93.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span><span id="bib.bib93.5.3" class="ltx_text" style="font-size:90%;">,
pages 69–85. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text" style="font-size:90%;">
Licheng Yu, Patrick Poirson, Shan Yang, Alexander&nbsp;C. Berg, and Tamara&nbsp;L. Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.2.1" class="ltx_text" style="font-size:90%;">Modeling context in referring expressions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.3.1" class="ltx_text" style="font-size:90%;">In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors,
</span><span id="bib.bib94.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision – ECCV 2016</span><span id="bib.bib94.5.3" class="ltx_text" style="font-size:90%;">, pages 69–85, Cham, 2016. Springer
International Publishing.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text" style="font-size:90%;">
Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu,
Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao, Zhen
Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.2.1" class="ltx_text" style="font-size:90%;">Florence: A new foundation model for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.11432</span><span id="bib.bib95.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text" style="font-size:90%;">
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.2.1" class="ltx_text" style="font-size:90%;">Scaling vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib96.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib96.5.3" class="ltx_text" style="font-size:90%;">, pages 12104–12113, 2022.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text" style="font-size:90%;">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel&nbsp;M Ni, and
Heung-Yeung Shum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.2.1" class="ltx_text" style="font-size:90%;">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.03605</span><span id="bib.bib97.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text" style="font-size:90%;">
Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio
Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.2.1" class="ltx_text" style="font-size:90%;">Scene parsing through ade20k dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib98.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib98.5.3" class="ltx_text" style="font-size:90%;">, pages 633–641, 2017.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text" style="font-size:90%;">
Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia Pan, Mingbao Lin, Chao
Chen, Liujuan Cao, Xiaoshuai Sun, and Rongrong Ji.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.2.1" class="ltx_text" style="font-size:90%;">Seqtr: A simple yet universal network for visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib99.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib99.5.3" class="ltx_text" style="font-size:90%;">, pages 598–615.
Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.2.1" class="ltx_text" style="font-size:90%;">Deformable detr: Deformable transformers for end-to-end object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.04159</span><span id="bib.bib100.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Supported Tasks and Annotations in Florence-2</h2>

<figure id="A1.T13" class="ltx_table">
<table id="A1.T13.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T13.2.1.1" class="ltx_tr">
<th id="A1.T13.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Task</th>
<th id="A1.T13.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Annotation Type</th>
<th id="A1.T13.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Prompt Input</th>
<th id="A1.T13.2.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Output</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T13.2.2.1" class="ltx_tr">
<td id="A1.T13.2.2.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Caption</td>
<td id="A1.T13.2.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
<td id="A1.T13.2.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.2.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
</tr>
<tr id="A1.T13.2.3.2" class="ltx_tr">
<td id="A1.T13.2.3.2.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Detailed caption</td>
<td id="A1.T13.2.3.2.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
<td id="A1.T13.2.3.2.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.3.2.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
</tr>
<tr id="A1.T13.2.4.3" class="ltx_tr">
<td id="A1.T13.2.4.3.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;More detailed caption</td>
<td id="A1.T13.2.4.3.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
<td id="A1.T13.2.4.3.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.4.3.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
</tr>
<tr id="A1.T13.2.5.4" class="ltx_tr">
<td id="A1.T13.2.5.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region proposal</td>
<td id="A1.T13.2.5.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region</td>
<td id="A1.T13.2.5.4.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.5.4.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region</td>
</tr>
<tr id="A1.T13.2.6.5" class="ltx_tr">
<td id="A1.T13.2.6.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Object detection</td>
<td id="A1.T13.2.6.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.6.5.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.6.5.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.7.6" class="ltx_tr">
<td id="A1.T13.2.7.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dense region caption</td>
<td id="A1.T13.2.7.6.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.7.6.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.7.6.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.8.7" class="ltx_tr">
<td id="A1.T13.2.8.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Phrase grounding</td>
<td id="A1.T13.2.8.7.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text-Phrase-Region</td>
<td id="A1.T13.2.8.7.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.8.7.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.9.8" class="ltx_tr">
<td id="A1.T13.2.9.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Referring expression comprehension</td>
<td id="A1.T13.2.9.8.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.9.8.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.9.8.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.10.9" class="ltx_tr">
<td id="A1.T13.2.10.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Open vocabulary detection</td>
<td id="A1.T13.2.10.9.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.10.9.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.10.9.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.11.10" class="ltx_tr">
<td id="A1.T13.2.11.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Referring segmentation</td>
<td id="A1.T13.2.11.10.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.11.10.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.11.10.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
<tr id="A1.T13.2.12.11" class="ltx_tr">
<td id="A1.T13.2.12.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region to text</td>
<td id="A1.T13.2.12.11.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.12.11.3" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text, region</td>
<td id="A1.T13.2.12.11.4" class="ltx_td ltx_align_left" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text</td>
</tr>
<tr id="A1.T13.2.13.12" class="ltx_tr">
<td id="A1.T13.2.13.12.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text detection and recognition</td>
<td id="A1.T13.2.13.12.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region-Text</td>
<td id="A1.T13.2.13.12.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Image, text</td>
<td id="A1.T13.2.13.12.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:18.0pt;padding-right:18.0pt;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text, region</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T13.4.1.1" class="ltx_text" style="font-size:90%;">Table 13</span>: </span><span id="A1.T13.5.2" class="ltx_text" style="font-size:90%;">Supported Tasks and annotations used for <em id="A1.T13.5.2.1" class="ltx_emph ltx_font_italic">Florence-2</em> pretraining.</span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Supervised Data Collection for Generalist Model Fine-tuning</h2>

<figure id="A2.T14" class="ltx_table">
<table id="A2.T14.6" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T14.6.7.1" class="ltx_tr">
<th id="A2.T14.6.7.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.7.1.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Task</span></th>
<td id="A2.T14.6.7.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.7.1.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Dataset</span></td>
</tr>
<tr id="A2.T14.6.8.2" class="ltx_tr">
<th id="A2.T14.6.8.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.8.2.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Caption</span></th>
<td id="A2.T14.6.8.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.8.2.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;COCO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.8.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib13" title="" class="ltx_ref">13</a><span id="A2.T14.6.8.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.8.2.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.9.3" class="ltx_tr">
<th id="A2.T14.6.9.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.9.3.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Text Caption</span></th>
<td id="A2.T14.6.9.3.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.9.3.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TextCaps&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.9.3.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib72" title="" class="ltx_ref">72</a><span id="A2.T14.6.9.3.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.9.3.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.10.4" class="ltx_tr">
<th id="A2.T14.6.10.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.10.4.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paragraph caption</span></th>
<td id="A2.T14.6.10.4.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.10.4.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Standford Paragraph Caption&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.10.4.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib35" title="" class="ltx_ref">35</a><span id="A2.T14.6.10.4.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.10.4.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.11.5" class="ltx_tr">
<th id="A2.T14.6.11.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.11.5.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Detailed caption</span></th>
<td id="A2.T14.6.11.5.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.11.5.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Localized Narratives&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.11.5.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib62" title="" class="ltx_ref">62</a><span id="A2.T14.6.11.5.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.11.5.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.2.2" class="ltx_tr">
<th id="A2.T14.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.2.2.3.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Detection</span></th>
<td id="A2.T14.2.2.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.2.2.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;COCO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.2.2.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib47" title="" class="ltx_ref">47</a><span id="A2.T14.2.2.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.2.2.2.4" class="ltx_text" style="font-size:90%;">, Object365</span><sup id="A2.T14.2.2.2.5" class="ltx_sup"><span id="A2.T14.2.2.2.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.2.2.2.6" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.2.2.2.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a><span id="A2.T14.2.2.2.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.2.2.2.9" class="ltx_text" style="font-size:90%;">, Open Images</span><sup id="A2.T14.2.2.2.10" class="ltx_sup"><span id="A2.T14.2.2.2.10.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.2.2.2.11" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.2.2.2.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib39" title="" class="ltx_ref">39</a><span id="A2.T14.2.2.2.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.2.2.2.14" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.4.4" class="ltx_tr">
<th id="A2.T14.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.4.4.3.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Phrase Grounding</span></th>
<td id="A2.T14.4.4.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.4.4.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Flickr30k, Object365</span><sup id="A2.T14.4.4.2.2" class="ltx_sup"><span id="A2.T14.4.4.2.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.4.4.2.3" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.4.4.2.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a><span id="A2.T14.4.4.2.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.4.4.2.6" class="ltx_text" style="font-size:90%;">, Open Images</span><sup id="A2.T14.4.4.2.7" class="ltx_sup"><span id="A2.T14.4.4.2.7.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.4.4.2.8" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.4.4.2.9.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib39" title="" class="ltx_ref">39</a><span id="A2.T14.4.4.2.10.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.4.4.2.11" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.12.6" class="ltx_tr">
<th id="A2.T14.6.12.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.12.6.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Referring expression</span></th>
<td id="A2.T14.6.12.6.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.12.6.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg)&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.12.6.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib31" title="" class="ltx_ref">31</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib93" title="" class="ltx_ref">93</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib56" title="" class="ltx_ref">56</a><span id="A2.T14.6.12.6.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.12.6.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.13.7" class="ltx_tr">
<th id="A2.T14.6.13.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.13.7.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Referring expression segmentation</span></th>
<td id="A2.T14.6.13.7.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.13.7.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RefCOCO-mix (RefCOCO, RefCOCO+, RefCOCOg)&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.13.7.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib31" title="" class="ltx_ref">31</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib93" title="" class="ltx_ref">93</a>, <a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib56" title="" class="ltx_ref">56</a><span id="A2.T14.6.13.7.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.13.7.2.4" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.6" class="ltx_tr">
<th id="A2.T14.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.6.3.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region to category</span></th>
<td id="A2.T14.6.6.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.6.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;COCO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.6.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib47" title="" class="ltx_ref">47</a><span id="A2.T14.6.6.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.6.2.4" class="ltx_text" style="font-size:90%;">, Object365</span><sup id="A2.T14.6.6.2.5" class="ltx_sup"><span id="A2.T14.6.6.2.5.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.6.6.2.6" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.6.2.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib70" title="" class="ltx_ref">70</a><span id="A2.T14.6.6.2.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.6.2.9" class="ltx_text" style="font-size:90%;">, Open Images</span><sup id="A2.T14.6.6.2.10" class="ltx_sup"><span id="A2.T14.6.6.2.10.1" class="ltx_text ltx_font_italic" style="font-size:90%;">∗</span></sup><span id="A2.T14.6.6.2.11" class="ltx_text" style="font-size:90%;">&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.6.2.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib39" title="" class="ltx_ref">39</a><span id="A2.T14.6.6.2.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.6.2.14" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.14.8" class="ltx_tr">
<th id="A2.T14.6.14.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.14.8.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Region to polygon</span></th>
<td id="A2.T14.6.14.8.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.14.8.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;COCO&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.14.8.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib47" title="" class="ltx_ref">47</a><span id="A2.T14.6.14.8.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.14.8.2.4" class="ltx_text" style="font-size:90%;"> (after deduplicating RefCOCO-mix val)</span>
</td>
</tr>
<tr id="A2.T14.6.15.9" class="ltx_tr">
<th id="A2.T14.6.15.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.15.9.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQA</span></th>
<td id="A2.T14.6.15.9.2" class="ltx_td ltx_align_left" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.15.9.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQAv2&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.15.9.2.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib22" title="" class="ltx_ref">22</a><span id="A2.T14.6.15.9.2.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.15.9.2.4" class="ltx_text" style="font-size:90%;">, OKVQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.15.9.2.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib57" title="" class="ltx_ref">57</a><span id="A2.T14.6.15.9.2.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.15.9.2.7" class="ltx_text" style="font-size:90%;">, AOKVQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.15.9.2.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib69" title="" class="ltx_ref">69</a><span id="A2.T14.6.15.9.2.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.15.9.2.10" class="ltx_text" style="font-size:90%;">, TextVQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.15.9.2.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib73" title="" class="ltx_ref">73</a><span id="A2.T14.6.15.9.2.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.15.9.2.13" class="ltx_text" style="font-size:90%;">, ViZWiz VQA&nbsp;</span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.T14.6.15.9.2.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib23" title="" class="ltx_ref">23</a><span id="A2.T14.6.15.9.2.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.T14.6.15.9.2.16" class="ltx_text" style="font-size:90%;"></span>
</td>
</tr>
<tr id="A2.T14.6.16.10" class="ltx_tr">
<th id="A2.T14.6.16.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:20.0pt;padding-right:20.0pt;"><span id="A2.T14.6.16.10.1.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OCR</span></th>
<td id="A2.T14.6.16.10.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:20.0pt;padding-right:20.0pt;">
<span id="A2.T14.6.16.10.2.1" class="ltx_text" style="font-size:90%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subset from </span><em id="A2.T14.6.16.10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">FLD-5B</em><span id="A2.T14.6.16.10.2.3" class="ltx_text" style="font-size:90%;"> OCR (2 millon samples)</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 14: </span>Collection of dataset for finetuning one single generalist model for downstream tasks evaluation. <sup id="A2.T14.16.1" class="ltx_sup"><span id="A2.T14.16.1.1" class="ltx_text ltx_font_italic">∗</span></sup> indicates using the annotations from <em id="A2.T14.17.2" class="ltx_emph ltx_font_italic">FLD-5B</em>, which merges original annotations with ours.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Model Configuration</h2>

<figure id="A3.T15" class="ltx_table">
<table id="A3.T15.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T15.2.1.1" class="ltx_tr">
<th id="A3.T15.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;" rowspan="2"><span id="A3.T15.2.1.1.1.1" class="ltx_text" style="font-size:90%;">Model</span></th>
<th id="A3.T15.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;" colspan="4"><span id="A3.T15.2.1.1.2.1" class="ltx_text" style="font-size:90%;">Image Encoder (DaViT)</span></th>
<th id="A3.T15.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.7pt;padding-right:3.7pt;" colspan="4"><span id="A3.T15.2.1.1.3.1" class="ltx_text" style="font-size:90%;">Encoder-Decoder (Transformer)</span></th>
</tr>
<tr id="A3.T15.2.2.2" class="ltx_tr">
<th id="A3.T15.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.1.1" class="ltx_text" style="font-size:90%;">dimensions</span></th>
<th id="A3.T15.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.2.1" class="ltx_text" style="font-size:90%;">blocks</span></th>
<th id="A3.T15.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.3.1" class="ltx_text" style="font-size:90%;">heads/groups</span></th>
<th id="A3.T15.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.4.1" class="ltx_text" style="font-size:90%;">#params</span></th>
<th id="A3.T15.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.5.1" class="ltx_text" style="font-size:90%;">encoder layers</span></th>
<th id="A3.T15.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.6.1" class="ltx_text" style="font-size:90%;">decoder layers</span></th>
<th id="A3.T15.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.7.1" class="ltx_text" style="font-size:90%;">dimensions</span></th>
<th id="A3.T15.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.2.2.8.1" class="ltx_text" style="font-size:90%;">#params</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T15.2.3.1" class="ltx_tr">
<th id="A3.T15.2.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><em id="A3.T15.2.3.1.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-B</em></th>
<td id="A3.T15.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.2.1" class="ltx_text" style="font-size:90%;">[128, 256, 512, 1024]</span></td>
<td id="A3.T15.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.3.1" class="ltx_text" style="font-size:90%;">[1, 1, 9, 1]</span></td>
<td id="A3.T15.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.4.1" class="ltx_text" style="font-size:90%;">[4, 8, 16, 32]</span></td>
<td id="A3.T15.2.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.5.1" class="ltx_text" style="font-size:90%;">90M</span></td>
<td id="A3.T15.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.6.1" class="ltx_text" style="font-size:90%;">6</span></td>
<td id="A3.T15.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.7.1" class="ltx_text" style="font-size:90%;">6</span></td>
<td id="A3.T15.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.8.1" class="ltx_text" style="font-size:90%;">768</span></td>
<td id="A3.T15.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.3.1.9.1" class="ltx_text" style="font-size:90%;">140M</span></td>
</tr>
<tr id="A3.T15.2.4.2" class="ltx_tr">
<th id="A3.T15.2.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><em id="A3.T15.2.4.2.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Florence-2-L</em></th>
<td id="A3.T15.2.4.2.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.2.1" class="ltx_text" style="font-size:90%;">[256, 512, 1024, 2048]</span></td>
<td id="A3.T15.2.4.2.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.3.1" class="ltx_text" style="font-size:90%;">[1, 1, 9, 1]</span></td>
<td id="A3.T15.2.4.2.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.4.1" class="ltx_text" style="font-size:90%;">[8, 16, 32, 64]</span></td>
<td id="A3.T15.2.4.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.5.1" class="ltx_text" style="font-size:90%;">360M</span></td>
<td id="A3.T15.2.4.2.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.6.1" class="ltx_text" style="font-size:90%;">12</span></td>
<td id="A3.T15.2.4.2.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.7.1" class="ltx_text" style="font-size:90%;">12</span></td>
<td id="A3.T15.2.4.2.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.8.1" class="ltx_text" style="font-size:90%;">1024</span></td>
<td id="A3.T15.2.4.2.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.7pt;padding-right:3.7pt;"><span id="A3.T15.2.4.2.9.1" class="ltx_text" style="font-size:90%;">410M</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 15: </span>Model configuration of different size.</figcaption>
</figure>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>More Examples of Annotations in <em id="A4.5.1" class="ltx_emph ltx_font_italic">FLD-5B</em>
</h2>

<div id="A4.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A4.2.p1" class="ltx_para">
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x13.png" id="A4.1.g1" class="ltx_graphics ltx_img_portrait" width="426" height="661" alt="[Uncaptioned image]">
</div>
<figure id="A4.F8" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A4.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A4.F8.4.2" class="ltx_text" style="font-size:90%;">Examples of annotations in <em id="A4.F8.4.2.1" class="ltx_emph ltx_font_italic">FLD-5B</em>.</span></figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="A4.4" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A4.4.p1" class="ltx_para">
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x14.png" id="A4.3.g1" class="ltx_graphics ltx_img_portrait" width="415" height="655" alt="[Uncaptioned image]">
</div>
<figure id="A4.F9" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A4.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A4.F9.4.2" class="ltx_text" style="font-size:90%;">Examples of annotations in <em id="A4.F9.4.2.1" class="ltx_emph ltx_font_italic">FLD-5B</em> (continued).</span></figcaption>
</figure>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Qualitative Evaluation and Visualization Results</h2>

<section id="A5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Detailed Image Caption</h3>

<div id="A5.SS1.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS1.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS1.1.p1.pic1" class="ltx_picture" height="797.9" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,797.9) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 779.86 C 0 783.12 2.64 785.76 5.91 785.76 L 650.23 785.76 C 653.49 785.76 656.13 783.12 656.13 779.86 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 779.86 C 1.97 782.03 3.73 783.79 5.91 783.79 L 650.23 783.79 C 652.4 783.79 654.16 782.03 654.16 779.86 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 775.76)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 165.38 22.14 C 167.01 22.14 168.33 20.82 168.33 19.19 L 168.33 2.95 C 168.33 1.32 167.01 0 165.38 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 165.38 22.14 C 167.01 22.14 168.33 20.82 168.33 19.19 L 168.33 2.95 C 168.33 1.32 167.01 0 165.38 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Detailed Image Caption</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="752.24" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS1.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.4" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS1.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.p4" class="ltx_para">
<span id="A5.SS1.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.p4.1" class="ltx_p"><span id="A5.SS1.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.p4.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Describe with a paragraph what is shown in the image.</span>
<span id="A5.SS1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/cap_1.jpg" id="A5.SS1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="172" height="172" alt="[Uncaptioned image]">
</span>
<span id="A5.SS1.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.p4.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS1.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.p4.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">The image is a hand-drawn illustration of a glass of gin and tonic. The glass is filled with ice cubes, lime slices, and a slice of lemon. There is a straw sticking out of the top of the glass. The background is a light green color with a grungy texture. On the right side of the image, there is a list of ingredients written in black ink. The text reads "Gin &amp; Tonic" and "2oz gin, 5oz tonic water, 2-4 ice cubes lime slice for garnish".
</pre>
</span>
</span>
<span id="A5.SS1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A5.SS1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS1.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.2.p5" class="ltx_para">
<span id="A5.SS1.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/cap_2.jpg" id="A5.SS1.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.g1" class="ltx_graphics ltx_img_landscape" width="172" height="128" alt="[Uncaptioned image]">
</span>
<span id="A5.SS1.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.2.p5.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS1.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.2.p5.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">The image shows the interior of a grocery store with a large variety of fruits and vegetables on display. The store has a high ceiling with green and white striped awnings, and the floor is made of wood. There are rows of wooden crates filled with different types of fruits, including apples, oranges, lemons, limes, and limes. The crates are arranged in neat rows, and there are price tags attached to each crate. In the background, there are shelves stocked with various items such as fruits, vegetables, and other grocery items. The overall atmosphere of the store is bright and colorful, with a sense of freshness and abundance.
</pre>
</span>
</span>
<span id="A5.SS1.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2" class="ltx_para ltx_noindent"><svg id="A5.SS1.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS1.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.3.p6" class="ltx_para">
<span id="A5.SS1.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/cap_3.jpg" id="A5.SS1.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.g1" class="ltx_graphics ltx_img_portrait" width="172" height="254" alt="[Uncaptioned image]">
</span>
<span id="A5.SS1.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.3.p6.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS1.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.3.p6.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">The image is a movie poster for the film "Harry Potter and the Sorcerer’s Stone" starring Ron Weasley. The poster features a young boy, Ron Weasley, standing in front of a dark background with a castle-like structure in the background. He is wearing a school uniform with a Gryffindor crest on his jacket and a red tie. He has blonde hair and is looking directly at the camera with a serious expression on his face. The title of the film is written in white text at the top of the poster, with the tagline "20 years of movie magic" written in smaller text below.
</pre>
</span>
</span>
<span id="A5.SS1.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.p3" class="ltx_para ltx_noindent"><svg id="A5.SS1.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.p3.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS1.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.4.p7" class="ltx_para">
<span id="A5.SS1.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.4" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/cap_4.jpg" id="A5.SS1.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.4.g1" class="ltx_graphics ltx_img_landscape" width="172" height="98" alt="[Uncaptioned image]">
</span>
<span id="A5.SS1.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.4.p7.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS1.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.4.p7.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">The image is a digital illustration of a girl hugging a white cat. The girl is wearing a pink sweater and has long brown hair. She is sitting on a green surface with several potted plants and flowers around her. The plants have green leaves and pink and white flowers. There are also two butterflies fluttering around the scene. The background is white. The overall style of the illustration is cartoon-like and playful.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F10" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A5.F10.3.2" class="ltx_text" style="font-size:90%;">Detailed captioning prediction results.</span></figcaption>
</figure>
</div>
</section>
<section id="A5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Visual Grounding</h3>

<div id="A5.SS2.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS2.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS2.1.p1.pic1" class="ltx_picture" height="778.88" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,778.88) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 760.84 C 0 764.1 2.64 766.74 5.91 766.74 L 650.23 766.74 C 653.49 766.74 656.13 764.1 656.13 760.84 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 760.84 C 1.97 763.01 3.73 764.77 5.91 764.77 L 650.23 764.77 C 652.4 764.77 654.16 763.01 654.16 760.84 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 756.74)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 128.12 22.14 C 129.75 22.14 131.07 20.82 131.07 19.19 L 131.07 2.95 C 131.07 1.32 129.75 0 128.12 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 128.12 22.14 C 129.75 22.14 131.07 20.82 131.07 19.19 L 131.07 2.95 C 131.07 1.32 129.75 0 128.12 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Visual Grounding</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="733.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3" class="ltx_para">
<span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1" class="ltx_p"><span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate the phrases in the caption: {caption}</span>
<span id="A5.SS2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_6.png" id="A5.SS2.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="177" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">               The image shows a group of five cartoon monsters. On the left side, there is <span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1" class="ltx_text" style="background-color:#00C6C5;">a brown monster<sup id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1.1" class="ltx_sup">1</sup></span> with horns and a big smile on its face. Next to it, there are two <span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2" class="ltx_text" style="background-color:#49E4AB;">smaller monsters<sup id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2.1" class="ltx_sup">2</sup></span>, one black and one green. <span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3" class="ltx_text" style="background-color:#C3FEA8;">The black monster<sup id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3.1" class="ltx_sup">3</sup></span> has two large horns on its head and is standing in the center of the group. <span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4" class="ltx_text" style="background-color:#FFC182;">The green monster<sup id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4.1" class="ltx_sup">4</sup></span> on the right side is a green monster with big eyes and a long antennae. It is standing on its hind legs with its arms stretched out to the sides. In the middle of the image, there appears to be <span id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5" class="ltx_text" style="background-color:#FF917C;">a small blue monster<sup id="A5.SS2.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5.1" class="ltx_sup">5</sup></span> with a round head and two antennae on its back. The background is light beige with small green circles scattered around.
</pre>
</span>
</span>
<span id="A5.SS2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A5.SS2.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4" class="ltx_para">
<span id="A5.SS2.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">        
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_7.png" id="A5.SS2.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="288" height="216" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">                 The image shows a cluttered room with a black and white checkered <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1" class="ltx_text" style="background-color:#00C6C5;">floor<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1.1" class="ltx_sup">1</sup></span>. On the right side of the image, there is <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2" class="ltx_text" style="background-color:#49E4AB;">a small white cabinet<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2.1" class="ltx_sup">2</sup></span> with a <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3" class="ltx_text" style="background-color:#C3FEA8;">television<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3.1" class="ltx_sup">3</sup></span> on top of it. Next to the cabinet, there are <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4" class="ltx_text" style="background-color:#FFF7AB;">several items<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4.1" class="ltx_sup">4</sup></span> scattered on the floor, including a red <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5" class="ltx_text" style="background-color:#FFC182;">blanket<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5.1" class="ltx_sup">5</sup></span>, <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6" class="ltx_text" style="background-color:#FF917C;">a wooden stool<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6.1" class="ltx_sup">6</sup></span>, and a pile of trash. On top of the cabinet is <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7" class="ltx_text" style="background-color:#FF6E9B;">a picture frame<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7.1" class="ltx_sup">7</sup></span> and a <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8" class="ltx_text" style="background-color:#0676A8;">hat<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8.1" class="ltx_sup">8</sup></span>. In the center of the room is <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.9" class="ltx_text" style="background-color:#0BC0CC;">a white refrigerator<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.9.1" class="ltx_sup">9</sup></span> with a few items on top. <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.10" class="ltx_text" style="background-color:#9BFECB;">The walls<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.10.1" class="ltx_sup">10</sup></span> are painted white and there are <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.11" class="ltx_text" style="background-color:#FF7B75;">a few clothes<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.11.1" class="ltx_sup">11</sup></span> hanging on a <span id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.12" class="ltx_text" style="background-color:#FF428A;">rack<sup id="A5.SS2.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.12.1" class="ltx_sup">12</sup></span> on the left wall. The room appears to be in disarray, with some items strewn about and others scattered around.
</pre>
</span>
</span>
<span id="A5.SS2.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2" class="ltx_para ltx_noindent"><svg id="A5.SS2.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5" class="ltx_para">
<span id="A5.SS2.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_8.png" id="A5.SS2.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="231" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">           The image shows a kitchen countertop with various kitchen items on it. On the left side of the countertop, there is a microscope with a black body and a white <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1" class="ltx_text" style="background-color:#00C6C5;">lens<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1.1" class="ltx_sup">1</sup></span>. Next to the microscope, there are two bottles of <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2" class="ltx_text" style="background-color:#49E4AB;">condiments<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2.1" class="ltx_sup">2</sup></span> - one with <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3" class="ltx_text" style="background-color:#FF917C;">a red label<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3.1" class="ltx_sup">3</sup></span><span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4" class="ltx_text" style="background-color:#FF6E9B;"><sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4.1" class="ltx_sup">4</sup></span> and the other with green. On top of the microscope is <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5" class="ltx_text" style="background-color:#0676A8;">a yellow banana<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5.1" class="ltx_sup">5</sup></span>, <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6" class="ltx_text" style="background-color:#0BC0CC;">a blue spatula<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6.1" class="ltx_sup">6</sup></span>, <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7" class="ltx_text" style="background-color:#9BFECB;">a red plate<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7.1" class="ltx_sup">7</sup></span>, and <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8" class="ltx_text" style="background-color:#FFE19E;">a yellow corn<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8.1" class="ltx_sup">8</sup></span><span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9" class="ltx_text" style="background-color:#FF7B75;"><sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9.1" class="ltx_sup">9</sup></span> on the cob. In the center of the image, there appears to be <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10" class="ltx_text" style="background-color:#FF428A;">a frying pan<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10.1" class="ltx_sup">10</sup></span> with a <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11" class="ltx_text" style="background-color:#AF299D;">fried egg<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11.1" class="ltx_sup">11</sup></span> on it, and on the right side is <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12" class="ltx_text" style="background-color:#99FFB1;">a white sink<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12.1" class="ltx_sup">12</sup></span> with a white <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13" class="ltx_text" style="background-color:#99C2FF;">faucet<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13.1" class="ltx_sup">13</sup></span>. <span id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.14" class="ltx_text" style="background-color:#FFBC99;">The countertop<sup id="A5.SS2.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.14.1" class="ltx_sup">14</sup></span> is made of wood and has a gray tile backsplash.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F11" class="ltx_figure">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A5.F11.3.2" class="ltx_text" style="font-size:90%;">Visual grounding prediction results.</span></figcaption>
</figure>
</div>
<div id="A5.SS2.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS2.2.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS2.2.p1.pic1" class="ltx_picture" height="848.16" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,848.16) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 830.11 C 0 833.38 2.64 836.02 5.91 836.02 L 650.23 836.02 C 653.49 836.02 656.13 833.38 656.13 830.11 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 830.11 C 1.97 832.29 3.73 834.05 5.91 834.05 L 650.23 834.05 C 652.4 834.05 654.16 832.29 654.16 830.11 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 826.02)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 128.12 22.14 C 129.75 22.14 131.07 20.82 131.07 19.19 L 131.07 2.95 C 131.07 1.32 129.75 0 128.12 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 128.12 22.14 C 129.75 22.14 131.07 20.82 131.07 19.19 L 131.07 2.95 C 131.07 1.32 129.75 0 128.12 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Visual Grounding</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="802.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3" class="ltx_para">
<span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1" class="ltx_p"><span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate the phrases in the caption: {caption}</span>
<span id="A5.SS2.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_1.png" id="A5.SS2.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="206" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">   The image is a flat lay of various food items arranged on a white marble countertop. On the left side of the image, there is <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1" class="ltx_text" style="background-color:#00C6C5;">a piece of salmon<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1.1" class="ltx_sup">1</sup></span>. Next to it, there are <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2" class="ltx_text" style="background-color:#49E4AB;">slices of cheese<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2.1" class="ltx_sup">2</sup></span>, <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3" class="ltx_text" style="background-color:#FFF7AB;">a glass of oil<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3.1" class="ltx_sup">3</sup></span>, <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4" class="ltx_text" style="background-color:#FFC182;">coffee beans<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4.1" class="ltx_sup">4</sup></span>, <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5" class="ltx_text" style="background-color:#FF917C;">a zucchini<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5.1" class="ltx_sup">5</sup></span>, a bunch of <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.6" class="ltx_text" style="background-color:#FF6E9B;">strawberries<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.6.1" class="ltx_sup">6</sup></span>, two <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.7" class="ltx_text" style="background-color:#0BC0CC;">chicken breasts<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.7.1" class="ltx_sup">7</sup></span>, <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.8" class="ltx_text" style="background-color:#9BFECB;">a avocado<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.8.1" class="ltx_sup">8</sup></span> and <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.9" class="ltx_text" style="background-color:#FFE19E;">a few whole spinach leaves<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.9.1" class="ltx_sup">9</sup></span>. In the center of the table, there appears to be  <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.10" class="ltx_text" style="background-color:#FF428A;">a pile of ground beef<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.10.1" class="ltx_sup">10</sup></span> on <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.11" class="ltx_text" style="background-color:#AF299D;">paper<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.11.1" class="ltx_sup">11</sup></span>, two <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.12" class="ltx_text" style="background-color:#99FFB1;">eggs<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.12.1" class="ltx_sup">12</sup></span>, two <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.13" class="ltx_text" style="background-color:#B299FF;">orange bell peppers<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.13.1" class="ltx_sup">13</sup></span>, and <span id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.14" class="ltx_text" style="background-color:#99FFD5;">some dark chocolate bars<sup id="A5.SS2.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.14.1" class="ltx_sup">14</sup></span>. The items are arranged in a way that suggests they are being prepared for a meal.

</pre>
</span>
</span>
<span id="A5.SS2.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A5.SS2.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4" class="ltx_para">
<span id="A5.SS2.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">        
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_2.png" id="A5.SS2.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="288" height="296" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">             The image shows a modern kitchen with a large window on the left side. <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1" class="ltx_text" style="background-color:#00C6C5;">The window<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1.1" class="ltx_sup">1</sup></span> has a view of trees and greenery outside. On the left side of the image, there is <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2" class="ltx_text" style="background-color:#FFF7AB;">a blue sofa<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2.1" class="ltx_sup">2</sup></span> with a wooden coffee table in front of it. Above the table, there are <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3" class="ltx_text" style="background-color:#FFC182;">three copper pendant lights<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3.1" class="ltx_sup">3</sup></span> hanging from the ceiling. There is <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4" class="ltx_text" style="background-color:#FF917C;">a large island<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4.1" class="ltx_sup">4</sup></span> with a white countertop. There are <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5" class="ltx_text" style="background-color:#FF6E9B;">two bar stools<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5.1" class="ltx_sup">5</sup></span> next to the table. In the center of the kitchen, there is <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6" class="ltx_text" style="background-color:#0676A8;">a bottle green plants<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6.1" class="ltx_sup">6</sup></span> on the table. <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7" class="ltx_text" style="background-color:#0BC0CC;">The floor<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7.1" class="ltx_sup">7</sup></span> is made of light-colored wood and <span id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8" class="ltx_text" style="background-color:#FFE19E;">the walls<sup id="A5.SS2.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8.1" class="ltx_sup">8</sup></span> are painted in a dark blue color.
<br class="ltx_break">%
</pre>
</span>
</span>
<span id="A5.SS2.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2" class="ltx_para ltx_noindent"><svg id="A5.SS2.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5" class="ltx_para">
<span id="A5.SS2.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/grounding_5.png" id="A5.SS2.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="206" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">         The image shows a <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1" class="ltx_text" style="background-color:#00C6C5;">man<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1.1" class="ltx_sup">1</sup></span> standing in a kitchen with a small dog. <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2" class="ltx_text" style="background-color:#00C6C5;">The man<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2.1" class="ltx_sup">1</sup></span> is wearing a plaid <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3" class="ltx_text" style="background-color:#49E4AB;">shirt<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3.1" class="ltx_sup">2</sup></span> and <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4" class="ltx_text" style="background-color:#C3FEA8;">jeans<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4.1" class="ltx_sup">3</sup></span> and is holding a red <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5" class="ltx_text" style="background-color:#FFF7AB;">cup<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5.1" class="ltx_sup">4</sup></span> in his hand. <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6" class="ltx_text" style="background-color:#FFC182;">The dog<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6.1" class="ltx_sup">5</sup></span> is a light brown color and is standing on a tiled <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7" class="ltx_text" style="background-color:#FF917C;">floor<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7.1" class="ltx_sup">6</sup></span>. <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8" class="ltx_text" style="background-color:#FF6E9B;">The kitchen<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8.1" class="ltx_sup">7</sup></span> has wooden <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9" class="ltx_text" style="background-color:#0676A8;">cabinets<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9.1" class="ltx_sup">8</sup></span> and a <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10" class="ltx_text" style="background-color:#9BFECB;">countertop<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10.1" class="ltx_sup">9</sup></span> with various kitchen utensils hanging on the wall. There is <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11" class="ltx_text" style="background-color:#FFE19E;">a window<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11.1" class="ltx_sup">10</sup></span> with yellow <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12" class="ltx_text" style="background-color:#FF7B75;">curtains<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12.1" class="ltx_sup">11</sup></span> in the background. On the right side of the image, there is <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13" class="ltx_text" style="background-color:#FF428A;">a wooden cutting board<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13.1" class="ltx_sup">12</sup></span> and a wooden <span id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.14" class="ltx_text" style="background-color:#AF299D;">stool<sup id="A5.SS2.2.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.14.1" class="ltx_sup">13</sup></span>.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F12" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A5.F12.3.2" class="ltx_text" style="font-size:90%;">Visual grounding prediction results. (continued)</span></figcaption>
</figure>
</div>
</section>
<section id="A5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.3 </span>Dense Region Caption</h3>

<div id="A5.SS3.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS3.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS3.1.p1.pic1" class="ltx_picture" height="865.06" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,865.06) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 847.16 C 0 850.42 2.64 853.07 5.91 853.07 L 650.23 853.07 C 653.49 853.07 656.13 850.42 656.13 847.16 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 847.16 C 1.97 849.34 3.73 851.1 5.91 851.1 L 650.23 851.1 C 652.4 851.1 654.16 849.34 654.16 847.16 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 843.07)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.04 C 0 20.67 1.32 21.99 2.95 21.99 L 156.04 21.99 C 157.67 21.99 158.99 20.67 158.99 19.04 L 158.99 2.95 C 158.99 1.32 157.67 0 156.04 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.04 C 0 20.67 1.32 21.99 2.95 21.99 L 156.04 21.99 C 157.67 21.99 158.99 20.67 158.99 19.04 L 158.99 2.95 C 158.99 1.32 157.67 0 156.04 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Dense Region Caption</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="819.55" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS3.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS3.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.p3" class="ltx_para">
<span id="A5.SS3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_1.png" id="A5.SS3.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="309" height="232" alt="[Uncaptioned image]">
</span>
<span id="A5.SS3.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_2.png" id="A5.SS3.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="343" height="229" alt="[Uncaptioned image]">
</span>
</span>
<span id="A5.SS3.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p1" class="ltx_para ltx_noindent"><svg id="A5.SS3.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS3.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.2.p4" class="ltx_para">
<span id="A5.SS3.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.3" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_3.png" id="A5.SS3.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.3.g1" class="ltx_graphics ltx_img_landscape" width="274" height="206" alt="[Uncaptioned image]">
</span>
<span id="A5.SS3.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.4" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_4.png" id="A5.SS3.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.4.g1" class="ltx_graphics ltx_img_landscape" width="261" height="174" alt="[Uncaptioned image]">
</span>
</span>
<span id="A5.SS3.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.p2" class="ltx_para ltx_noindent"><svg id="A5.SS3.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS3.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.3.p5" class="ltx_para">
<span id="A5.SS3.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.5" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_5.png" id="A5.SS3.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.5.g1" class="ltx_graphics ltx_img_portrait" width="240" height="326" alt="[Uncaptioned image]">
</span>
<span id="A5.SS3.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.6" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/dense_cap_6.png" id="A5.SS3.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.6.g1" class="ltx_graphics ltx_img_portrait" width="240" height="320" alt="[Uncaptioned image]">
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F13" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A5.F13.3.2" class="ltx_text" style="font-size:90%;">Dense region caption prediction results.</span></figcaption>
</figure>
</div>
</section>
<section id="A5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.4 </span>Open Vocabulary Detection</h3>

<div id="A5.SS4.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS4.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS4.1.p1.pic1" class="ltx_picture" height="860.23" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,860.23) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 842.18 C 0 845.45 2.64 848.09 5.91 848.09 L 650.23 848.09 C 653.49 848.09 656.13 845.45 656.13 842.18 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 842.18 C 1.97 844.36 3.73 846.12 5.91 846.12 L 650.23 846.12 C 652.4 846.12 654.16 844.36 654.16 842.18 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 838.09)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 234.99 22.14 C 236.62 22.14 237.94 20.82 237.94 19.19 L 237.94 2.95 C 237.94 1.32 236.62 0 234.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 234.99 22.14 C 236.62 22.14 237.94 20.82 237.94 19.19 L 237.94 2.95 C 237.94 1.32 236.62 0 234.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Open Vocabulary Object Detection</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="814.57" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS4.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS4.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.1.p3" class="ltx_para">
<span id="A5.SS4.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;">
<span id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_p"><span id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate <span id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_text" style="background-color:#E6E6E6;">Five Alive juice box</span><math id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\langle</annotation></semantics></math>and<math id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">\rangle</annotation></semantics></math><span id="A5.SS4.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3" class="ltx_text" style="background-color:#E6E6E6;">Colgate toothpaste</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_1.png" id="A5.SS4.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="174" alt="[Uncaptioned image]">
</span>
<span id="A5.SS4.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;">
<span id="A5.SS4.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1" class="ltx_p"><span id="A5.SS4.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate <span id="A5.SS4.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.2" class="ltx_text" style="background-color:#E6E6E6;">Chewbacca</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_2.png" id="A5.SS4.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="343" height="193" alt="[Uncaptioned image]">
</span>
</span>
<span id="A5.SS4.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.p1" class="ltx_para ltx_noindent"><svg id="A5.SS4.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS4.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.2.p4" class="ltx_para">
<span id="A5.SS4.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.5" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;">
<span id="A5.SS4.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.5.1" class="ltx_p"><span id="A5.SS4.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.5.1.1" class="ltx_text ltx_font_bold">Prompt</span>:
Locate <span id="A5.SS4.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.5.1.2" class="ltx_text" style="background-color:#E6E6E6;">giraffe</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_3.png" id="A5.SS4.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="274" height="206" alt="[Uncaptioned image]">
</span>
<span id="A5.SS4.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.10" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;">
<span id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4" class="ltx_p"><span id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.1" class="ltx_text ltx_font_bold">Prompt</span>:
Locate <span id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.2" class="ltx_text" style="background-color:#E6E6E6;">Mercedes-Benz</span><math id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1.1" xref="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1b"><ci id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1.1.cmml" xref="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.6.1.m1.1c">\langle</annotation></semantics></math>and<math id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1.1" xref="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1b"><ci id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1.1.cmml" xref="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.7.2.m2.1c">\rangle</annotation></semantics></math><span id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.3" class="ltx_text" style="background-color:#E6E6E6;">M2</span><math id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1.1" xref="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1b"><ci id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1.1.cmml" xref="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.8.3.m3.1c">\langle</annotation></semantics></math>and<math id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1a"><mo stretchy="false" id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1.1" xref="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1b"><ci id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1.1.cmml" xref="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.m4.1c">\rangle</annotation></semantics></math><span id="A5.SS4.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.9.4.4" class="ltx_text" style="background-color:#E6E6E6;">Audi</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_4.png" id="A5.SS4.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="261" height="261" alt="[Uncaptioned image]">
</span>
</span>
<span id="A5.SS4.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.p2" class="ltx_para ltx_noindent"><svg id="A5.SS4.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS4.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.3.p5" class="ltx_para">
<span id="A5.SS4.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.11" class="ltx_block ltx_parbox ltx_align_top" style="width:223.6pt;">
<span id="A5.SS4.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.11.1" class="ltx_p"><span id="A5.SS4.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.11.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate the <span id="A5.SS4.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.11.1.2" class="ltx_text" style="background-color:#E6E6E6;">objects with category name</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_5.png" id="A5.SS4.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="206" alt="[Uncaptioned image]">
</span>
<span id="A5.SS4.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.12" class="ltx_block ltx_parbox ltx_align_top" style="width:248.4pt;">
<span id="A5.SS4.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.12.1" class="ltx_p"><span id="A5.SS4.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.12.1.1" class="ltx_text ltx_font_bold">Prompt</span>: Locate the <span id="A5.SS4.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.12.1.2" class="ltx_text" style="background-color:#E6E6E6;">objects with category name</span> in the image.</span><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/od_6.png" id="A5.SS4.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="222" alt="[Uncaptioned image]">
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F14" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A5.F14.3.2" class="ltx_text" style="font-size:90%;">Open vocabulary object detection prediction results.</span></figcaption>
</figure>
</div>
</section>
<section id="A5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.5 </span>OCR</h3>

<div id="A5.SS5.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS5.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS5.1.p1.pic1" class="ltx_picture" height="1028.37" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,1028.37) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 1010.32 C 0 1013.58 2.64 1016.23 5.91 1016.23 L 650.23 1016.23 C 653.49 1016.23 656.13 1013.58 656.13 1010.32 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 1010.32 C 1.97 1012.5 3.73 1014.26 5.91 1014.26 L 650.23 1014.26 C 652.4 1014.26 654.16 1012.5 654.16 1010.32 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 1006.23)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 116.07 22.14 C 117.7 22.14 119.02 20.82 119.02 19.19 L 119.02 2.95 C 119.02 1.32 117.7 0 116.07 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 116.07 22.14 C 117.7 22.14 119.02 20.82 119.02 19.19 L 119.02 2.95 C 119.02 1.32 117.7 0 116.07 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Ocr with region</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="982.71" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3" class="ltx_para">
<span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1" class="ltx_p"><span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.1.1" class="ltx_text ltx_font_bold">Prompt</span>: What is the text in the image, with regions?</span>
<span id="A5.SS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ocr_1.png" id="A5.SS5.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="309" height="209" alt="[Uncaptioned image]">
</span>
<span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1" class="ltx_text" style="background-color:#00C6C5;">Easy Stroganoff<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.1.1" class="ltx_sup">1</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2" class="ltx_text" style="background-color:#49E4AB;">Brown - 1 lb. ground beef in skillet<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.2.1" class="ltx_sup">2</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3" class="ltx_text" style="background-color:#C3FEA8;">Add - 1 can beef broth<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.3.1" class="ltx_sup">3</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4" class="ltx_text" style="background-color:#FFF7AB;">1 can cream of mushroom soup<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.4.1" class="ltx_sup">4</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5" class="ltx_text" style="background-color:#FFC182;">Cut in squares &amp; 2dld to above -<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.5.1" class="ltx_sup">5</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.6" class="ltx_text" style="background-color:#FF917C;">1/ Boz pkg. cream cheese<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.6.1" class="ltx_sup">6</sup></span> 
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.7" class="ltx_text" style="background-color:#FF6E9B;">Simmer - 20-30 min.<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.7.1" class="ltx_sup">7</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.8" class="ltx_text" style="background-color:#0676A8;">Serve over hotrice /noodles.<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.8.1" class="ltx_sup">8</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.9" class="ltx_text" style="background-color:#0BC0CC;">Vintage. Recipes/Easy-Stroganof<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.9.1" class="ltx_sup">9</sup></span>
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.10" class="ltx_text" style="background-color:#9BFECB;">Charlotte Miller<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.10.1" class="ltx_sup">10</sup></span>
<br class="ltx_break">
      
     <span id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.11" class="ltx_text" style="background-color:#FFE19E;">Tulsa<sup id="A5.SS5.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p3.2.1.11.1" class="ltx_sup">11</sup></span>
    </pre>
</span>
</span>
<span id="A5.SS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A5.SS5.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4" class="ltx_para">
<span id="A5.SS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ocr_4.png" id="A5.SS5.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.2.g1" class="ltx_graphics ltx_img_landscape" width="309" height="216" alt="[Uncaptioned image]">
</span>
<span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1" class="ltx_text" style="background-color:#00C6C5;">COFFEE+TEA<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.1.1" class="ltx_sup">1</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2" class="ltx_text" style="background-color:#49E4AB;">BLENDED<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.2.1" class="ltx_sup">2</sup></span>
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3" class="ltx_text" style="background-color:#C3FEA8;">$1.69/$1.89/$2.09<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.3.1" class="ltx_sup">3</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4" class="ltx_text" style="background-color:#FFF7AB;">$3.49/$3.99<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.4.1" class="ltx_sup">4</sup></span>
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5" class="ltx_text" style="background-color:#FFC182;">Hot Coffee/Tea<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.5.1" class="ltx_sup">5</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6" class="ltx_text" style="background-color:#FF917C;">Taro<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.6.1" class="ltx_sup">6</sup></span>
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7" class="ltx_text" style="background-color:#FF6E9B;">Iced Coffee/ Tea<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.7.1" class="ltx_sup">7</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8" class="ltx_text" style="background-color:#0676A8;">Mango<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.8.1" class="ltx_sup">8</sup></span>
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.9" class="ltx_text" style="background-color:#0BC0CC;">Hot Chocolate<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.9.1" class="ltx_sup">9</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.10" class="ltx_text" style="background-color:#9BFECB;">Honeydew<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.10.1" class="ltx_sup">10</sup></span> 
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.11" class="ltx_text" style="background-color:#FFE19E;">$3,49/$ 3.99<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.11.1" class="ltx_sup">11</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.12" class="ltx_text" style="background-color:#FF7B75;">Strawberry<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.12.1" class="ltx_sup">12</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.13" class="ltx_text" style="background-color:#AF299D;">Mocha<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.13.1" class="ltx_sup">14</sup></span>
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.14" class="ltx_text" style="background-color:#FF428A;">Thai Iced Tea / Coffee<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.14.1" class="ltx_sup">13</sup></span>
       
      <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.15" class="ltx_text" style="background-color:#99FFB1;">Caramel<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.15.1" class="ltx_sup">15</sup></span> 
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.16" class="ltx_text" style="background-color:#99C2FF;">$1,99/$2,29/$2:59<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.16.1" class="ltx_sup">16</sup></span>
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.17" class="ltx_text" style="background-color:#FFBC99;">SPECIALTY Brew !!<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.17.1" class="ltx_sup">17</sup></span>  
<br class="ltx_break">
       <span id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.18" class="ltx_text" style="background-color:#B299FF;">Jasmine GreenTea<sup id="A5.SS5.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p4.1.1.18.1" class="ltx_sup">18</sup></span>
    </pre>
</span>
</span>
<span id="A5.SS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2" class="ltx_para ltx_noindent"><svg id="A5.SS5.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5" class="ltx_para">
<span id="A5.SS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ocr_2.png" id="A5.SS5.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.3.g1" class="ltx_graphics ltx_img_portrait" width="247" height="371" alt="[Uncaptioned image]">
</span>
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"> 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1" class="ltx_text" style="background-color:#00C6C5;">LEONARDO<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.1.1" class="ltx_sup">1</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2" class="ltx_text" style="background-color:#49E4AB;">DiCAPRIO<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.2.1" class="ltx_sup">2</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3" class="ltx_text" style="background-color:#C3FEA8;">ROBERT<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.3.1" class="ltx_sup">3</sup></span>
<br class="ltx_break">
 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4" class="ltx_text" style="background-color:#FFF7AB;">DE NIRO<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.4.1" class="ltx_sup">4</sup></span>
<br class="ltx_break">
 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5" class="ltx_text" style="background-color:#FFC182;">LILY<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.5.1" class="ltx_sup">5</sup></span>
<br class="ltx_break">
 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6" class="ltx_text" style="background-color:#FF917C;">GLADSTONE<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.6.1" class="ltx_sup">6</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7" class="ltx_text" style="background-color:#FF6E9B;">A MARTIN SCORSESE PICTURE<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.7.1" class="ltx_sup">7</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8" class="ltx_text" style="background-color:#0676A8;">KILLERS<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.8.1" class="ltx_sup">8</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9" class="ltx_text" style="background-color:#0BC0CC;">OF<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.9.1" class="ltx_sup">9</sup></span>  <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10" class="ltx_text" style="background-color:#9BFECB;">FLOWER<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.10.1" class="ltx_sup">10</sup></span>
<br class="ltx_break">
 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11" class="ltx_text" style="background-color:#FFE19E;">MOON<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.11.1" class="ltx_sup">11</sup></span>
<br class="ltx_break">
 <span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12" class="ltx_text" style="background-color:#FF7B75;">SCREENLY ERIC ROTH AND MARTIIN SCORSESE DIRECTED BYMARTIN SORSESE<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.12.1" class="ltx_sup">12</sup></span>
<br class="ltx_break">
 
<span id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13" class="ltx_text" style="background-color:#FF428A;">ONLY IN THEATRES OCTOBER 20<sup id="A5.SS5.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.3.p5.1.1.13.1" class="ltx_sup">13</sup></span>
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F15" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A5.F15.3.2" class="ltx_text" style="font-size:90%;">OCR with region prediction results.</span></figcaption>
</figure>
</div>
</section>
<section id="A5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.6 </span>Region to segmentation</h3>

<div id="A5.SS6.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A5.SS6.1.p1" class="ltx_para ltx_noindent">
<svg id="A5.SS6.1.p1.pic1" class="ltx_picture" height="812.01" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,812.01) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 794.12 C 0 797.38 2.64 800.03 5.91 800.03 L 650.23 800.03 C 653.49 800.03 656.13 797.38 656.13 794.12 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 794.12 C 1.97 796.3 3.73 798.06 5.91 798.06 L 650.23 798.06 C 652.4 798.06 654.16 796.3 654.16 794.12 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 790.03)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.04 C 0 20.67 1.32 21.99 2.95 21.99 L 165.77 21.99 C 167.4 21.99 168.72 20.67 168.72 19.04 L 168.72 2.95 C 168.72 1.32 167.4 0 165.77 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.04 C 0 20.67 1.32 21.99 2.95 21.99 L 165.77 21.99 C 167.4 21.99 168.72 20.67 168.72 19.04 L 168.72 2.95 C 168.72 1.32 167.4 0 165.77 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Region to Segmentation</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="766.5" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A5.SS6.1.p1.pic1.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.3" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A5.SS6.1.p1.pic1.57.57.57.57.57.57.57.57.57.57.57.57.57.57.57.57.57.57.1.p3" class="ltx_para">
<span id="A5.SS6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region
<math id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">\langle</annotation></semantics></math>loc_586<math id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1" xref="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m3.1c">\langle</annotation></semantics></math>loc_294<math id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1" xref="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1" xref="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m5.1c">\langle</annotation></semantics></math>loc_929<math id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1" xref="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1" xref="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m7.1c">\langle</annotation></semantics></math>loc_814<math id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1" xref="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_1.png" id="A5.SS6.1.p1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="343" height="200" alt="[Uncaptioned image]"></span>
<span id="A5.SS6.1.p1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region <math id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1.1" xref="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.m1.1c">\langle</annotation></semantics></math>loc_317<math id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1.1" xref="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1.1" xref="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.3.m3.1c">\langle</annotation></semantics></math>loc_314<math id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1.1" xref="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1.1" xref="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.5.m5.1c">\langle</annotation></semantics></math>loc_893<math id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1.1" xref="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1.1" xref="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.7.m7.1c">\langle</annotation></semantics></math>loc_904<math id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1.1" xref="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_2.png" id="A5.SS6.1.p1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="174" alt="[Uncaptioned image]"></span>
</span>
<span id="A5.SS6.1.p1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.p1" class="ltx_para ltx_noindent"><svg id="A5.SS6.1.p1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS6.1.p1.pic1.58.58.58.58.58.58.58.58.58.58.58.58.58.58.58.58.58.58.2.p4" class="ltx_para">
<span id="A5.SS6.1.p1.pic1.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.27.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.27.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region
<math id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1.1" xref="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.19.1.m1.1c">\langle</annotation></semantics></math>loc_541<math id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1.1" xref="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.20.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1.1" xref="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.21.3.m3.1c">\langle</annotation></semantics></math>loc_266<math id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1.1" xref="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.22.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1.1" xref="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.23.5.m5.1c">\langle</annotation></semantics></math>loc_692<math id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1.1" xref="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.24.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1.1" xref="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.26.25.7.m7.1c">\langle</annotation></semantics></math>loc_627<math id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1.1" xref="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.27.26.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_3.png" id="A5.SS6.1.p1.pic1.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.28.27.9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="343" height="229" alt="[Uncaptioned image]"></span>
<span id="A5.SS6.1.p1.pic1.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.36.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.36.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region
<math id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1.1" xref="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.29.28.1.m1.1c">\langle</annotation></semantics></math>loc_583<math id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1.1" xref="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.29.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1.1" xref="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.31.30.3.m3.1c">\langle</annotation></semantics></math>loc_66<math id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1.1" xref="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.32.31.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1.1" xref="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.33.32.5.m5.1c">\langle</annotation></semantics></math>loc_794<math id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1.1" xref="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.34.33.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1.1" xref="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.35.34.7.m7.1c">\langle</annotation></semantics></math>loc_331<math id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1.1" xref="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.36.35.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_4.png" id="A5.SS6.1.p1.pic1.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.37.36.9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="206" height="274" alt="[Uncaptioned image]"></span>
</span>
<span id="A5.SS6.1.p1.pic1.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.p2" class="ltx_para ltx_noindent"><svg id="A5.SS6.1.p1.pic1.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.38.p2.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A5.SS6.1.p1.pic1.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.59.3.p5" class="ltx_para">
<span id="A5.SS6.1.p1.pic1.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.45.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.45.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region <math id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1.1" xref="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.39.37.1.m1.1c">\langle</annotation></semantics></math>loc_386<math id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1.1" xref="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.40.38.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1.1" xref="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.41.39.3.m3.1c">\langle</annotation></semantics></math>loc_53<math id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1.1" xref="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.42.40.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1.1" xref="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.43.41.5.m5.1c">\langle</annotation></semantics></math>loc_759<math id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1.1" xref="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.44.42.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1.1" xref="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.45.43.7.m7.1c">\langle</annotation></semantics></math>loc_998<math id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1.1" xref="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.46.44.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_5.png" id="A5.SS6.1.p1.pic1.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.47.45.9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="174" alt="[Uncaptioned image]"></span>
<span id="A5.SS6.1.p1.pic1.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.54.9" class="ltx_p ltx_parbox ltx_align_top" style="width:248.4pt;"><span id="A5.SS6.1.p1.pic1.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.54.9.1" class="ltx_text ltx_font_bold">Prompt</span>:
What is the polygon mask of region <math id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1.1" xref="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1b"><ci id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1.1.cmml" xref="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.48.46.1.m1.1c">\langle</annotation></semantics></math>loc_102<math id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1.1" xref="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1b"><ci id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1.1.cmml" xref="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.49.47.2.m2.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1.1" xref="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1b"><ci id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1.1.cmml" xref="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.50.48.3.m3.1c">\langle</annotation></semantics></math>loc_7<math id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1.1" xref="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1b"><ci id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1.1.cmml" xref="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.51.49.4.m4.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1.1" xref="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1b"><ci id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1.1.cmml" xref="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.52.50.5.m5.1c">\langle</annotation></semantics></math>loc_375<math id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1.1" xref="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1b"><ci id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1.1.cmml" xref="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.53.51.6.m6.1c">\rangle</annotation></semantics></math><math id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1.1" xref="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1b"><ci id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1.1.cmml" xref="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.54.52.7.m7.1c">\langle</annotation></semantics></math>loc_648<math id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1a"><mo stretchy="false" id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1.1" xref="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1b"><ci id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1.1.cmml" xref="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.SS6.1.p1.pic1.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.55.53.8.m8.1c">\rangle</annotation></semantics></math>
<img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/seg_6.png" id="A5.SS6.1.p1.pic1.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.56.54.9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="309" height="155" alt="[Uncaptioned image]"></span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A5.F16" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A5.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A5.F16.3.2" class="ltx_text" style="font-size:90%;">Region to segmentation prediction results.</span></figcaption>
</figure>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Comparision with LMMs on Detailed Caption Task</h2>

<div id="A6.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A6.1.p1" class="ltx_para ltx_noindent">
<svg id="A6.1.p1.pic1" class="ltx_picture" height="703.03" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,703.03) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 684.99 C 0 688.25 2.64 690.89 5.91 690.89 L 650.23 690.89 C 653.49 690.89 656.13 688.25 656.13 684.99 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 684.99 C 1.97 687.16 3.73 688.92 5.91 688.92 L 650.23 688.92 C 652.4 688.92 654.16 687.16 654.16 684.99 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 680.89)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison with LMMs on Detailed Image Caption Task</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="657.37" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A6.1.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.p2" class="ltx_para">
<span id="A6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/x15.jpg" id="A6.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="365" height="487" alt="[Uncaptioned image]">
</span>
</span>
<span id="A6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A6.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3" class="ltx_para">
<span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><pre id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><em id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1.1" class="ltx_emph ltx_font_serif ltx_font_bold ltx_font_italic">Florence-2</em>: The image is a photograph of a handwritten note from a person named Angelica. The note is written in black ink on a white background. The handwriting is neat and legible, and it appears to be a handwritten letter. The person’s name, Jonathan, is written at the top of the note in a cursive font. Below the name, there is a note that reads, "I’ve got to admit, London’s lights have nothing on our family get together. I miss that, I miss you guys too. Let’s not get too happy about it though I’m keeping my fingers crossed for a trip back stateside. Give a big hug to Amelia for me. Will you?"
</pre>
<pre id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2.1" class="ltx_text ltx_font_serif ltx_font_bold">GPT-4V<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">1</span></span><a target="_blank" href="https://chat.openai.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://chat.openai.com</a></span></span></span></span>: The image shows a handwritten note addressed to Jonathan. The note conveys a personal message from the sender, Angelica, who admits to missing family gatherings and the feeling of home, contrasting it with the lights of London. Angelica expresses a longing to return stateside and asks Jonathan to give Amelia a big hug on her behalf. The tone of the note is casual and warm, with a sense of fondness and a hint of homesickness. The note is written informally, suggesting a close relationship between the sender and the recipients.
</pre>
<pre id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3.1" class="ltx_text ltx_font_serif ltx_font_bold">LLaVA<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note"><span id="footnote2.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">2</span></span><a target="_blank" href="https://llava.hliu.cc/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://llava.hliu.cc</a></span></span></span>:</span> The image is a handwritten letter on a piece of paper, written by a person named Jonathan. The letter is addressed to someone named Angela, and it appears to be a heartfelt message expressing love and affection. The letter is written in cursive, giving it a personal and intimate touch.
</pre>
<pre id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.1.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4.1" class="ltx_text ltx_font_serif ltx_font_bold">MiniGPT-4<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">3</span></span><a target="_blank" href="https://huggingface.co/spaces/Vision-CAIR/minigpt4" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://huggingface.co/spaces/Vision-CAIR/minigpt4</a></span></span></span>:</span> The image is a handwritten letter from an unknown person to someone named Angel. The letter is written in black ink on a piece of white paper. The letter is addressed to Angel and begins with the words, “Dear Angel,”. The letter goes on to describe the writer’s feelings about a recent event or situation, but the details are unclear. The letter ends with the words, “Sincerely, [Writer’s Name].”

</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A6.F17" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A6.F17.2.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A6.F17.3.2" class="ltx_text" style="font-size:90%;">Comparison with LMMs on detailed caption task.</span></figcaption>
</figure>
</div>
<div id="A6.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A6.2.p1" class="ltx_para ltx_noindent">
<svg id="A6.2.p1.pic1" class="ltx_picture" height="661.63" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,661.63) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 643.59 C 0 646.85 2.64 649.49 5.91 649.49 L 650.23 649.49 C 653.49 649.49 656.13 646.85 656.13 643.59 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 643.59 C 1.97 645.76 3.73 647.52 5.91 647.52 L 650.23 647.52 C 652.4 647.52 654.16 645.76 654.16 643.59 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 639.49)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison with LMMs on Detailed Image Caption Task</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="615.97" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A6.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.p2" class="ltx_para">
<span id="A6.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/com_5.jpg" id="A6.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="480" height="481" alt="[Uncaptioned image]">
</span>
</span>
<span id="A6.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A6.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3" class="ltx_para">
<span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><pre id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><em id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1.1" class="ltx_emph ltx_font_serif ltx_font_bold ltx_font_italic">Florence-2</em>: The image is a close-up of a red apple with a green label on it. The label has the text "FUJI 94131 USA" and a barcode on it, indicating that the apple is from the United States. The apple appears to be fresh and has a shiny texture. The background is white, making the apple stand out.
</pre>
<pre id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2.1" class="ltx_text ltx_font_serif ltx_font_bold">GPT-4V<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">4</span></span><a target="_blank" href="https://chat.openai.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://chat.openai.com</a></span></span></span></span>: The image displays a Fuji apple with a sticker indicating it is organic, along with the identifying number 94131, and specifying that it is from the USA. The apple has a reddish-pink skin with some yellow and green coloration, which is characteristic of Fuji apples. The apple is placed on a white background, which appears to be a textured fabric.
</pre>
<pre id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3.1" class="ltx_text ltx_font_serif ltx_font_bold">LLaVA<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note"><span id="footnote5.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">5</span></span><a target="_blank" href="https://llava.hliu.cc/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://llava.hliu.cc</a></span></span></span>:</span> The image features a large, fresh, and ripe red apple sitting on a white surface. The apple has a sticker on it, which is likely a barcode or a label providing information about the product. The apple is prominently displayed, taking up a significant portion of the image.
</pre>
<pre id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4.1" class="ltx_text ltx_font_serif ltx_font_bold">MiniGPT-4<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span id="footnote6.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">6</span></span><a target="_blank" href="https://huggingface.co/spaces/Vision-CAIR/minigpt4" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://huggingface.co/spaces/Vision-CAIR/minigpt4</a></span></span></span>:</span> The image shows a red apple with a label on it that reads “Organic” in white letters. The apple is sitting on a white surface.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A6.F18" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A6.F18.2.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="A6.F18.3.2" class="ltx_text" style="font-size:90%;">Comparison with LMMs on detailed caption task (continued).</span></figcaption>
</figure>
</div>
<div id="A6.3" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A6.3.p1" class="ltx_para ltx_noindent">
<svg id="A6.3.p1.pic1" class="ltx_picture" height="643.68" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,643.68) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 625.63 C 0 628.89 2.64 631.54 5.91 631.54 L 650.23 631.54 C 653.49 631.54 656.13 628.89 656.13 625.63 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 625.63 C 1.97 627.81 3.73 629.57 5.91 629.57 L 650.23 629.57 C 652.4 629.57 654.16 627.81 654.16 625.63 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 621.54)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison with LMMs on Detailed Image Caption Task</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="598.01" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A6.3.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.p2" class="ltx_para">
<span id="A6.3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/com_2.jpg" id="A6.3.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="366" alt="[Uncaptioned image]">
</span>
</span>
<span id="A6.3.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A6.3.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3" class="ltx_para">
<span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><pre id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><em id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1.1" class="ltx_emph ltx_font_serif ltx_font_bold ltx_font_italic">Florence-2</em>: The image shows a highway with multiple lanes. There are several vehicles on the road, including a black van and a red truck. On the left side of the image, there is a white semi-truck with the word "Martini" written on it. Above the highway, there are several bridges with cars and trucks on them. The sky is blue and there are trees and grass on the right side. In the center of the highway is a green sign that reads "Exit 94, Chamblee Tucker Rd, Mercer Univ."
</pre>
<pre id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2.1" class="ltx_text ltx_font_serif ltx_font_bold">GPT-4V<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span id="footnote7.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">7</span></span><a target="_blank" href="https://chat.openai.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://chat.openai.com</a></span></span></span></span>: The image shows a highway scene with several lanes of traffic. There are overpasses with cars and trucks traveling over them, and the highway itself appears to be busy with vehicles, including a prominent black van in the foreground.

A road sign indicates that Exit 94 for Chamblee-Tucker Rd and Mercer University is 1 1/4 miles ahead. The presence of the sign suggests that drivers in the right lanes will soon have the opportunity to take the exit to reach these destinations.

The infrastructure suggests a well-traveled interstate or highway system, possibly in an urban or suburban area given the overpass complexity and road signage for local roads and institutions. The vehicles on the road are diverse, indicating a mix of commercial and personal transport.
</pre>
<pre id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3.1" class="ltx_text ltx_font_serif ltx_font_bold">LLaVA<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note"><span id="footnote8.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">8</span></span><a target="_blank" href="https://llava.hliu.cc/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://llava.hliu.cc</a></span></span></span>:</span> The image depicts a busy highway with multiple lanes of traffic, including cars and trucks. There is a large truck driving on the highway, and several other trucks are also visible in the scene. The highway is filled with cars, and there is a traffic sign above the lanes, indicating the direction to Tucker Road. The scene captures the hustle and bustle of a typical day on a highway filled with vehicles.
</pre>
<pre id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.3.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4.1" class="ltx_text ltx_font_serif ltx_font_bold">MiniGPT-4<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note"><span id="footnote9.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">9</span></span><a target="_blank" href="https://huggingface.co/spaces/Vision-CAIR/minigpt4" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://huggingface.co/spaces/Vision-CAIR/minigpt4</a></span></span></span>:</span> The image shows a highway with multiple lanes of traffic moving in both directions. There are several large bridges spanning the highway, and in the distance, there are mountains and a blue sky. There are also several large trucks and cars on the highway, including a black van in the center of the image. The road surface is smooth and well-maintained.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A6.F19" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A6.F19.2.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="A6.F19.3.2" class="ltx_text" style="font-size:90%;">Comparison with LMMs on detailed caption task (continued).</span></figcaption>
</figure>
</div>
<div id="A6.4" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A6.4.p1" class="ltx_para ltx_noindent">
<svg id="A6.4.p1.pic1" class="ltx_picture" height="781.27" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,781.27) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 763.22 C 0 766.48 2.64 769.13 5.91 769.13 L 650.23 769.13 C 653.49 769.13 656.13 766.48 656.13 763.22 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 763.22 C 1.97 765.39 3.73 767.16 5.91 767.16 L 650.23 767.16 C 652.4 767.16 654.16 765.39 654.16 763.22 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 759.13)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 370.32 22.14 C 371.95 22.14 373.28 20.82 373.28 19.19 L 373.28 2.95 C 373.28 1.32 371.95 0 370.32 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison with LMMs on Detailed Image Caption Task</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="735.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A6.4.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.p2" class="ltx_para">
<span id="A6.4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/com_3.jpg" id="A6.4.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="302" height="452" alt="[Uncaptioned image]">
</span>
</span>
<span id="A6.4.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1" class="ltx_para ltx_noindent"><svg id="A6.4.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3" class="ltx_para">
<span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:496.9pt;"><pre id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><em id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.1.1" class="ltx_emph ltx_font_serif ltx_font_bold ltx_font_italic">Florence-2</em>: The image shows a kitchen countertop with a laptop on it. The laptop is open and the screen is displaying a blue abstract design. On the countertop, there are various kitchen utensils hanging on a metal rack. The rack is attached to the wall above the sink and has a variety of kitchen tools hanging on it, including a grater, a spatula, a ladle, a spoon, and a whisk. There are also a few other kitchen items scattered around the counter. The countertop is made of wood and there is a wire rack at the bottom of the image.
</pre>
<pre id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.2.1" class="ltx_text ltx_font_serif ltx_font_bold">GPT-4V<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">10</span></span><a target="_blank" href="https://chat.openai.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://chat.openai.com</a></span></span></span></span>: The image shows a modern kitchen setting where culinary tools are organized neatly on a rack against the wall. These tools include a variety of utensils such as a grater, tongs, a whisk, ladles, and spatulas. On the counter, there is an open laptop with a vibrant blue wallpaper on the screen, which suggests that someone might be using it for a recipe or as entertainment while cooking.
In front of the laptop, there appears to be a cutting board with some chopped ingredients and a knife, indicating that food preparation is underway or has recently taken place. There are also some dried chilies and a plastic container that might contain other ingredients or spices, further suggesting cooking activities.
The counter also has an electrical outlet and some covered sockets, and below the main counter, there are storage bins that likely hold additional kitchen supplies. The environment looks lived-in and practical, with a mix of technology and traditional cooking tools.
</pre>
<pre id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.3.1" class="ltx_text ltx_font_serif ltx_font_bold">LLaVA<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note"><span id="footnote11.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">11</span></span><a target="_blank" href="https://llava.hliu.cc/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://llava.hliu.cc</a></span></span></span>:</span> The image features a wooden kitchen counter with a laptop computer sitting on top of it. The laptop is open and displaying a picture of a cake. The counter is also cluttered with various kitchen utensils, including multiple spoons and knives, as well as a bowl. The scene suggests that someone might be using the laptop for cooking or baking purposes, possibly following a recipe or watching a tutorial.
</pre>
<pre id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"><span id="A6.4.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.2.p3.1.4.1" class="ltx_text ltx_font_serif ltx_font_bold">MiniGPT-4<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note"><span id="footnote12.1.1.1" class="ltx_text ltx_font_medium" style="font-size:143%;">12</span></span><a target="_blank" href="https://huggingface.co/spaces/Vision-CAIR/minigpt4" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:143%;">https://huggingface.co/spaces/Vision-CAIR/minigpt4</a></span></span></span>:</span> The image shows a kitchen counter with a laptop computer on it, a set of cooking utensils hanging from a rack above it, and a knife block on the counter. The laptop appears to be open, with a blue screen displaying an image of a kitchen. The utensils hanging from the rack include a spatula, a ladle, a whisk, and a cutting board. The knife block contains several knives, including a chef’s knife, a paring knife, and a bread knife.
</pre>
</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A6.F20" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A6.F20.2.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="A6.F20.3.2" class="ltx_text" style="font-size:90%;">Comparison with LMMs on detailed caption task (continued).</span></figcaption>
</figure>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Comarison with Kosmos-2 on Detailed Caption and Grounding Tasks</h2>

<div id="A7.1" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A7.1.p1" class="ltx_para ltx_noindent">
<svg id="A7.1.p1.pic1" class="ltx_picture" height="683.04" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,683.04) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 664.99 C 0 668.25 2.64 670.89 5.91 670.89 L 650.23 670.89 C 653.49 670.89 656.13 668.25 656.13 664.99 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 664.99 C 1.97 667.16 3.73 668.93 5.91 668.93 L 650.23 668.93 C 652.4 668.93 654.16 667.16 654.16 664.99 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 660.89)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 344.99 22.14 C 346.62 22.14 347.95 20.82 347.95 19.19 L 347.95 2.95 C 347.95 1.32 346.62 0 344.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 344.99 22.14 C 346.62 22.14 347.95 20.82 347.95 19.19 L 347.95 2.95 C 347.95 1.32 346.62 0 344.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison on detailed caption and grounding tasks.</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="637.37" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A7.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ouput_1_kosmos2.jpg" id="A7.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="309" height="309" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><code id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"> The image features a home office with <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.1" class="ltx_text" style="background-color:#69FF85;">two white desks</span>, one on the left and the other on the right. The desks are connected by  <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.2" class="ltx_text" style="background-color:#D0544F;">a white metal frame</span>, creating a functional and stylish workspace. <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.3" class="ltx_text" style="background-color:#73F7F3;">A laptop</span>  is placed on the desk on the far left, and   <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.4" class="ltx_text" style="background-color:#E86CE7;">a keyboard</span>  is visible on the other desk.   <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3.1.5" class="ltx_text" style="background-color:#DEE35F;">Two chairs</span>  are placed in front of the desks, one closer to the left desk and the second one on its right side.
</code>
</span>
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4" class="ltx_p ltx_align_center">(a) Result from Kosmos-2.</span>
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/output1_fld2.jpg" id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="309" height="309" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><pre id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">      The image shows a modern home office setup with two white <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1" class="ltx_text" style="background-color:#00C6C5;">desks<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.1.1" class="ltx_sup">1</sup></span> and <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.2" class="ltx_text" style="background-color:#49E4AB;">two black chairs<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.2.1" class="ltx_sup">2</sup></span>. <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.3" class="ltx_text" style="background-color:#FFF7AB;">The desks<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.3.1" class="ltx_sup">3</sup></span> are arranged in a corner of the room with a large window on the left side. On the right side of the desk, there is <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.4" class="ltx_text" style="background-color:#FF917C;">a computer monitor<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.4.1" class="ltx_sup">4</sup></span>, a <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.5" class="ltx_text" style="background-color:#0676A8;">keyboard<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.5.1" class="ltx_sup">5</sup></span>, a mouse, <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.6" class="ltx_text" style="background-color:#0BC0CC;">a printer<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.6.1" class="ltx_sup">6</sup></span>, and a <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.7" class="ltx_text" style="background-color:#9BFECB;">laptop<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.7.1" class="ltx_sup">7</sup></span>. Above the computer monitor and keyboard, there are <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.8" class="ltx_text" style="background-color:#FFE19E;">two framed pictures<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.8.1" class="ltx_sup">8</sup></span> hanging on the wall. <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.9" class="ltx_text" style="background-color:#AF299D;">The walls<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.9.1" class="ltx_sup">9</sup></span> are painted in a light green color and <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.10" class="ltx_text" style="background-color:#99FFB1;">the floor<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.10.1" class="ltx_sup">10</sup></span> is made of light-colored wood. <span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.11" class="ltx_text" style="background-color:#99C2FF;">The floor<sup id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5.1.11.1" class="ltx_sup">11</sup></span> is covered with a beige area rug with a geometric pattern. The overall style of the space is minimal and contemporary.
</pre>
</span>
<span id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6" class="ltx_p ltx_align_center">(b) Result from <em id="A7.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1" class="ltx_emph ltx_font_italic">Florence-2</em>.</span>
</span></foreignobject></g></g></svg>
</div>
<figure id="A7.F21" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A7.F21.2.1.1" class="ltx_text" style="font-size:90%;">Figure 21</span>: </span><span id="A7.F21.3.2" class="ltx_text" style="font-size:90%;">Systematic comparison with Kosmos-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib60" title="" class="ltx_ref">60</a>]</cite> on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from <a target="_blank" href="https://huggingface.co/spaces/ydshieh/Kosmos-2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/ydshieh/Kosmos-2</a>.</span></figcaption>
</figure>
</div>
<div id="A7.2" class="ltx_logical-block ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div id="A7.2.p1" class="ltx_para ltx_noindent">
<svg id="A7.2.p1.pic1" class="ltx_picture" height="770.88" overflow="visible" version="1.1" width="656.13"><g transform="translate(0,770.88) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000" fill-opacity="1.0"><path d="M 0 5.91 L 0 752.84 C 0 756.1 2.64 758.74 5.91 758.74 L 650.23 758.74 C 653.49 758.74 656.13 756.1 656.13 752.84 L 656.13 5.91 C 656.13 2.64 653.49 0 650.23 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 752.84 C 1.97 755.01 3.73 756.77 5.91 756.77 L 650.23 756.77 C 652.4 756.77 654.16 755.01 654.16 752.84 L 654.16 5.91 C 654.16 3.73 652.4 1.97 650.23 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 15 748.74)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 344.99 22.14 C 346.62 22.14 347.95 20.82 347.95 19.19 L 347.95 2.95 C 347.95 1.32 346.62 0 344.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill="#000000" fill-opacity="1.0"><path d="M 0 2.95 L 0 19.19 C 0 20.82 1.32 22.14 2.95 22.14 L 344.99 22.14 C 346.62 22.14 347.95 20.82 347.95 19.19 L 347.95 2.95 C 347.95 1.32 346.62 0 344.99 0 L 2.95 0 C 1.32 0 0 1.32 0 2.95 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.81 7.61)"><text transform="matrix(1 0 0 -1 0 0)" color="#000000">Comparison on detailed caption and grounding tasks.</text></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="612.83" height="725.22" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000"><span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2" class="ltx_inline-logical-block ltx_minipage ltx_align_bottom" style="width:442.9pt;">
<span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2" class="ltx_para">
<span id="A7.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/output_2_kosmos2.jpg" id="A7.2.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="274" height="155" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><code id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;"> The image features a white dining table with a variety of food items on it. On the table, there is a bowl of bread, a bowl with a salad, and a plate with a side of yogurt. In addition to the food, there are   <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.1.1.1" class="ltx_text" style="background-color:#69FF85;">two cups</span>  placed on the table. One cup is located near the left side of the table and the other cup is situated on the right side. The table is set with a fork and a knife, ready for a meal.
</code>
</span>
<span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.2" class="ltx_p ltx_align_center">(c) Result from Kosmos-2.</span>
<span id="A7.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/output2_fld2.jpg" id="A7.2.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="274" height="155" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><pre id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1" class="ltx_verbatim ltx_font_typewriter"><span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1" class="ltx_text" style="font-size:70%;">
  The image shows a table with a basket of bread and a plate of <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.1" class="ltx_text" style="background-color:#00C6C5;">salad<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.1.1" class="ltx_sup">1</sup></span>. <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.2" class="ltx_text" style="background-color:#49E4AB;">The basket<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.2.1" class="ltx_sup">2</sup></span> is made of woven straw and has several slices of bread in it. Next to the basket, there is <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.3" class="ltx_text" style="background-color:#00C6C5;">a small bowl<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.3.1" class="ltx_sup">1</sup></span> of salad with a variety of vegetables, including <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.4" class="ltx_text" style="background-color:#C3FEA8;">lettuce<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.4.1" class="ltx_sup">3</sup></span>, <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.5" class="ltx_text" style="background-color:#FFF7AB;">tomatoes<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.5.1" class="ltx_sup">4</sup></span>, <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.6" class="ltx_text" style="background-color:#FFC182;">cucumbers<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.6.1" class="ltx_sup">5</sup></span>, and <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.7" class="ltx_text" style="background-color:#FF917C;">feta cheese<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.7.1" class="ltx_sup">6</sup></span>. There are also <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.8" class="ltx_text" style="background-color:#FF6E9B;">two salt<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.8.1" class="ltx_sup">7</sup></span> and pepper <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.9" class="ltx_text" style="background-color:#FF6E9B;">shakers<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.9.1" class="ltx_sup">7</sup></span> on the table. On the right side of the table, there are <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.10" class="ltx_text" style="background-color:#0676A8;">two white plates<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.10.1" class="ltx_sup">8</sup></span><span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.11" class="ltx_text" style="background-color:#9BFECB;"><sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.11.1" class="ltx_sup">9</sup></span> with a dollop of white sauce on them. The table is covered with a white tablecloth and there are <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.12" class="ltx_text" style="background-color:#9BFECB;">a few other dishes<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.12.1" class="ltx_sup">9</sup></span> and <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.13" class="ltx_text" style="background-color:#FFE19E;">utensils<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.13.1" class="ltx_sup">10</sup></span> scattered around. <span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.14" class="ltx_text" style="background-color:#FF7B75;">A person"s hand<sup id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.3.1.1.14.1" class="ltx_sup">11</sup></span> can be seen in the top right corner of the image.
</span></pre>
</span>
<span id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.4" class="ltx_p ltx_align_center">(d) Result from <em id="A7.2.p1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.p2.4.1" class="ltx_emph ltx_font_italic">Florence-2</em>.</span>
</span>
<span id="A7.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p1" class="ltx_para ltx_noindent"><svg id="A7.2.p1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.p1.pic1" class="ltx_picture" height="23.62" overflow="visible" version="1.1" width="652.75"><g transform="translate(0,23.62) matrix(1 0 0 -1 0 0) translate(19.96,0) translate(0,11.81)" fill="#000000" fill-opacity="1.0" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-width="0.4pt"><path d="M -19.68 0 L 632.51 0" style="fill:none"></path></g></svg>
</span>
<span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3" class="ltx_para">
<span id="A7.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.3.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/output_3_kosmos2.jpg" id="A7.2.p1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="274" height="183" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.1" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><code id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.1.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">  The image features a family of   <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.1.1.1" class="ltx_text" style="background-color:#69FF85;">elephants</span>  walking together in a lush green forest. The  <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.1.1.2" class="ltx_text" style="background-color:#69FF85;">elephants</span> are walking in a line, with one of them walking in front of the rest of the family. The family consists of a mother and her two calves, with the mother and the two calves walking side by side. The forest is filled with green trees, adding to the natural beauty of the scene.
</code>
</span>
<span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.2" class="ltx_p ltx_align_center">(e) Result from Kosmos-2.</span>
<span id="A7.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.4" class="ltx_block ltx_parbox ltx_align_middle" style="width:248.4pt;"><pre id="A7.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.4.1" class="ltx_verbatim ltx_font_typewriter" style="font-size:70%;">    
        <img src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/output3_fld2.jpg" id="A7.2.p1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="274" height="183" alt="[Uncaptioned image]">
    </pre>
</span>
<span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3" class="ltx_block ltx_parbox ltx_align_middle" style="width:223.6pt;"><pre id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1" class="ltx_verbatim ltx_font_typewriter"><span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1" class="ltx_text" style="font-size:70%;">
  The image shows a group of three elephants standing in a dirt field with trees and bushes in the background. <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.1" class="ltx_text" style="background-color:#00C6C5;">The elephants<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.1.1" class="ltx_sup">1</sup></span> are standing close together, with the largest elephant in the center and two smaller ones on either side. <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.2" class="ltx_text" style="background-color:#00C6C5;">The largest elephant<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.2.1" class="ltx_sup">1</sup></span><span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.3" class="ltx_text" style="background-color:#49E4AB;"><sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.3.1" class="ltx_sup">2</sup></span>  on the left is standing with its <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.4" class="ltx_text" style="background-color:#C3FEA8;">trunk<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.4.1" class="ltx_sup">3</sup></span> extended, while <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.5" class="ltx_text" style="background-color:#FFF7AB;">the smaller one<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.5.1" class="ltx_sup">4</sup></span> is standing next to it. <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.6" class="ltx_text" style="background-color:#00C6C5;">All three elephants<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.6.1" class="ltx_sup">1</sup></span> have <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.7" class="ltx_text" style="background-color:#FFC182;">tusks<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.7.1" class="ltx_sup">5</sup></span> and appear to be in their natural habitat. <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.8" class="ltx_text" style="background-color:#0BC0CC;">The ground<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.8.1" class="ltx_sup">6</sup></span> is covered in dirt and there is <span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.9" class="ltx_text" style="background-color:#9BFECB;">a small pile of dirt<sup id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.3.1.1.9.1" class="ltx_sup">7</sup></span> in front of them. The overall mood of the image is peaceful and serene.
</span></pre>
</span>
<span id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.4" class="ltx_p ltx_align_center">(f) Result from <em id="A7.2.p1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.2.p3.4.1" class="ltx_emph ltx_font_italic">Florence-2</em>.</span>
</span></span></foreignobject></g></g></svg>
</div>
<figure id="A7.F22" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A7.F22.2.1.1" class="ltx_text" style="font-size:90%;">Figure 22</span>: </span><span id="A7.F22.3.2" class="ltx_text" style="font-size:90%;">Systematic comparison with Kosmos-2&nbsp;<cite class="ltx_cite ltx_citemacro_cite">[<a href="https://ar5iv.labs.arxiv.org/html/2311.06242#bib.bib60" title="" class="ltx_ref">60</a>]</cite> on detailed caption and grounding tasks. The models generate both the detailed caption and grounding results. The results of Kosmos-2 are from <a target="_blank" href="https://huggingface.co/spaces/ydshieh/Kosmos-2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/spaces/ydshieh/Kosmos-2</a>. (continued)</span></figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="https://ar5iv.labs.arxiv.org/html/2311.06241" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="https://ar5iv.labs.arxiv.org/"><img height="40" alt="ar5iv homepage" src="./[2311.06242] Florence-2_ Advancing a Unified Representation for a Variety of Vision Tasks_files/ar5iv.png"></a>
    <a href="https://ar5iv.labs.arxiv.org/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="https://ar5iv.labs.arxiv.org/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="https://ar5iv.labs.arxiv.org/log/2311.06242" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&amp;title=Improve+article+2311.06242">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2311.06242" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="https://ar5iv.labs.arxiv.org/html/2311.06243" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 20:00:44 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    

</body></html>