The data annotation workflow for the FLD-5B dataset consists of three essential phases:

1. Initial annotation employing specialist models: To begin the annotation process for each annotation type, the authors use synthetic labels obtained from specialist models. These specialist models are a combination of offline models trained on a diverse range of publicly available datasets and online services hosted on cloud platforms. They are specifically tailored to excel in annotating their respective annotation types.  For certain image datasets that already contain partial annotations for some annotation types, the authors merge the pre-existing annotations with the synthetic labels generated by the specialist models.    

For text annotation, images are categorized using three types of granularities: brief, detailed, and more detailed. For the brief text, a Florence-2 model is trained as the specialist on publicly available image caption and image-text datasets, creating an image-to-text model for initial annotations.    

For region-text pairs, text regions are labeled using Azure AI Services OCR API, while visual objects are initially annotated with a DINO object detector trained on public datasets.    

For text-phrase-region triplets, the text includes brief, detailed, and more detailed text generated earlier. For each text, the Grounding DINO model identifies noun phrases and creates bounding boxes for them. Additionally, the SAM model generates segmentation masks for each box, offering more precise object localization.    

2. Data filtering to correct errors and remove irrelevant annotations: The initial annotations obtained from the specialist models are susceptible to noise and imprecision. To address this, the authors implemented a multifaceted filtering process to refine and eliminate undesired annotations.    

For textual annotations, the authors develop a parsing tool based on SpaCy to extract objects, attributes, and actions. They filter out texts containing excessive objects, as they tend to introduce noise and may not accurately reflect the actual content in the corresponding images. Additionally, they assess the complexity of the actions and objects by measuring their degree of node in the dependency parsing tree. They retain texts with a certain minimum action and object complexity to ensure the richness of visual concepts in the images.    

In relation to the region annotations, specifically bounding boxes, they remove the noisy boxes under a confidence score threshold. Complementing this, they also employ non-maximum suppression to reduce redundant or overlapping bounding boxes.    

3. An iterative process for data refinement: Using the filtered initial annotations, the authors trained a multitask model that processes sequences of data. Upon evaluating this model against their training images, they discerned a marked enhancement in its predictions, particularly in instances where original labels were marred by inaccuracies or extraneous noise, such as in alt-texts. Motivated by these findings, they integrated these updated annotations with their original ones and subjected the model to another training iteration. This cyclical refinement process incrementally improves the quality of their training dataset.    

In the case of tasks they initially bypassed due to insufficient data for the training of a robust specialist model, they leveraged the iteratively trained model for pre-training purposes. Subsequent fine-tuning of this pre-trained model with the sparse dataset showcased superior performance compared to a model trained from scratch on the same data. Thus, they harness the fine-tuned model as a specialist for annotating their expansive dataset comprising 126 million images, ensuring comprehensive annotation coverage.    

The authors also utilize large language models (LLMs) or large multimodal models (LMMs) to generate comprehensive descriptions. Due to the high cost of the large models, only a small set of detailed text and more detailed text are generated. These are used to fine-tune the caption specialist, developing a detailed description specialist for further annotations.    

For region-text pairs, textual annotations for the visual object regions are further enriched by brief text generated from an image-to-text model with cropped image regions. Each region then receives three textual annotations: phrase from object category, brief text, and noun phrase chunks from the brief text. The Florence-1 model determines the most similar textual annotation to each image region.    

During data filtering for text-phrase-region triplets, a confidence score threshold is applied to both noun phrases and bounding boxes to ensure relevance. A blacklist is also used to exclude irrelevant noun phrases like pronouns and abstract concepts. 